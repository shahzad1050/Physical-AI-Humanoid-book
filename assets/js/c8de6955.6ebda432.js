"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[8438],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:n},e.children)}},8948:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module3/lab-ai-robot","title":"Practical Lab: Building an AI-Powered Robot","description":"Objective","source":"@site/docs/module3/lab-ai-robot.md","sourceDirName":"module3","slug":"/module3/lab-ai-robot","permalink":"/Physical-AI-Humanoid-book/docs/module3/lab-ai-robot","draft":false,"unlisted":false,"editUrl":"https://github.com/user/physical-ai-humanoid-robotics/tree/main/docs/module3/lab-ai-robot.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim for Advanced Simulation","permalink":"/Physical-AI-Humanoid-book/docs/module3/isaac-sim"},"next":{"title":"Manipulation and Grasping","permalink":"/Physical-AI-Humanoid-book/docs/module3/manipulation-grasping"}}');var o=t(4848),a=t(8453);const s={},r="Practical Lab: Building an AI-Powered Robot",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Project Setup and Environment Configuration",id:"step-1-project-setup-and-environment-configuration",level:2},{value:"Step 2: Create the Robot URDF Model",id:"step-2-create-the-robot-urdf-model",level:2},{value:"Step 3: Create the Perception Node",id:"step-3-create-the-perception-node",level:2},{value:"Step 4: Create the Navigation Node",id:"step-4-create-the-navigation-node",level:2},{value:"Step 5: Create the Manipulation Node",id:"step-5-create-the-manipulation-node",level:2},{value:"Step 6: Create the Main Control Node",id:"step-6-create-the-main-control-node",level:2},{value:"Step 7: Create Launch Files",id:"step-7-create-launch-files",level:2},{value:"Step 8: Update setup.py",id:"step-8-update-setuppy",level:2},{value:"Step 9: Create a Simulation Environment",id:"step-9-create-a-simulation-environment",level:2},{value:"Step 10: Build and Run the System",id:"step-10-build-and-run-the-system",level:2},{value:"Step 11: Testing and Validation",id:"step-11-testing-and-validation",level:2},{value:"Expected Results",id:"expected-results",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Extensions",id:"extensions",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"practical-lab-building-an-ai-powered-robot",children:"Practical Lab: Building an AI-Powered Robot"})}),"\n",(0,o.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"In this lab, you will create a complete AI-powered robot system using the NVIDIA Isaac Platform. You will design a robot with perception capabilities, implement AI-based control algorithms, and create a complete pipeline that integrates simulation and real-world deployment considerations."}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Completed all previous modules (ROS 2, Simulation, Isaac Platform)"}),"\n",(0,o.jsx)(n.li,{children:"Access to Isaac Sim (Omniverse or Docker)"}),"\n",(0,o.jsx)(n.li,{children:"NVIDIA GPU with CUDA support (recommended: RTX 3080 or better)"}),"\n",(0,o.jsx)(n.li,{children:"Basic understanding of Python and C++"}),"\n",(0,o.jsx)(n.li,{children:"Familiarity with Docker for Isaac Sim"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-1-project-setup-and-environment-configuration",children:"Step 1: Project Setup and Environment Configuration"}),"\n",(0,o.jsx)(n.p,{children:"First, create a workspace for your AI-powered robot project:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Create project directory\nmkdir -p ~/isaac_ai_robot_ws/src\ncd ~/isaac_ai_robot_ws/src\n\n# Create a new ROS 2 package for the robot system\nros2 pkg create --build-type ament_python ai_robot_system --dependencies rclpy std_msgs sensor_msgs geometry_msgs vision_msgs message_filters cv_bridge tf2_ros\n\n# Create additional directories for models and configurations\nmkdir -p ~/isaac_ai_robot_ws/src/ai_robot_system/config\nmkdir -p ~/isaac_ai_robot_ws/src/ai_robot_system/models\nmkdir -p ~/isaac_ai_robot_ws/src/ai_robot_system/launch\nmkdir -p ~/isaac_ai_robot_ws/src/ai_robot_system/ai_models\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-2-create-the-robot-urdf-model",children:"Step 2: Create the Robot URDF Model"}),"\n",(0,o.jsxs)(n.p,{children:["Create a simple mobile manipulator robot model in ",(0,o.jsx)(n.code,{children:"~/isaac_ai_robot_ws/src/ai_robot_system/models/ai_robot.urdf"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="ai_robot" xmlns:xacro="http://www.ros.org/wiki/xacro">\n  \x3c!-- Robot properties --\x3e\n  <xacro:property name="M_PI" value="3.1415926535897931" />\n\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder length="0.3" radius="0.3"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.3" radius="0.3"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="20.0"/>\n      <inertia\n        ixx="1.0" ixy="0.0" ixz="0.0"\n        iyy="1.0" iyz="0.0"\n        izz="1.0"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Camera link --\x3e\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.1"/>\n      <inertia\n        ixx="0.001" ixy="0.0" ixz="0.0"\n        iyy="0.001" iyz="0.0"\n        izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Camera joint --\x3e\n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.2 0 0.15" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Arm base link --\x3e\n  <link name="arm_base_link">\n    <visual>\n      <geometry>\n        <cylinder length="0.1" radius="0.05"/>\n      </geometry>\n      <material name="gray">\n        <color rgba="0.5 0.5 0.5 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.1" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia\n        ixx="0.001" ixy="0.0" ixz="0.0"\n        iyy="0.001" iyz="0.0"\n        izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Arm base joint --\x3e\n  <joint name="arm_base_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="arm_base_link"/>\n    <origin xyz="0.1 0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- First arm link --\x3e\n  <link name="arm_link1">\n    <visual>\n      <geometry>\n        <cylinder length="0.3" radius="0.02"/>\n      </geometry>\n      <material name="silver">\n        <color rgba="0.75 0.75 0.75 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.3" radius="0.02"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.5"/>\n      <inertia\n        ixx="0.001" ixy="0.0" ixz="0.0"\n        iyy="0.001" iyz="0.0"\n        izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- First arm joint --\x3e\n  <joint name="arm_joint1" type="revolute">\n    <parent link="arm_base_link"/>\n    <child link="arm_link1"/>\n    <origin xyz="0 0 0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n    <limit lower="${-M_PI/2}" upper="${M_PI/2}" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- Second arm link --\x3e\n  <link name="arm_link2">\n    <visual>\n      <geometry>\n        <cylinder length="0.25" radius="0.02"/>\n      </geometry>\n      <material name="silver">\n        <color rgba="0.75 0.75 0.75 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.25" radius="0.02"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.4"/>\n      <inertia\n        ixx="0.001" ixy="0.0" ixz="0.0"\n        iyy="0.001" iyz="0.0"\n        izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Second arm joint --\x3e\n  <joint name="arm_joint2" type="revolute">\n    <parent link="arm_link1"/>\n    <child link="arm_link2"/>\n    <origin xyz="0 0 0.15" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="${-M_PI/2}" upper="${M_PI/2}" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- End effector link --\x3e\n  <link name="end_effector">\n    <visual>\n      <geometry>\n        <box size="0.02 0.02 0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.02 0.02 0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.1"/>\n      <inertia\n        ixx="0.0001" ixy="0.0" ixz="0.0"\n        iyy="0.0001" iyz="0.0"\n        izz="0.0001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- End effector joint --\x3e\n  <joint name="ee_joint" type="fixed">\n    <parent link="arm_link2"/>\n    <child link="end_effector"/>\n    <origin xyz="0 0 0.125" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugins --\x3e\n  <gazebo>\n    \x3c!-- Differential drive plugin for base --\x3e\n    <plugin name="differential_drive" filename="libgazebo_ros_diff_drive.so">\n      <left_joint>wheel_left_joint</left_joint>\n      <right_joint>wheel_right_joint</right_joint>\n      <wheel_separation>0.5</wheel_separation>\n      <wheel_diameter>0.2</wheel_diameter>\n      <command_topic>cmd_vel</command_topic>\n      <odometry_topic>odom</odometry_topic>\n      <odometry_frame>odom</odometry_frame>\n      <robot_base_frame>base_link</robot_base_frame>\n    </plugin>\n  </gazebo>\n\n  \x3c!-- Add wheel joints and links here if needed --\x3e\n  <link name="wheel_left">\n    <visual>\n      <geometry>\n        <cylinder length="0.05" radius="0.1"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.05" radius="0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia\n        ixx="0.001" ixy="0.0" ixz="0.0"\n        iyy="0.001" iyz="0.0"\n        izz="0.002"/>\n    </inertial>\n  </link>\n\n  <link name="wheel_right">\n    <visual>\n      <geometry>\n        <cylinder length="0.05" radius="0.1"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.05" radius="0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia\n        ixx="0.001" ixy="0.0" ixz="0.0"\n        iyy="0.001" iyz="0.0"\n        izz="0.002"/>\n    </inertial>\n  </link>\n\n  <joint name="wheel_left_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_left"/>\n    <origin xyz="0 0.2 -0.1" rpy="${M_PI/2} 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <joint name="wheel_right_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_right"/>\n    <origin xyz="0 -0.2 -0.1" rpy="${M_PI/2} 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n</robot>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-3-create-the-perception-node",children:"Step 3: Create the Perception Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create the AI perception node in ",(0,o.jsx)(n.code,{children:"~/isaac_ai_robot_ws/src/ai_robot_system/ai_robot_system/perception_node.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\n\"\"\"\nAI Perception Node\nImplements object detection and scene understanding using Isaac GEMs\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nimport tf2_ros\nfrom tf2_ros import TransformException\nimport message_filters\n\n\nclass AI PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('ai_perception_node')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Initialize neural network model\n        self.initialize_model()\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Create publisher for detections\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/ai_robot/detections',\n            10\n        )\n\n        # Initialize camera parameters\n        self.camera_matrix = None\n        self.dist_coeffs = None\n\n        # TF broadcaster and listener\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        self.get_logger().info('AI Perception Node initialized')\n\n    def initialize_model(self):\n        \"\"\"Initialize the AI model for object detection\"\"\"\n        try:\n            # Load pre-trained model\n            self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n            self.model.eval()\n\n            # Define class names (COCO dataset classes)\n            self.class_names = [\n                '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n                'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n                'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n                'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n                'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n                'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n                'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n                'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n                'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n                'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n                'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n                'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n            ]\n\n            self.get_logger().info('AI model loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Failed to initialize AI model: {e}')\n            # Fallback to a simple detection method\n            self.model = None\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image messages\"\"\"\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Run object detection\n            if self.model is not None:\n                detections = self.run_object_detection(cv_image)\n            else:\n                # Fallback: simple color-based detection\n                detections = self.fallback_detection(cv_image)\n\n            # Create detection message\n            detection_msg = self.create_detection_message(detections, msg.header)\n\n            # Publish detections\n            self.detection_pub.publish(detection_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def run_object_detection(self, image):\n        \"\"\"Run object detection using the AI model\"\"\"\n        # Preprocess image\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n        input_tensor = transform(image)\n        input_batch = input_tensor.unsqueeze(0)\n\n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(input_batch)\n\n        # Process outputs\n        detections = []\n        for i, (boxes, scores, labels) in enumerate(zip(outputs[0]['boxes'],\n                                                        outputs[0]['scores'],\n                                                        outputs[0]['labels'])):\n            # Filter detections by confidence\n            keep = scores > 0.5\n            boxes = boxes[keep].cpu().numpy()\n            scores = scores[keep].cpu().numpy()\n            labels = labels[keep].cpu().numpy()\n\n            for box, score, label in zip(boxes, scores, labels):\n                if label < len(self.class_names):\n                    detection = {\n                        'bbox': box,\n                        'score': score,\n                        'class_name': self.class_names[label],\n                        'class_id': label\n                    }\n                    detections.append(detection)\n\n        return detections\n\n    def fallback_detection(self, image):\n        \"\"\"Fallback detection method using color thresholding\"\"\"\n        # Simple red object detection as fallback\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        lower_red = np.array([170, 50, 50])\n        upper_red = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask = mask1 + mask2\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        detections = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                x, y, w, h = cv2.boundingRect(contour)\n                detection = {\n                    'bbox': [x, y, x + w, y + h],\n                    'score': 0.8,  # Fallback confidence\n                    'class_name': 'red_object',\n                    'class_id': 0\n                }\n                detections.append(detection)\n\n        return detections\n\n    def camera_info_callback(self, msg):\n        \"\"\"Update camera calibration parameters\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.dist_coeffs = np.array(msg.d)\n\n    def create_detection_message(self, detections, header):\n        \"\"\"Create Detection2DArray message from detections\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            detection_2d = Detection2D()\n            detection_2d.header = header\n\n            # Set bounding box\n            bbox = detection['bbox']\n            detection_2d.bbox.center.x = (bbox[0] + bbox[2]) / 2.0\n            detection_2d.bbox.center.y = (bbox[1] + bbox[3]) / 2.0\n            detection_2d.bbox.size_x = abs(bbox[2] - bbox[0])\n            detection_2d.bbox.size_y = abs(bbox[3] - bbox[1])\n\n            # Set results\n            result = ObjectHypothesisWithPose()\n            result.hypothesis.class_id = str(detection['class_name'])\n            result.hypothesis.score = float(detection['score'])\n            detection_2d.results.append(result)\n\n            detection_array.detections.append(detection_2d)\n\n        return detection_array\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AI PerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-4-create-the-navigation-node",children:"Step 4: Create the Navigation Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create the navigation node in ",(0,o.jsx)(n.code,{children:"~/isaac_ai_robot_ws/src/ai_robot_system/ai_robot_system/navigation_node.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nAI Navigation Node\nImplements autonomous navigation using reinforcement learning\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped, Point\nfrom sensor_msgs.msg import LaserScan\nfrom nav_msgs.msg import Odometry\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\n\n\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super(DQN, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n\n    def forward(self, state):\n        return self.network(state)\n\n\nclass NavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'navigation_node\')\n\n        # Initialize DQN\n        self.state_dim = 36 + 3  # 36 laser readings + 3 (x, y, theta)\n        self.action_dim = 4  # 4 discrete actions: forward, left, right, stop\n        self.q_network = DQN(self.state_dim, self.action_dim)\n        self.target_network = DQN(self.state_dim, self.action_dim)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=1e-3)\n\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n        # Training parameters\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.01\n        self.batch_size = 32\n\n        # Replay buffer\n        self.memory = deque(maxlen=10000)\n\n        # Current state\n        self.current_scan = None\n        self.current_odom = None\n        self.target_position = np.array([5.0, 5.0])  # Target coordinates\n\n        # Create publishers and subscribers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.scan_sub = self.create_subscription(LaserScan, \'/scan\', self.scan_callback, 10)\n        self.odom_sub = self.create_subscription(Odometry, \'/odom\', self.odom_callback, 10)\n\n        # Create timer for navigation loop\n        self.nav_timer = self.create_timer(0.1, self.navigation_loop)\n\n        # Initialize TF\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        self.get_logger().info(\'AI Navigation Node initialized\')\n\n    def scan_callback(self, msg):\n        """Process laser scan data"""\n        # Process laser scan to reduce dimensionality\n        ranges = np.array(msg.ranges)\n        ranges = np.nan_to_num(ranges, nan=np.inf)  # Replace NaN with infinity\n\n        # Reduce to 36 readings for computational efficiency\n        step = len(ranges) // 36\n        self.current_scan = ranges[::step][:36]\n\n    def odom_callback(self, msg):\n        """Process odometry data"""\n        self.current_odom = msg\n\n    def get_state(self):\n        """Get current state for the RL agent"""\n        if self.current_scan is None or self.current_odom is None:\n            return None\n\n        # Get robot position and orientation\n        pos = self.current_odom.pose.pose.position\n        current_pos = np.array([pos.x, pos.y])\n\n        # Get robot orientation\n        quat = self.current_odom.pose.pose.orientation\n        # Convert quaternion to euler (simplified - just get yaw)\n        # In a real implementation, you\'d use tf2 for proper conversion\n        yaw = self.quaternion_to_yaw(quat)\n\n        # Calculate relative target position\n        relative_target = self.target_position - current_pos\n\n        # Combine laser scan and relative target position\n        state = np.concatenate([self.current_scan / 10.0, relative_target, [yaw]])\n\n        return state\n\n    def quaternion_to_yaw(self, quat):\n        """Convert quaternion to yaw angle (simplified)"""\n        # This is a simplified conversion - in practice, use tf2\n        siny_cosp = 2 * (quat.w * quat.z + quat.x * quat.y)\n        cosy_cosp = 1 - 2 * (quat.y * quat.y + quat.z * quat.z)\n        return np.arctan2(siny_cosp, cosy_cosp)\n\n    def select_action(self, state):\n        """Select action using epsilon-greedy policy"""\n        if np.random.random() <= self.epsilon:\n            return random.randrange(self.action_dim)\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.cpu().data.numpy())\n\n    def calculate_reward(self, state, action, next_state):\n        """Calculate reward based on state transition"""\n        # Get current and next positions\n        current_pos = np.array([state[-3], state[-2]])  # x, y from state\n        next_pos = np.array([next_state[-3], next_state[-2]])\n\n        # Distance to target\n        current_dist = np.linalg.norm(self.target_position - current_pos)\n        next_dist = np.linalg.norm(self.target_position - next_pos)\n\n        # Reward based on progress toward target\n        progress_reward = current_dist - next_dist\n\n        # Penalty for getting too close to obstacles\n        min_scan = np.min(state[:-3])  # Exclude position/orientation\n        obstacle_penalty = 0\n        if min_scan < 0.5:  # Too close to obstacle\n            obstacle_penalty = -1.0\n\n        # Large reward for reaching target\n        target_reward = 0\n        if next_dist < 0.5:  # Within 0.5m of target\n            target_reward = 100.0\n\n        # Small penalty for each step (encourage efficiency)\n        step_penalty = -0.1\n\n        total_reward = progress_reward + obstacle_penalty + target_reward + step_penalty\n\n        return total_reward\n\n    def remember(self, state, action, reward, next_state, done):\n        """Store experience in replay buffer"""\n        self.memory.append((state, action, reward, next_state, done))\n\n    def replay(self):\n        """Train the network on a batch of experiences"""\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.LongTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch])\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.BoolTensor([e[4] for e in batch])\n\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def navigation_loop(self):\n        """Main navigation loop"""\n        state = self.get_state()\n        if state is None:\n            return\n\n        # Select action\n        action = self.select_action(state)\n\n        # Execute action\n        cmd_vel = Twist()\n        if action == 0:  # Forward\n            cmd_vel.linear.x = 0.5\n            cmd_vel.angular.z = 0.0\n        elif action == 1:  # Turn left\n            cmd_vel.linear.x = 0.2\n            cmd_vel.angular.z = 0.5\n        elif action == 2:  # Turn right\n            cmd_vel.linear.x = 0.2\n            cmd_vel.angular.z = -0.5\n        elif action == 3:  # Stop\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.0\n\n        # Publish command\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Update network with experience\n        next_state = self.get_state()\n        if next_state is not None:\n            reward = self.calculate_reward(state, action, next_state)\n            done = np.linalg.norm(self.target_position - np.array([next_state[-3], next_state[-2]])) < 0.5\n            self.remember(state, action, reward, next_state, done)\n            self.replay()\n\n    def update_target_network(self):\n        """Update target network with current network weights"""\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NavigationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-5-create-the-manipulation-node",children:"Step 5: Create the Manipulation Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create the manipulation node in ",(0,o.jsx)(n.code,{children:"~/isaac_ai_robot_ws/src/ai_robot_system/ai_robot_system/manipulation_node.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nAI Manipulation Node\nImplements robotic arm control and grasping using AI techniques\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom std_msgs.msg import Float64MultiArray\nfrom vision_msgs.msg import Detection2DArray\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom scipy.spatial.transform import Rotation as R\nimport time\n\n\nclass GraspNet(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(GraspNet, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n\n    def forward(self, state):\n        return torch.tanh(self.network(state))\n\n\nclass ManipulationNode(Node):\n    def __init__(self):\n        super().__init__(\'manipulation_node\')\n\n        # Initialize grasp network\n        self.state_dim = 7 + 3 + 4  # joint states (7) + object position (3) + object orientation (4)\n        self.action_dim = 7  # 7 joint positions for the arm\n        self.grasp_net = GraspNet(self.state_dim, self.action_dim)\n\n        # Current states\n        self.current_joints = None\n        self.detected_objects = []\n        self.target_object = None\n\n        # Create subscribers\n        self.joint_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_state_callback, 10\n        )\n\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, \'/ai_robot/detections\', self.detection_callback, 10\n        )\n\n        # Create publishers\n        self.joint_cmd_pub = self.create_publisher(\n            Float64MultiArray, \'/joint_group_position_controller/commands\', 10\n        )\n\n        # Create timer for manipulation loop\n        self.manip_timer = self.create_timer(0.1, self.manipulation_loop)\n\n        # Initialize TF\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        self.get_logger().info(\'AI Manipulation Node initialized\')\n\n    def joint_state_callback(self, msg):\n        """Process joint state messages"""\n        self.current_joints = np.array(msg.position)\n\n    def detection_callback(self, msg):\n        """Process object detection messages"""\n        self.detected_objects = []\n\n        for detection in msg.detections:\n            if len(detection.results) > 0:\n                result = detection.results[0]\n                obj = {\n                    \'class\': result.hypothesis.class_id,\n                    \'confidence\': result.hypothesis.score,\n                    \'position\': detection.bbox.center  # This is image center, need to convert to 3D\n                }\n                self.detected_objects.append(obj)\n\n    def convert_image_to_3d(self, image_point, depth):\n        """Convert 2D image coordinates to 3D world coordinates"""\n        # This is a simplified conversion - in practice, you\'d use camera calibration\n        # For this example, we\'ll assume a fixed conversion\n        if depth is None:\n            depth = 1.0  # Default depth if not available\n\n        # Simplified conversion (would use camera matrix in real implementation)\n        x_3d = (image_point.x - 320) * depth / 640  # Assuming 640x480 image\n        y_3d = (image_point.y - 240) * depth / 480\n        z_3d = depth\n\n        return np.array([x_3d, y_3d, z_3d])\n\n    def select_target_object(self):\n        """Select the most promising object for manipulation"""\n        if not self.detected_objects:\n            return None\n\n        # For this example, select the object with highest confidence\n        best_obj = max(self.detected_objects, key=lambda x: x[\'confidence\'])\n\n        # Convert 2D position to 3D (simplified)\n        # In a real implementation, you\'d use depth information\n        obj_3d_pos = self.convert_image_to_3d(best_obj[\'position\'], None)\n\n        return {\n            \'class\': best_obj[\'class\'],\n            \'confidence\': best_obj[\'confidence\'],\n            \'position\': obj_3d_pos\n        }\n\n    def plan_grasp(self, object_pos):\n        """Plan a grasp for the target object"""\n        if self.current_joints is None:\n            return None\n\n        # Simplified grasp planning\n        # In a real implementation, you\'d use inverse kinematics and grasp planning algorithms\n\n        # Calculate desired end-effector position (slightly above object)\n        grasp_pos = object_pos.copy()\n        grasp_pos[2] += 0.2  # Approach from above\n\n        # Define grasp orientation (pointing down)\n        grasp_orientation = [0, 0, 0, 1]  # Quaternion for downward orientation\n\n        # Plan joint positions to reach this pose\n        # This is a simplified approach - real implementation would use IK\n        desired_joints = self.plan_arm_motion_to_pose(grasp_pos, grasp_orientation)\n\n        return desired_joints\n\n    def plan_arm_motion_to_pose(self, target_pos, target_orientation):\n        """Plan arm motion to reach target pose (simplified)"""\n        if self.current_joints is None:\n            return np.zeros(7)\n\n        # This is a very simplified approach\n        # In practice, you\'d use inverse kinematics (IK) solvers\n\n        # For this example, we\'ll just return a simple motion plan\n        # that moves the arm toward the target\n        current_pos = self.get_end_effector_position()\n\n        # Calculate motion direction\n        direction = target_pos - current_pos\n        step_size = 0.1  # Move in 10cm steps\n\n        # This is a placeholder - real implementation would use proper IK\n        desired_joints = self.current_joints.copy()\n\n        # Simple joint adjustments based on target position\n        # This is highly simplified and would not work in practice\n        desired_joints[0] += np.arctan2(target_pos[1], target_pos[0]) * 0.1  # Base joint\n        desired_joints[1] += (target_pos[2] - current_pos[2]) * 0.5  # Shoulder joint\n        desired_joints[2] += (target_pos[0] - current_pos[0]) * 0.2  # Elbow joint\n\n        # Constrain joint limits\n        joint_limits = [\n            [-2.967, 2.967],   # Joint 1\n            [-1.832, 1.832],   # Joint 2\n            [-2.618, 2.618],   # Joint 3\n            [-3.141, 3.141],   # Joint 4\n            [-2.967, 2.967],   # Joint 5\n            [-3.141, 3.141],   # Joint 6\n            [-2.967, 2.967]    # Joint 7\n        ]\n\n        for i, (min_limit, max_limit) in enumerate(joint_limits):\n            desired_joints[i] = np.clip(desired_joints[i], min_limit, max_limit)\n\n        return desired_joints\n\n    def get_end_effector_position(self):\n        """Get current end-effector position (simplified)"""\n        # This is a simplified forward kinematics calculation\n        # In practice, you\'d use a proper FK solver\n        if self.current_joints is None:\n            return np.array([0, 0, 0.5])  # Default position\n\n        # Simplified calculation based on joint angles\n        # This is not accurate and just for demonstration\n        q = self.current_joints\n\n        # Base position\n        x = 0.1  # Offset from base\n        y = 0\n        z = 0.1  # Base height\n\n        # Add contributions from each joint\n        # This is a very simplified approximation\n        for i, angle in enumerate(q[:3]):  # Consider first 3 joints\n            radius = 0.1 * (i + 1)  # Simplified link length\n            x += radius * np.cos(sum(q[:i+1]))\n            y += radius * np.sin(sum(q[:i+1]))\n            z += 0.05  # Height contribution\n\n        return np.array([x, y, z])\n\n    def execute_grasp(self, grasp_joints):\n        """Execute the planned grasp"""\n        if grasp_joints is None:\n            return False\n\n        # Create joint command message\n        cmd_msg = Float64MultiArray()\n        cmd_msg.data = grasp_joints.tolist()\n\n        # Publish command\n        self.joint_cmd_pub.publish(cmd_msg)\n\n        self.get_logger().info(f\'Executing grasp with joints: {grasp_joints}\')\n        return True\n\n    def manipulation_loop(self):\n        """Main manipulation loop"""\n        # Select target object\n        self.target_object = self.select_target_object()\n\n        if self.target_object is not None:\n            self.get_logger().info(f\'Target object: {self.target_object["class"]} at {self.target_object["position"]}\')\n\n            # Plan grasp\n            grasp_joints = self.plan_grasp(self.target_object[\'position\'])\n\n            if grasp_joints is not None:\n                # Execute grasp\n                success = self.execute_grasp(grasp_joints)\n\n                if success:\n                    self.get_logger().info(\'Grasp executed successfully\')\n                else:\n                    self.get_logger().warn(\'Failed to execute grasp\')\n            else:\n                self.get_logger().warn(\'Could not plan grasp for target object\')\n        else:\n            self.get_logger().info(\'No objects detected for manipulation\')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ManipulationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-6-create-the-main-control-node",children:"Step 6: Create the Main Control Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create the main control node in ",(0,o.jsx)(n.code,{children:"~/isaac_ai_robot_ws/src/ai_robot_system/ai_robot_system/ai_robot_controller.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\n\"\"\"\nAI Robot Main Controller\nCoordinates perception, navigation, and manipulation modules\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Twist\nfrom sensor_msgs.msg import JointState\nfrom vision_msgs.msg import Detection2DArray\nfrom ai_robot_system.perception_node import AI PerceptionNode\nfrom ai_robot_system.navigation_node import NavigationNode\nfrom ai_robot_system.manipulation_node import ManipulationNode\nimport time\nimport threading\n\n\nclass AIRobotController(Node):\n    def __init__(self):\n        super().__init__('ai_robot_controller')\n\n        # Initialize component nodes\n        self.perception_node = AI PerceptionNode()\n        self.navigation_node = NavigationNode()\n        self.manipulation_node = ManipulationNode()\n\n        # Create state machine\n        self.current_state = \"IDLE\"  # IDLE, NAVIGATING, MANIPULATING, PERCEIVING\n        self.task_queue = []\n        self.robot_tasks = {\n            'explore': self.explore_environment,\n            'find_object': self.find_object_task,\n            'navigate_to_object': self.navigate_to_object_task,\n            'grasp_object': self.grasp_object_task,\n            'place_object': self.place_object_task\n        }\n\n        # Create publisher for state updates\n        self.state_pub = self.create_publisher(String, '/ai_robot/state', 10)\n\n        # Create timer for main control loop\n        self.control_timer = self.create_timer(1.0, self.control_loop)\n\n        self.get_logger().info('AI Robot Controller initialized')\n\n    def control_loop(self):\n        \"\"\"Main control loop\"\"\"\n        # Publish current state\n        state_msg = String()\n        state_msg.data = self.current_state\n        self.state_pub.publish(state_msg)\n\n        # Process task queue\n        if self.task_queue:\n            next_task = self.task_queue[0]\n            self.execute_task(next_task)\n        else:\n            # Default behavior when no tasks\n            self.current_state = \"IDLE\"\n            self.idle_behavior()\n\n    def execute_task(self, task):\n        \"\"\"Execute a specific task\"\"\"\n        if task in self.robot_tasks:\n            self.get_logger().info(f'Executing task: {task}')\n            self.current_state = task.upper()\n            success = self.robot_tasks[task]()\n\n            if success:\n                # Remove completed task\n                self.task_queue.pop(0)\n                self.get_logger().info(f'Task {task} completed successfully')\n            else:\n                self.get_logger().warn(f'Task {task} failed')\n        else:\n            self.get_logger().error(f'Unknown task: {task}')\n            self.task_queue.pop(0)  # Remove invalid task\n\n    def add_task(self, task):\n        \"\"\"Add a task to the queue\"\"\"\n        self.task_queue.append(task)\n        self.get_logger().info(f'Task {task} added to queue')\n\n    def idle_behavior(self):\n        \"\"\"Behavior when robot is idle\"\"\"\n        # For now, just stay still\n        # In a real implementation, you might have the robot patrol or charge\n        pass\n\n    def explore_environment(self):\n        \"\"\"Explore the environment\"\"\"\n        self.get_logger().info('Exploring environment...')\n\n        # This would involve moving to various locations to build a map\n        # For this example, we'll just move in a simple pattern\n        self.navigation_node.target_position = [2.0, 2.0]\n        time.sleep(5)  # Simulate exploration time\n\n        self.navigation_node.target_position = [-2.0, 2.0]\n        time.sleep(5)\n\n        self.navigation_node.target_position = [0.0, 0.0]  # Return to start\n        time.sleep(5)\n\n        return True\n\n    def find_object_task(self):\n        \"\"\"Find a specific object in the environment\"\"\"\n        self.get_logger().info('Searching for objects...')\n\n        # Wait for object detections\n        timeout = time.time() + 60*2  # 2 minutes timeout\n        while time.time() < timeout:\n            if self.perception_node.current_scan is not None:\n                # Check if any objects were detected\n                if self.perception_node.detection_pub.get_subscription_count() > 0:\n                    # In a real implementation, you'd check the actual detections\n                    # For this example, we'll assume an object was found\n                    self.get_logger().info('Object found!')\n                    return True\n            time.sleep(0.1)\n\n        self.get_logger().warn('Object not found within timeout')\n        return False\n\n    def navigate_to_object_task(self):\n        \"\"\"Navigate to a detected object\"\"\"\n        self.get_logger().info('Navigating to object...')\n\n        # In a real implementation, you'd use the object's position\n        # For this example, we'll navigate to a fixed location\n        self.navigation_node.target_position = [1.0, 1.0]\n\n        # Wait until navigation is complete\n        timeout = time.time() + 60  # 1 minute timeout\n        while time.time() < timeout:\n            # Check if robot is close to target\n            if self.navigation_node.current_odom is not None:\n                pos = self.navigation_node.current_odom.pose.pose.position\n                current_pos = np.array([pos.x, pos.y])\n                dist = np.linalg.norm(self.navigation_node.target_position - current_pos)\n\n                if dist < 0.5:  # Within 0.5m of target\n                    self.get_logger().info('Navigation to object completed')\n                    return True\n            time.sleep(0.1)\n\n        self.get_logger().warn('Navigation to object failed')\n        return False\n\n    def grasp_object_task(self):\n        \"\"\"Grasp a nearby object\"\"\"\n        self.get_logger().info('Attempting to grasp object...')\n\n        # In a real implementation, you'd coordinate with perception and manipulation\n        # For this example, we'll just execute a simple grasp motion\n        if self.manipulation_node.current_joints is not None:\n            # Plan a simple grasp motion\n            grasp_joints = self.manipulation_node.current_joints.copy()\n            # Move to a pre-defined grasp position\n            grasp_joints[0] = 0.5  # Base joint\n            grasp_joints[1] = 0.3  # Shoulder joint\n            grasp_joints[2] = -0.2  # Elbow joint\n\n            self.manipulation_node.execute_grasp(grasp_joints)\n            time.sleep(3)  # Wait for grasp to complete\n\n            # Close gripper (simplified)\n            # In a real implementation, you'd control the gripper separately\n            self.get_logger().info('Grasp attempt completed')\n            return True\n\n        self.get_logger().warn('Cannot execute grasp - no joint information available')\n        return False\n\n    def place_object_task(self):\n        \"\"\"Place the grasped object at a location\"\"\"\n        self.get_logger().info('Placing object...')\n\n        # Move to placement location\n        self.navigation_node.target_position = [3.0, 0.0]\n\n        # Wait for navigation to complete\n        timeout = time.time() + 60  # 1 minute timeout\n        while time.time() < timeout:\n            if self.navigation_node.current_odom is not None:\n                pos = self.navigation_node.current_odom.pose.pose.position\n                current_pos = np.array([pos.x, pos.y])\n                dist = np.linalg.norm(self.navigation_node.target_position - current_pos)\n\n                if dist < 0.5:\n                    # Open gripper to place object (simplified)\n                    # In a real implementation, you'd control the gripper separately\n                    self.get_logger().info('Object placed successfully')\n                    return True\n            time.sleep(0.1)\n\n        self.get_logger().warn('Object placement failed')\n        return False\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = AIRobotController()\n\n    # Add some sample tasks to the queue\n    controller.add_task('explore')\n    controller.add_task('find_object')\n    controller.add_task('navigate_to_object')\n    controller.add_task('grasp_object')\n    controller.add_task('place_object')\n\n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-7-create-launch-files",children:"Step 7: Create Launch Files"}),"\n",(0,o.jsxs)(n.p,{children:["Create the main launch file in ",(0,o.jsx)(n.code,{children:"~/isaac_ai_robot_ws/src/ai_robot_system/launch/ai_robot_system.launch.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Package directory\n    pkg_dir = get_package_share_directory('ai_robot_system')\n\n    # Launch arguments\n    use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='true',\n        description='Use simulation (Gazebo) clock if true'\n    )\n\n    # Launch Gazebo simulation (if available)\n    # This would typically be a separate launch file, but we'll include it here\n    # For this example, we'll assume a separate Gazebo launch is handled externally\n\n    # AI Perception Node\n    perception_node = Node(\n        package='ai_robot_system',\n        executable='perception_node',\n        name='ai_perception_node',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    # AI Navigation Node\n    navigation_node = Node(\n        package='ai_robot_system',\n        executable='navigation_node',\n        name='ai_navigation_node',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    # AI Manipulation Node\n    manipulation_node = Node(\n        package='ai_robot_system',\n        executable='manipulation_node',\n        name='ai_manipulation_node',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    # AI Robot Controller Node\n    controller_node = Node(\n        package='ai_robot_system',\n        executable='ai_robot_controller',\n        name='ai_robot_controller',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        use_sim_time,\n        perception_node,\n        navigation_node,\n        manipulation_node,\n        controller_node\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-8-update-setuppy",children:"Step 8: Update setup.py"}),"\n",(0,o.jsxs)(n.p,{children:["Update the ",(0,o.jsx)(n.code,{children:"setup.py"})," file in ",(0,o.jsx)(n.code,{children:"~/isaac_ai_robot_ws/src/ai_robot_system/setup.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\n\npackage_name = 'ai_robot_system'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        ('share/' + package_name + '/launch', ['launch/ai_robot_system.launch.py']),\n        ('share/' + package_name + '/models', ['models/ai_robot.urdf']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Your Name',\n    maintainer_email='your.email@example.com',\n    description='AI-powered robot system using NVIDIA Isaac Platform',\n    license='Apache License 2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'perception_node = ai_robot_system.perception_node:main',\n            'navigation_node = ai_robot_system.navigation_node:main',\n            'manipulation_node = ai_robot_system.manipulation_node:main',\n            'ai_robot_controller = ai_robot_system.ai_robot_controller:main',\n        ],\n    },\n)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-9-create-a-simulation-environment",children:"Step 9: Create a Simulation Environment"}),"\n",(0,o.jsxs)(n.p,{children:["Create a simple world file for Gazebo in ",(0,o.jsx)(n.code,{children:"~/isaac_ai_robot_ws/src/ai_robot_system/worlds/ai_robot_world.world"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0" ?>\n<sdf version="1.7">\n  <world name="ai_robot_world">\n    \x3c!-- Include ground plane --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    \x3c!-- Include sun --\x3e\n    <include>\n      <uri>model://sun</uri>\n    </include>\n\n    \x3c!-- Add some objects for the robot to interact with --\x3e\n    <model name="red_box">\n      <pose>2 2 0.1 0 0 0</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>0.2 0.2 0.2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>0.2 0.2 0.2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>1 0 0 1</ambient>\n            <diffuse>1 0 0 1</diffuse>\n          </material>\n        </visual>\n        <inertial>\n          <mass>0.5</mass>\n          <inertia>\n            <ixx>0.0017</ixx>\n            <ixy>0</ixy>\n            <ixz>0</ixz>\n            <iyy>0.0017</iyy>\n            <iyz>0</iyz>\n            <izz>0.0017</izz>\n          </inertia>\n        </inertial>\n      </link>\n    </model>\n\n    <model name="blue_cylinder">\n      <pose>-2 -1 0.15 0 0 0</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <cylinder>\n              <radius>0.1</radius>\n              <length>0.3</length>\n            </cylinder>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <cylinder>\n              <radius>0.1</radius>\n              <length>0.3</length>\n            </cylinder>\n          </geometry>\n          <material>\n            <ambient>0 0 1 1</ambient>\n            <diffuse>0 0 1 1</diffuse>\n          </material>\n        </visual>\n        <inertial>\n          <mass>0.3</mass>\n          <inertia>\n            <ixx>0.0015</ixx>\n            <ixy>0</ixy>\n            <ixz>0</ixz>\n            <iyy>0.0015</iyy>\n            <iyz>0</iyz>\n            <izz>0.001</izz>\n          </inertia>\n        </inertial>\n      </link>\n    </model>\n\n    \x3c!-- Physics configuration --\x3e\n    <physics type="ode">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1</real_time_factor>\n      <real_time_update_rate>1000</real_time_update_rate>\n    </physics>\n  </world>\n</sdf>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-10-build-and-run-the-system",children:"Step 10: Build and Run the System"}),"\n",(0,o.jsx)(n.p,{children:"Build your package:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ai_robot_ws\ncolcon build --packages-select ai_robot_system\nsource install/setup.bash\n"})}),"\n",(0,o.jsx)(n.p,{children:"Run the complete AI robot system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch the robot system\nros2 launch ai_robot_system ai_robot_system.launch.py\n\n# Terminal 2: (Optional) Launch Gazebo simulation with your robot\n# This would depend on your specific Gazebo setup\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-11-testing-and-validation",children:"Step 11: Testing and Validation"}),"\n",(0,o.jsx)(n.p,{children:"Test the system by sending commands and monitoring the behavior:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Monitor robot state\nros2 topic echo /ai_robot/state\n\n# Monitor detections\nros2 topic echo /ai_robot/detections\n\n# Monitor navigation commands\nros2 topic echo /cmd_vel\n\n# Send manual navigation commands if needed\nros2 topic pub /cmd_vel geometry_msgs/msg/Twist '{linear: {x: 0.5}, angular: {z: 0.2}}'\n"})}),"\n",(0,o.jsx)(n.h2,{id:"expected-results",children:"Expected Results"}),"\n",(0,o.jsx)(n.p,{children:"When you run the complete AI robot system:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"The perception node should detect objects in the environment"}),"\n",(0,o.jsx)(n.li,{children:"The navigation node should plan paths to navigate to objects"}),"\n",(0,o.jsx)(n.li,{children:"The manipulation node should plan and execute grasping motions"}),"\n",(0,o.jsx)(n.li,{children:"The main controller should coordinate all modules to perform complex tasks"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(n.p,{children:"If you encounter issues:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Missing dependencies"}),": Install required Python packages:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip3 install torch torchvision opencv-python tf2_ros\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"CUDA issues"}),": Ensure your NVIDIA drivers and CUDA are properly installed"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 topics not connecting"}),": Check that all nodes are using the same ROS domain ID"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Performance issues"}),": The AI models may be computationally intensive; ensure your GPU can handle the workload"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"extensions",children:"Extensions"}),"\n",(0,o.jsx)(n.p,{children:"To extend this system:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Add more sophisticated AI models"}),": Replace the simple models with more advanced ones"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Implement learning algorithms"}),": Add reinforcement learning for improved navigation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Add more sensors"}),": Include depth cameras, IMUs, or other sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Improve manipulation"}),": Add grasp planning and force control"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Add semantic mapping"}),": Create maps with object labels and relationships"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this lab, you have created a complete AI-powered robot system that integrates:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Perception using computer vision and AI"}),"\n",(0,o.jsx)(n.li,{children:"Navigation using reinforcement learning"}),"\n",(0,o.jsx)(n.li,{children:"Manipulation using AI-based grasp planning"}),"\n",(0,o.jsx)(n.li,{children:"High-level task coordination"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This system demonstrates the power of the NVIDIA Isaac Platform for creating sophisticated AI-powered robotic applications that can perceive, navigate, and manipulate in their environment."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);