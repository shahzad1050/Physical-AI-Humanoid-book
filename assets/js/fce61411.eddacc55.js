"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[3898],{3871:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4/lab-humanoid-controller","title":"Practical Lab: Building a Humanoid Robot Controller","description":"Objective","source":"@site/docs/module4/lab-humanoid-controller.md","sourceDirName":"module4","slug":"/module4/lab-humanoid-controller","permalink":"/Physical-AI-Humanoid-book/docs/module4/lab-humanoid-controller","draft":false,"unlisted":false,"editUrl":"https://github.com/user/physical-ai-humanoid-robotics/tree/main/docs/module4/lab-humanoid-controller.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Robot Kinematics","permalink":"/Physical-AI-Humanoid-book/docs/module4/kinematics"},"next":{"title":"Manipulation and Grasping","permalink":"/Physical-AI-Humanoid-book/docs/module4/manipulation"}}');var o=r(4848),s=r(8453);const i={},a="Practical Lab: Building a Humanoid Robot Controller",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Project Setup and Architecture",id:"step-1-project-setup-and-architecture",level:2},{value:"Step 2: Create the Main Controller Node",id:"step-2-create-the-main-controller-node",level:2},{value:"Step 3: Create the Balance Controller",id:"step-3-create-the-balance-controller",level:2},{value:"Step 4: Create the Manipulation Controller",id:"step-4-create-the-manipulation-controller",level:2},{value:"Step 5: Create the State Estimator",id:"step-5-create-the-state-estimator",level:2},{value:"Step 6: Create Social Behavior Manager",id:"step-6-create-social-behavior-manager",level:2},{value:"Step 7: Create the Main Launch File",id:"step-7-create-the-main-launch-file",level:2},{value:"Step 8: Update setup.py",id:"step-8-update-setuppy",level:2},{value:"Step 9: Create a Simple Test Script",id:"step-9-create-a-simple-test-script",level:2},{value:"Step 10: Build and Test the Controller",id:"step-10-build-and-test-the-controller",level:2},{value:"Step 11: Advanced Testing Scenarios",id:"step-11-advanced-testing-scenarios",level:2},{value:"Expected Results",id:"expected-results",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Extensions",id:"extensions",level:2}];function _(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"practical-lab-building-a-humanoid-robot-controller",children:"Practical Lab: Building a Humanoid Robot Controller"})}),"\n",(0,o.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"In this lab, you will implement a complete humanoid robot controller that integrates all the concepts learned in Module 4. You will create a system that can maintain balance, perform basic manipulation tasks, and interact socially with humans. The controller will demonstrate coordination between balance, locomotion, manipulation, and human-robot interaction capabilities."}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Completed all previous modules (ROS 2, Simulation, Isaac Platform, Humanoid Robotics fundamentals)"}),"\n",(0,o.jsx)(n.li,{children:"Access to a humanoid robot simulator (Gazebo, Isaac Sim, or similar)"}),"\n",(0,o.jsx)(n.li,{children:"Python 3.8+ with required packages installed"}),"\n",(0,o.jsx)(n.li,{children:"Understanding of control theory, kinematics, and dynamics"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-1-project-setup-and-architecture",children:"Step 1: Project Setup and Architecture"}),"\n",(0,o.jsx)(n.p,{children:"First, let's create the project structure for our humanoid controller:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Create project directory\r\nmkdir -p ~/humanoid_robot_controller/src\r\ncd ~/humanoid_robot_controller/src\r\n\r\n# Create ROS 2 package\r\nros2 pkg create --build-type ament_python humanoid_controller --dependencies rclpy std_msgs sensor_msgs geometry_msgs builtin_interfaces message_filters cv_bridge tf2_ros tf2_geometry_msgs\r\n\r\n# Create directory structure\r\nmkdir -p ~/humanoid_robot_controller/src/humanoid_controller/{controllers,sensors,utils,behaviors}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-2-create-the-main-controller-node",children:"Step 2: Create the Main Controller Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create the main controller node in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/humanoid_controller_node.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\n\"\"\"\r\nHumanoid Robot Controller\r\nIntegrates balance, manipulation, and social interaction capabilities\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import JointState, Imu, Image\r\nfrom geometry_msgs.msg import Twist, Pose, Point, Vector3\r\nfrom std_msgs.msg import String, Float64MultiArray\r\nfrom builtin_interfaces.msg import Duration\r\nfrom tf2_ros import TransformException\r\nfrom tf2_ros.buffer import Buffer\r\nfrom tf2_ros.transform_listener import TransformListener\r\nfrom tf2_ros import TransformBroadcaster\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport math\r\nimport time\r\nfrom collections import deque\r\n\r\n# Import controller modules\r\nfrom .controllers.balance_controller import BalanceController\r\nfrom .controllers.manipulation_controller import ManipulationController\r\nfrom .controllers.locomotion_controller import LocomotionController\r\nfrom .sensors.state_estimator import StateEstimator\r\nfrom .behaviors.social_behavior import SocialBehaviorManager\r\n\r\n\r\nclass HumanoidController(Node):\r\n    def __init__(self):\r\n        super().__init__('humanoid_controller')\r\n\r\n        # Initialize components\r\n        self.balance_controller = BalanceController(self)\r\n        self.manipulation_controller = ManipulationController(self)\r\n        self.locomotion_controller = LocomotionController(self)\r\n        self.state_estimator = StateEstimator(self)\r\n        self.social_behavior_manager = SocialBehaviorManager(self)\r\n\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Robot state variables\r\n        self.joint_positions = np.zeros(28)  # Example: 28 DOF humanoid\r\n        self.joint_velocities = np.zeros(28)\r\n        self.imu_data = None\r\n        self.camera_image = None\r\n        self.force_torque_data = {'left_foot': [0,0,0], 'right_foot': [0,0,0]}\r\n\r\n        # Control state\r\n        self.current_mode = 'idle'  # idle, balance, walk, manipulate, interact\r\n        self.desired_com_position = np.array([0.0, 0.0, 0.8])  # Desired CoM height: 80cm\r\n        self.desired_com_velocity = np.zeros(3)\r\n\r\n        # Create subscribers\r\n        self.joint_state_sub = self.create_subscription(\r\n            JointState,\r\n            '/joint_states',\r\n            self.joint_state_callback,\r\n            10\r\n        )\r\n\r\n        self.imu_sub = self.create_subscription(\r\n            Imu,\r\n            '/imu/data',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        self.camera_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.camera_callback,\r\n            10\r\n        )\r\n\r\n        self.ft_sensor_sub = self.create_subscription(\r\n            Float64MultiArray,\r\n            '/force_torque_sensors',\r\n            self.force_torque_callback,\r\n            10\r\n        )\r\n\r\n        # Create publishers\r\n        self.joint_cmd_pub = self.create_publisher(\r\n            Float64MultiArray,\r\n            '/joint_group_position_controller/commands',\r\n            10\r\n        )\r\n\r\n        self.com_pub = self.create_publisher(\r\n            Point,\r\n            '/center_of_mass',\r\n            10\r\n        )\r\n\r\n        self.zmp_pub = self.create_publisher(\r\n            Point,\r\n            '/zero_moment_point',\r\n            10\r\n        )\r\n\r\n        self.status_pub = self.create_publisher(\r\n            String,\r\n            '/controller_status',\r\n            10\r\n        )\r\n\r\n        # Create service clients\r\n        self.service_clients = {}\r\n\r\n        # Timer for main control loop\r\n        self.control_timer = self.create_timer(0.01, self.main_control_loop)  # 100Hz\r\n\r\n        # TF broadcaster and listener\r\n        self.tf_buffer = Buffer()\r\n        self.tf_listener = TransformListener(self.tf_buffer, self)\r\n        self.tf_broadcaster = TransformBroadcaster(self)\r\n\r\n        # Initialize robot state\r\n        self.initialize_robot()\r\n\r\n        self.get_logger().info('Humanoid Controller initialized')\r\n\r\n    def initialize_robot(self):\r\n        \"\"\"Initialize robot to safe configuration\"\"\"\r\n        # Move to neutral standing position\r\n        neutral_pos = self.get_neutral_standing_position()\r\n\r\n        cmd_msg = Float64MultiArray()\r\n        cmd_msg.data = neutral_pos.tolist()\r\n        self.joint_cmd_pub.publish(cmd_msg)\r\n\r\n        # Wait for robot to reach position\r\n        time.sleep(2.0)\r\n\r\n        # Switch to balance mode\r\n        self.current_mode = 'balance'\r\n        self.get_logger().info('Robot initialized to balance mode')\r\n\r\n    def get_neutral_standing_position(self):\r\n        \"\"\"Get neutral standing joint configuration\"\"\"\r\n        # Example neutral standing position for a typical humanoid\r\n        # This would be specific to your robot model\r\n        neutral_pos = np.array([\r\n            0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # Head/Neck joints\r\n            0.0, 0.2, -0.4, 0.2, 0.0, 0.0, 0.0,  # Left arm\r\n            0.0, -0.2, 0.4, -0.2, 0.0, 0.0, 0.0,  # Right arm\r\n            0.0, 0.0, -0.3, 0.6, -0.3, 0.0,  # Left leg\r\n            0.0, 0.0, -0.3, 0.6, -0.3, 0.0   # Right leg\r\n        ])\r\n\r\n        return neutral_pos\r\n\r\n    def joint_state_callback(self, msg):\r\n        \"\"\"Process joint state messages\"\"\"\r\n        if len(msg.position) == len(self.joint_positions):\r\n            self.joint_positions = np.array(msg.position)\r\n\r\n        if len(msg.velocity) == len(self.joint_velocities):\r\n            self.joint_velocities = np.array(msg.velocity)\r\n\r\n    def imu_callback(self, msg):\r\n        \"\"\"Process IMU data\"\"\"\r\n        self.imu_data = {\r\n            'linear_acceleration': np.array([msg.linear_acceleration.x,\r\n                                           msg.linear_acceleration.y,\r\n                                           msg.linear_acceleration.z]),\r\n            'angular_velocity': np.array([msg.angular_velocity.x,\r\n                                        msg.angular_velocity.y,\r\n                                        msg.angular_velocity.z]),\r\n            'orientation': np.array([msg.orientation.w, msg.orientation.x,\r\n                                   msg.orientation.y, msg.orientation.z])\r\n        }\r\n\r\n    def camera_callback(self, msg):\r\n        \"\"\"Process camera images\"\"\"\r\n        try:\r\n            self.camera_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to convert image: {e}')\r\n\r\n    def force_torque_callback(self, msg):\r\n        \"\"\"Process force/torque sensor data\"\"\"\r\n        if len(msg.data) >= 6:  # At least 6 values for both feet\r\n            self.force_torque_data = {\r\n                'left_foot': msg.data[0:3],   # Fx, Fy, Fz\r\n                'right_foot': msg.data[3:6]   # Fx, Fy, Fz\r\n            }\r\n\r\n    def main_control_loop(self):\r\n        \"\"\"Main control loop that orchestrates all subsystems\"\"\"\r\n        # Update state estimation\r\n        current_state = self.state_estimator.estimate_state(\r\n            self.joint_positions,\r\n            self.joint_velocities,\r\n            self.imu_data,\r\n            self.force_torque_data\r\n        )\r\n\r\n        # Calculate control commands based on current mode\r\n        control_commands = self.compute_control_commands(current_state)\r\n\r\n        # Publish control commands\r\n        self.publish_control_commands(control_commands)\r\n\r\n        # Update status\r\n        self.publish_status()\r\n\r\n        # Execute social behaviors if in interaction mode\r\n        if self.current_mode == 'interact':\r\n            self.social_behavior_manager.execute_behaviors(current_state)\r\n\r\n    def compute_control_commands(self, current_state):\r\n        \"\"\"Compute control commands based on current mode\"\"\"\r\n        if self.current_mode == 'balance':\r\n            return self.balance_controller.compute_balance_control(current_state)\r\n        elif self.current_mode == 'walk':\r\n            return self.locomotion_controller.compute_locomotion_control(current_state)\r\n        elif self.current_mode == 'manipulate':\r\n            return self.manipulation_controller.compute_manipulation_control(current_state)\r\n        elif self.current_mode == 'interact':\r\n            # Combine balance, manipulation, and social behaviors\r\n            balance_cmds = self.balance_controller.compute_balance_control(current_state)\r\n            manipulation_cmds = self.manipulation_controller.compute_manipulation_control(current_state)\r\n            social_cmds = self.social_behavior_manager.get_social_commands(current_state)\r\n\r\n            # Combine commands with priorities\r\n            return self.combine_commands(balance_cmds, manipulation_cmds, social_cmds)\r\n        else:  # idle mode\r\n            # Maintain current position\r\n            return self.joint_positions\r\n\r\n    def combine_commands(self, balance_cmds, manipulation_cmds, social_cmds):\r\n        \"\"\"Combine different command types with priorities\"\"\"\r\n        # This is a simplified combination - in practice, you'd use null-space projections\r\n        # or task-priority based control\r\n\r\n        combined_commands = balance_cmds.copy()\r\n\r\n        # Add manipulation commands in null space of balance\r\n        # Add social behavior commands as secondary objectives\r\n\r\n        return combined_commands\r\n\r\n    def publish_control_commands(self, commands):\r\n        \"\"\"Publish control commands to robot\"\"\"\r\n        cmd_msg = Float64MultiArray()\r\n        cmd_msg.data = commands.tolist()\r\n        self.joint_cmd_pub.publish(cmd_msg)\r\n\r\n    def publish_status(self):\r\n        \"\"\"Publish controller status\"\"\"\r\n        status_msg = String()\r\n        status_msg.data = f\"Mode: {self.current_mode}, CoM: {self.state_estimator.com_position}\"\r\n        self.status_pub.publish(status_msg)\r\n\r\n        # Publish CoM and ZMP for visualization\r\n        com_msg = Point()\r\n        com_msg.x = float(self.state_estimator.com_position[0])\r\n        com_msg.y = float(self.state_estimator.com_position[1])\r\n        com_msg.z = float(self.state_estimator.com_position[2])\r\n        self.com_pub.publish(com_msg)\r\n\r\n        zmp_msg = Point()\r\n        zmp = self.state_estimator.calculate_zmp()\r\n        zmp_msg.x = float(zmp[0])\r\n        zmp_msg.y = float(zmp[1])\r\n        zmp_msg.z = 0.0  # ZMP is on ground plane\r\n        self.zmp_pub.publish(zmp_msg)\r\n\r\n    def switch_mode(self, new_mode):\r\n        \"\"\"Switch controller mode\"\"\"\r\n        if new_mode in ['idle', 'balance', 'walk', 'manipulate', 'interact']:\r\n            old_mode = self.current_mode\r\n            self.current_mode = new_mode\r\n\r\n            # Perform mode transition actions\r\n            if old_mode == 'balance' and new_mode != 'balance':\r\n                # Transitioning out of balance mode\r\n                pass\r\n            elif new_mode == 'balance' and old_mode != 'balance':\r\n                # Transitioning into balance mode\r\n                pass\r\n\r\n            self.get_logger().info(f'Switched from {old_mode} to {new_mode}')\r\n        else:\r\n            self.get_logger().warn(f'Invalid mode: {new_mode}')\r\n\r\n    def execute_simple_task(self, task_type, **kwargs):\r\n        \"\"\"Execute a simple task\"\"\"\r\n        if task_type == 'wave':\r\n            self.switch_mode('interact')\r\n            return self.social_behavior_manager.execute_wave_behavior(**kwargs)\r\n        elif task_type == 'reach':\r\n            self.switch_mode('manipulate')\r\n            return self.manipulation_controller.execute_reach(**kwargs)\r\n        elif task_type == 'step':\r\n            self.switch_mode('balance')\r\n            return self.balance_controller.execute_recovery_step(**kwargs)\r\n        else:\r\n            self.get_logger().warn(f'Unknown task type: {task_type}')\r\n            return False\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    controller = HumanoidController()\r\n\r\n    try:\r\n        rclpy.spin(controller)\r\n    except KeyboardInterrupt:\r\n        controller.get_logger().info('Shutting down Humanoid Controller...')\r\n    finally:\r\n        controller.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-3-create-the-balance-controller",children:"Step 3: Create the Balance Controller"}),"\n",(0,o.jsxs)(n.p,{children:["Create the balance controller in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/controllers/balance_controller.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\n"""\r\nBalance Controller for Humanoid Robot\r\nImplements LIPM-based balance control with ZMP tracking\r\n"""\r\n\r\nimport numpy as np\r\nimport math\r\nfrom scipy.spatial.transform import Rotation as R\r\nimport time\r\n\r\n\r\nclass BalanceController:\r\n    def __init__(self, parent_node):\r\n        self.parent = parent_node\r\n        self.com_height = 0.8  # Center of mass height (m)\r\n        self.gravity = 9.81\r\n        self.omega = math.sqrt(self.gravity / self.com_height)\r\n\r\n        # Control gains\r\n        self.Kp_com = np.diag([100, 100, 0])  # Position gains for CoM\r\n        self.Kd_com = np.diag([20, 20, 0])    # Velocity gains for CoM\r\n        self.Kp_zmp = np.array([50, 50])      # ZMP tracking gains\r\n\r\n        # State variables\r\n        self.previous_com_error = np.zeros(2)\r\n        self.integral_com_error = np.zeros(2)\r\n        self.zmp_reference = np.zeros(2)\r\n        self.support_polygon = self.calculate_default_support_polygon()\r\n\r\n        # Walking parameters (for stepping control)\r\n        self.step_width = 0.2  # Distance between feet (m)\r\n        self.step_length = 0.3  # Step length (m)\r\n        self.is_left_support = True  # Which foot is currently supporting\r\n\r\n    def compute_balance_control(self, current_state):\r\n        """\r\n        Compute balance control commands\r\n        """\r\n        # Extract current state\r\n        current_com = current_state[\'com_position\'][:2]  # Only x,y for balance\r\n        current_com_vel = current_state[\'com_velocity\'][:2]\r\n        current_zmp = current_state[\'zmp\'][:2]\r\n        support_polygon = current_state[\'support_polygon\']\r\n\r\n        # Calculate CoM tracking error\r\n        com_error = self.zmp_reference - current_com\r\n        com_vel_error = np.zeros(2) - current_com_vel  # Desired velocity is 0\r\n\r\n        # PID control for CoM\r\n        self.integral_com_error += com_error * 0.01  # dt = 0.01s\r\n        derivative_com_error = (com_error - self.previous_com_error) / 0.01\r\n\r\n        com_control = (self.Kp_com[:2, :2] @ com_error +\r\n                      self.Kd_com[:2, :2] @ com_vel_error +\r\n                      1.0 * self.integral_com_error)  # Integral gain of 1.0\r\n\r\n        # Calculate ZMP error\r\n        zmp_error = self.zmp_reference - current_zmp\r\n        zmp_control = self.Kp_zmp * zmp_error\r\n\r\n        # Combine controls\r\n        total_control = com_control + zmp_control\r\n\r\n        # Convert to joint torques using inverse dynamics\r\n        joint_commands = self.compute_joint_commands_from_balance_control(\r\n            total_control, current_state\r\n        )\r\n\r\n        # Update state for next iteration\r\n        self.previous_com_error = com_error.copy()\r\n\r\n        return joint_commands\r\n\r\n    def compute_joint_commands_from_balance_control(self, balance_control, current_state):\r\n        """\r\n        Convert balance control signals to joint commands\r\n        """\r\n        # This is a simplified approach - in practice, use whole-body control\r\n        current_joints = current_state[\'joint_positions\']\r\n\r\n        # Calculate required joint modifications to achieve balance\r\n        # This would typically use inverse kinematics or operational space control\r\n\r\n        # For this example, adjust ankle joints for balance\r\n        ankle_adjustments = self.calculate_ankle_adjustments(balance_control, current_joints)\r\n\r\n        # Apply adjustments to current joint positions\r\n        new_joints = current_joints.copy()\r\n\r\n        # Modify ankle joints (indices are example - depend on your robot)\r\n        # Assuming ankle joints are at indices 22-23 (left) and 24-25 (right)\r\n        new_joints[22] += ankle_adjustments[0]  # Left ankle roll\r\n        new_joints[23] += ankle_adjustments[1]  # Left ankle pitch\r\n        new_joints[24] += ankle_adjustments[2]  # Right ankle roll\r\n        new_joints[25] += ankle_adjustments[3]  # Right ankle pitch\r\n\r\n        # Add hip adjustments for larger balance corrections\r\n        hip_adjustments = self.calculate_hip_adjustments(balance_control, current_joints)\r\n        new_joints[16] += hip_adjustments[0]  # Left hip roll\r\n        new_joints[17] += hip_adjustments[1]  # Left hip pitch\r\n        new_joints[18] += hip_adjustments[2]  # Left hip yaw\r\n        new_joints[26] += hip_adjustments[3]  # Right hip roll\r\n        new_joints[27] += hip_adjustments[4]  # Right hip pitch\r\n        new_joints[28] += hip_adjustments[5]  # Right hip yaw\r\n\r\n        return new_joints\r\n\r\n    def calculate_ankle_adjustments(self, balance_control, current_joints):\r\n        """\r\n        Calculate ankle joint adjustments for balance control\r\n        """\r\n        # Balance control contains [x_control, y_control]\r\n        x_control, y_control = balance_control\r\n\r\n        # Convert balance commands to ankle adjustments\r\n        # This mapping depends on your robot\'s kinematics\r\n        ankle_roll_adjustment = 0.1 * y_control  # Y control -> roll\r\n        ankle_pitch_adjustment = -0.1 * x_control  # X control -> pitch\r\n\r\n        # Return adjustments for both ankles\r\n        return np.array([\r\n            ankle_roll_adjustment,   # Left ankle roll\r\n            ankle_pitch_adjustment,  # Left ankle pitch\r\n            -ankle_roll_adjustment,  # Right ankle roll (opposite)\r\n            -ankle_pitch_adjustment  # Right ankle pitch (opposite)\r\n        ])\r\n\r\n    def calculate_hip_adjustments(self, balance_control, current_joints):\r\n        """\r\n        Calculate hip joint adjustments for larger balance corrections\r\n        """\r\n        x_control, y_control = balance_control\r\n\r\n        # For larger disturbances, use hip joints\r\n        hip_roll_adjustment = 0.05 * y_control\r\n        hip_pitch_adjustment = -0.05 * x_control\r\n        hip_yaw_adjustment = 0.02 * (x_control + y_control)  # For turning\r\n\r\n        # Return adjustments for both hips\r\n        return np.array([\r\n            hip_roll_adjustment,   # Left hip roll\r\n            hip_pitch_adjustment,  # Left hip pitch\r\n            hip_yaw_adjustment,    # Left hip yaw\r\n            -hip_roll_adjustment,  # Right hip roll (opposite)\r\n            -hip_pitch_adjustment, # Right hip pitch (opposite)\r\n            -hip_yaw_adjustment    # Right hip yaw (opposite)\r\n        ])\r\n\r\n    def update_zmp_reference(self, new_zmp_ref):\r\n        """\r\n        Update the ZMP reference trajectory\r\n        """\r\n        self.zmp_reference = new_zmp_ref\r\n\r\n    def calculate_default_support_polygon(self):\r\n        """\r\n        Calculate default support polygon based on foot positions\r\n        """\r\n        # Simplified: assume feet are 20cm apart\r\n        foot_separation = 0.2\r\n        foot_length = 0.15\r\n        foot_width = 0.08\r\n\r\n        # Support polygon vertices (simplified as rectangle)\r\n        return {\r\n            \'min_x\': -foot_length/2,\r\n            \'max_x\': foot_length/2,\r\n            \'min_y\': -foot_separation/2 - foot_width/2,\r\n            \'max_y\': foot_separation/2 + foot_width/2\r\n        }\r\n\r\n    def is_zmp_stable(self, zmp_pos, support_polygon):\r\n        """\r\n        Check if ZMP is within support polygon\r\n        """\r\n        return (support_polygon[\'min_x\'] <= zmp_pos[0] <= support_polygon[\'max_x\'] and\r\n                support_polygon[\'min_y\'] <= zmp_pos[1] <= support_polygon[\'max_y\'])\r\n\r\n    def execute_recovery_step(self, current_state, direction=\'forward\'):\r\n        """\r\n        Execute a recovery step to regain balance\r\n        """\r\n        # Calculate capture point\r\n        current_com = current_state[\'com_position\'][:2]\r\n        current_com_vel = current_state[\'com_velocity\'][:2]\r\n\r\n        capture_point = current_com + current_com_vel / self.omega\r\n\r\n        # Determine step location based on direction\r\n        step_offset = np.array([0.0, 0.0])\r\n        if direction == \'forward\':\r\n            step_offset[0] = 0.1  # Step forward 10cm\r\n        elif direction == \'backward\':\r\n            step_offset[0] = -0.1  # Step backward 10cm\r\n        elif direction == \'left\':\r\n            step_offset[1] = 0.1  # Step left 10cm\r\n        elif direction == \'right\':\r\n            step_offset[1] = -0.1  # Step right 10cm\r\n\r\n        # Target step location\r\n        step_target = capture_point + step_offset\r\n\r\n        # Execute step using manipulation controller\r\n        # This is a simplified approach - in practice, use stepping controller\r\n        self.parent.manipulation_controller.execute_foot_placement(step_target)\r\n\r\n        # Update support polygon\r\n        self.update_support_polygon_after_step(step_target)\r\n\r\n        return True\r\n\r\n    def update_support_polygon_after_step(self, new_foot_pos):\r\n        """\r\n        Update support polygon after a step is taken\r\n        """\r\n        # This would update the support polygon based on new foot placement\r\n        # For now, just update the reference ZMP to the new foot location\r\n        self.zmp_reference = new_foot_pos\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-4-create-the-manipulation-controller",children:"Step 4: Create the Manipulation Controller"}),"\n",(0,o.jsxs)(n.p,{children:["Create the manipulation controller in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/controllers/manipulation_controller.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\n"""\r\nManipulation Controller for Humanoid Robot\r\nHandles arm control, grasping, and object manipulation\r\n"""\r\n\r\nimport numpy as np\r\nimport math\r\nfrom scipy.spatial.transform import Rotation as R\r\nimport time\r\n\r\n\r\nclass ManipulationController:\r\n    def __init__(self, parent_node):\r\n        self.parent = parent_node\r\n\r\n        # Arm configuration\r\n        self.arm_dof = 7  # 7 DOF arms\r\n        self.left_arm_indices = slice(7, 14)   # Joints 7-13 for left arm\r\n        self.right_arm_indices = slice(14, 21) # Joints 14-20 for right arm\r\n\r\n        # Control parameters\r\n        self.kp_pos = 100.0  # Position gain\r\n        self.kd_pos = 20.0   # Velocity gain\r\n        self.kp_ori = 10.0   # Orientation gain\r\n        self.kd_ori = 5.0    # Angular velocity gain\r\n\r\n        # Jacobian and inverse kinematics parameters\r\n        self.ik_lambda = 0.01  # Damping factor for Jacobian pseudoinverse\r\n        self.ik_max_iter = 100\r\n        self.ik_tolerance = 1e-4\r\n\r\n        # State variables\r\n        self.left_ee_pose = np.eye(4)  # Left end-effector pose\r\n        self.right_ee_pose = np.eye(4)  # Right end-effector pose\r\n        self.left_ee_vel = np.zeros(6)  # Left end-effector velocity\r\n        self.right_ee_vel = np.zeros(6)  # Right end-effector velocity\r\n\r\n        # Task state\r\n        self.active_left_task = None\r\n        self.active_right_task = None\r\n\r\n    def compute_manipulation_control(self, current_state):\r\n        """\r\n        Compute manipulation control commands\r\n        """\r\n        current_joints = current_state[\'joint_positions\']\r\n        current_velocities = current_state[\'joint_velocities\']\r\n\r\n        # Calculate desired joint positions for manipulation tasks\r\n        left_arm_cmd = self.compute_left_arm_control(current_state)\r\n        right_arm_cmd = self.compute_right_arm_control(current_state)\r\n\r\n        # Combine with other joints\r\n        new_joints = current_joints.copy()\r\n        new_joints[self.left_arm_indices] = left_arm_cmd\r\n        new_joints[self.right_arm_indices] = right_arm_cmd\r\n\r\n        return new_joints\r\n\r\n    def compute_left_arm_control(self, current_state):\r\n        """\r\n        Compute control for left arm\r\n        """\r\n        if self.active_left_task is None:\r\n            # Return current position (no active task)\r\n            return current_state[\'joint_positions\'][self.left_arm_indices]\r\n\r\n        task = self.active_left_task\r\n        current_joints = current_state[\'joint_positions\'][self.left_arm_indices]\r\n\r\n        if task[\'type\'] == \'reach\':\r\n            return self.ik_reach_target(\r\n                current_joints,\r\n                task[\'target_position\'],\r\n                task[\'target_orientation\']\r\n            )\r\n        elif task[\'type\'] == \'grasp\':\r\n            return self.execute_grasp_task(current_joints, task)\r\n        elif task[\'type\'] == \'hold\':\r\n            return self.hold_object(current_joints, task)\r\n        else:\r\n            return current_joints  # No active task\r\n\r\n    def compute_right_arm_control(self, current_state):\r\n        """\r\n        Compute control for right arm\r\n        """\r\n        if self.active_right_task is None:\r\n            # Return current position (no active task)\r\n            return current_state[\'joint_positions\'][self.right_arm_indices]\r\n\r\n        task = self.active_right_task\r\n        current_joints = current_state[\'joint_positions\'][self.right_arm_indices]\r\n\r\n        if task[\'type\'] == \'reach\':\r\n            return self.ik_reach_target(\r\n                current_joints,\r\n                task[\'target_position\'],\r\n                task[\'target_orientation\']\r\n            )\r\n        elif task[\'type\'] == \'grasp\':\r\n            return self.execute_grasp_task(current_joints, task)\r\n        elif task[\'type\'] == \'hold\':\r\n            return self.hold_object(current_joints, task)\r\n        else:\r\n            return current_joints  # No active task\r\n\r\n    def ik_reach_target(self, current_joints, target_pos, target_ori=None):\r\n        """\r\n        Inverse kinematics to reach target position and orientation\r\n        """\r\n        if target_ori is None:\r\n            # Use current orientation if not specified\r\n            current_ee_pose = self.forward_kinematics(current_joints)\r\n            target_ori = current_ee_pose[:3, :3]\r\n\r\n        target_pose = np.eye(4)\r\n        target_pose[:3, 3] = target_pos\r\n        target_pose[:3, :3] = target_ori if isinstance(target_ori, np.ndarray) else R.from_quat(target_ori).as_matrix()\r\n\r\n        # Use iterative inverse kinematics\r\n        new_joints = self.iterative_ik(current_joints, target_pose)\r\n\r\n        return new_joints\r\n\r\n    def iterative_ik(self, current_joints, target_pose):\r\n        """\r\n        Iterative inverse kinematics using Jacobian transpose/pseudoinverse\r\n        """\r\n        current_joints = current_joints.copy()\r\n\r\n        for iteration in range(self.ik_max_iter):\r\n            # Calculate current end-effector pose\r\n            current_pose = self.forward_kinematics(current_joints)\r\n\r\n            # Calculate error\r\n            pos_error = target_pose[:3, 3] - current_pose[:3, 3]\r\n            ori_error = self.rotation_error(current_pose[:3, :3], target_pose[:3, :3])\r\n\r\n            # Check convergence\r\n            if np.linalg.norm(pos_error) < self.ik_tolerance and np.linalg.norm(ori_error) < self.ik_tolerance:\r\n                break\r\n\r\n            # Calculate Jacobian\r\n            jacobian = self.calculate_jacobian(current_joints)\r\n\r\n            # Combine position and orientation errors\r\n            error = np.concatenate([pos_error, ori_error])\r\n\r\n            # Calculate joint updates using damped least squares\r\n            I = np.eye(len(current_joints))\r\n            jtj_lambda = jacobian.T @ jacobian + self.ik_lambda * I\r\n            joint_delta = np.linalg.solve(jtj_lambda, jacobian.T @ error)\r\n\r\n            # Apply updates\r\n            current_joints += 0.5 * joint_delta  # 0.5 for stability\r\n\r\n            # Apply joint limits\r\n            current_joints = self.apply_joint_limits(current_joints)\r\n\r\n        return current_joints\r\n\r\n    def forward_kinematics(self, joint_angles):\r\n        """\r\n        Simplified forward kinematics - in practice, use robot-specific FK\r\n        """\r\n        # This is a placeholder - implement robot-specific forward kinematics\r\n        # For this example, return a simple transformation\r\n        pose = np.eye(4)\r\n\r\n        # Simplified FK based on joint angles\r\n        # In practice, use DH parameters or other kinematic model\r\n        x = 0.3 + 0.1 * math.sin(joint_angles[0])\r\n        y = 0.2 + 0.1 * math.cos(joint_angles[1])\r\n        z = 1.0 + 0.1 * math.sin(joint_angles[2])\r\n\r\n        pose[0, 3] = x\r\n        pose[1, 3] = y\r\n        pose[2, 3] = z\r\n\r\n        # Simple orientation (identity for now)\r\n        return pose\r\n\r\n    def calculate_jacobian(self, joint_angles):\r\n        """\r\n        Calculate geometric Jacobian - in practice, use robot-specific method\r\n        """\r\n        # This is a simplified Jacobian calculation\r\n        # In practice, use analytical or numerical differentiation\r\n        n_joints = len(joint_angles)\r\n        jacobian = np.zeros((6, n_joints))  # 6 DOF (pos + ori)\r\n\r\n        # Simplified Jacobian - in practice, calculate properly based on kinematics\r\n        for i in range(n_joints):\r\n            # Position part of Jacobian\r\n            jacobian[0:3, i] = self.calculate_position_jacobian_column(i, joint_angles)\r\n            # Orientation part of Jacobian\r\n            jacobian[3:6, i] = self.calculate_orientation_jacobian_column(i, joint_angles)\r\n\r\n        return jacobian\r\n\r\n    def calculate_position_jacobian_column(self, joint_idx, joint_angles):\r\n        """\r\n        Calculate position part of Jacobian column\r\n        """\r\n        # Simplified calculation - in practice, use proper kinematic derivation\r\n        return np.array([0.1, 0.1, 0.1])  # Placeholder\r\n\r\n    def calculate_orientation_jacobian_column(self, joint_idx, joint_angles):\r\n        """\r\n        Calculate orientation part of Jacobian column\r\n        """\r\n        # Simplified calculation - in practice, use proper kinematic derivation\r\n        return np.array([0.01, 0.01, 0.01])  # Placeholder\r\n\r\n    def rotation_error(self, current_rotation, target_rotation):\r\n        """\r\n        Calculate rotational error as angle-axis representation\r\n        """\r\n        relative_rotation = target_rotation @ current_rotation.T\r\n        rotation_vector = R.from_matrix(relative_rotation).as_rotvec()\r\n        return rotation_vector\r\n\r\n    def apply_joint_limits(self, joints):\r\n        """\r\n        Apply joint limits to joint angles\r\n        """\r\n        # Example joint limits (these should match your robot)\r\n        min_limits = np.array([-2.0] * len(joints))\r\n        max_limits = np.array([2.0] * len(joints))\r\n\r\n        return np.clip(joints, min_limits, max_limits)\r\n\r\n    def execute_grasp_task(self, current_joints, task):\r\n        """\r\n        Execute grasp task\r\n        """\r\n        # Move to pre-grasp position\r\n        pre_grasp_pos = task[\'object_position\'] + np.array([0, 0, 0.1])  # 10cm above object\r\n        pre_grasp_ori = task[\'grasp_orientation\']\r\n\r\n        # Move to pre-grasp\r\n        joints = self.ik_reach_target(current_joints, pre_grasp_pos, pre_grasp_ori)\r\n\r\n        # Move down to object\r\n        grasp_pos = task[\'object_position\']\r\n        joints = self.ik_reach_target(joints, grasp_pos, pre_grasp_ori)\r\n\r\n        # Close gripper (simplified)\r\n        # In practice, control gripper separately\r\n        joints = self.execute_gripper_control(joints, \'close\')\r\n\r\n        # Lift object\r\n        lift_pos = grasp_pos + np.array([0, 0, 0.1])  # Lift 10cm\r\n        joints = self.ik_reach_target(joints, lift_pos, pre_grasp_ori)\r\n\r\n        return joints\r\n\r\n    def execute_gripper_control(self, joints, command):\r\n        """\r\n        Control gripper (simplified)\r\n        """\r\n        # In practice, this would control actual gripper joints\r\n        # For this example, we\'ll just return the joints unchanged\r\n        return joints\r\n\r\n    def hold_object(self, current_joints, task):\r\n        """\r\n        Hold object at specified location\r\n        """\r\n        # Maintain grasp while possibly moving to new location\r\n        target_pos = task.get(\'hold_position\', self.get_current_ee_position(current_joints))\r\n        target_ori = task.get(\'hold_orientation\', self.get_current_ee_orientation(current_joints))\r\n\r\n        return self.ik_reach_target(current_joints, target_pos, target_ori)\r\n\r\n    def get_current_ee_position(self, joints):\r\n        """\r\n        Get current end-effector position\r\n        """\r\n        pose = self.forward_kinematics(joints)\r\n        return pose[:3, 3]\r\n\r\n    def get_current_ee_orientation(self, joints):\r\n        """\r\n        Get current end-effector orientation\r\n        """\r\n        pose = self.forward_kinematics(joints)\r\n        return pose[:3, :3]\r\n\r\n    def execute_reach(self, target_position, arm=\'right\', orientation=None):\r\n        """\r\n        Execute reach task with specified arm\r\n        """\r\n        task = {\r\n            \'type\': \'reach\',\r\n            \'target_position\': np.array(target_position),\r\n            \'target_orientation\': orientation\r\n        }\r\n\r\n        if arm == \'left\':\r\n            self.active_left_task = task\r\n        else:  # right\r\n            self.active_right_task = task\r\n\r\n    def execute_grasp(self, object_position, grasp_type=\'top\', arm=\'right\'):\r\n        """\r\n        Execute grasp task\r\n        """\r\n        # Determine grasp orientation based on grasp type\r\n        if grasp_type == \'top\':\r\n            grasp_orientation = R.from_euler(\'xyz\', [0, 0, 0]).as_matrix()  # Grasp from top\r\n        elif grasp_type == \'side\':\r\n            grasp_orientation = R.from_euler(\'xyz\', [0, np.pi/2, 0]).as_matrix()  # Grasp from side\r\n        else:\r\n            grasp_orientation = R.from_euler(\'xyz\', [0, 0, 0]).as_matrix()  # Default\r\n\r\n        task = {\r\n            \'type\': \'grasp\',\r\n            \'object_position\': np.array(object_position),\r\n            \'grasp_orientation\': grasp_orientation\r\n        }\r\n\r\n        if arm == \'left\':\r\n            self.active_left_task = task\r\n        else:  # right\r\n            self.active_right_task = task\r\n\r\n    def execute_hold(self, hold_position, arm=\'right\'):\r\n        """\r\n        Execute hold task\r\n        """\r\n        task = {\r\n            \'type\': \'hold\',\r\n            \'hold_position\': np.array(hold_position)\r\n        }\r\n\r\n        if arm == \'left\':\r\n            self.active_left_task = task\r\n        else:  # right\r\n            self.active_right_task = task\r\n\r\n    def execute_foot_placement(self, target_position):\r\n        """\r\n        Execute foot placement for stepping (simplified)\r\n        """\r\n        # This would coordinate with balance controller for stepping\r\n        # For this example, just return True\r\n        return True\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-5-create-the-state-estimator",children:"Step 5: Create the State Estimator"}),"\n",(0,o.jsxs)(n.p,{children:["Create the state estimator in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/sensors/state_estimator.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\n"""\r\nState Estimator for Humanoid Robot\r\nEstimates robot state including CoM, ZMP, joint states, etc.\r\n"""\r\n\r\nimport numpy as np\r\nimport math\r\nfrom scipy.spatial.transform import Rotation as R\r\nfrom collections import deque\r\nimport time\r\n\r\n\r\nclass StateEstimator:\r\n    def __init__(self, parent_node):\r\n        self.parent = parent_node\r\n\r\n        # State variables\r\n        self.com_position = np.array([0.0, 0.0, 0.8])  # Initial CoM at 80cm height\r\n        self.com_velocity = np.zeros(3)\r\n        self.com_acceleration = np.zeros(3)\r\n\r\n        self.zmp = np.zeros(3)  # Zero Moment Point\r\n        self.support_polygon = self.calculate_initial_support_polygon()\r\n\r\n        self.foot_positions = {\r\n            \'left\': np.array([0.0, 0.1, 0.0]),   # Left foot position\r\n            \'right\': np.array([0.0, -0.1, 0.0])  # Right foot position\r\n        }\r\n\r\n        # Estimation parameters\r\n        self.com_height = 0.8  # Fixed CoM height assumption for LIPM\r\n        self.gravity = 9.81\r\n        self.omega = math.sqrt(self.gravity / self.com_height)\r\n\r\n        # Filtering parameters\r\n        self.com_velocity_filter = self.initialize_filter(3, 5)  # 3D, 5 taps\r\n        self.com_position_history = deque(maxlen=10)  # Keep last 10 positions\r\n\r\n        # Sensor fusion weights\r\n        self.kf_process_noise = np.diag([0.01, 0.01, 0.1])  # Process noise\r\n        self.kf_measurement_noise = np.diag([0.001, 0.001, 0.01])  # Measurement noise\r\n\r\n    def estimate_state(self, joint_positions, joint_velocities, imu_data, ft_data):\r\n        """\r\n        Estimate robot state from sensor data\r\n        """\r\n        # Update CoM position and velocity\r\n        self.update_com_state(joint_positions)\r\n\r\n        # Update ZMP from force/torque sensors\r\n        self.update_zmp(ft_data)\r\n\r\n        # Update support polygon\r\n        self.update_support_polygon()\r\n\r\n        # Update foot positions\r\n        self.update_foot_positions(joint_positions)\r\n\r\n        # Package state for controller\r\n        state = {\r\n            \'com_position\': self.com_position.copy(),\r\n            \'com_velocity\': self.com_velocity.copy(),\r\n            \'com_acceleration\': self.com_acceleration.copy(),\r\n            \'zmp\': self.zmp.copy(),\r\n            \'support_polygon\': self.support_polygon.copy(),\r\n            \'foot_positions\': self.foot_positions.copy(),\r\n            \'joint_positions\': joint_positions,\r\n            \'joint_velocities\': joint_velocities,\r\n            \'imu_data\': imu_data,\r\n            \'force_torque_data\': ft_data\r\n        }\r\n\r\n        return state\r\n\r\n    def update_com_state(self, joint_positions):\r\n        """\r\n        Update CoM position and velocity from joint positions\r\n        """\r\n        # Calculate CoM position from joint configuration\r\n        # This is a simplified approach - in practice, use full kinematic model\r\n        new_com_pos = self.calculate_com_from_joints(joint_positions)\r\n\r\n        # Update position history\r\n        self.com_position_history.append(new_com_pos)\r\n\r\n        # Calculate velocity from position differences\r\n        if len(self.com_position_history) >= 2:\r\n            dt = 0.01  # Assuming 100Hz control\r\n            pos_diff = new_com_pos - self.com_position_history[-2]\r\n            new_com_vel = pos_diff / dt\r\n\r\n            # Apply simple filtering to velocity\r\n            if np.linalg.norm(self.com_velocity) > 0:\r\n                self.com_velocity = 0.7 * self.com_velocity + 0.3 * new_com_vel\r\n            else:\r\n                self.com_velocity = new_com_vel\r\n\r\n        # Update CoM position (apply some smoothing)\r\n        alpha = 0.9  # Smoothing factor\r\n        self.com_position = alpha * self.com_position + (1 - alpha) * new_com_pos\r\n\r\n    def calculate_com_from_joints(self, joint_positions):\r\n        """\r\n        Calculate CoM from joint positions using simplified model\r\n        """\r\n        # This would use the full kinematic model in practice\r\n        # For this example, use a simplified calculation\r\n\r\n        # Simplified CoM calculation based on joint positions\r\n        # In practice, use mass distribution and full forward kinematics\r\n        base_com = np.array([0.0, 0.0, 0.8])  # Base CoM position\r\n\r\n        # Add influence from joint positions\r\n        # This is a very simplified model\r\n        joint_influence = np.zeros(3)\r\n\r\n        # Influence from leg joints (affects x,y,z)\r\n        leg_joints = joint_positions[21:29]  # Assuming leg joints are at indices 21-28\r\n        for i, joint in enumerate(leg_joints):\r\n            # Each leg joint has some influence on CoM position\r\n            influence = 0.01 * math.sin(joint)  # Simplified influence\r\n            joint_influence[0] += influence * ((i % 3) == 0)  # x influence\r\n            joint_influence[1] += influence * ((i % 3) == 1)  # y influence\r\n            joint_influence[2] += influence * ((i % 3) == 2)  # z influence\r\n\r\n        return base_com + joint_influence\r\n\r\n    def update_zmp(self, ft_data):\r\n        """\r\n        Update Zero Moment Point from force/torque sensors\r\n        """\r\n        # Calculate ZMP from foot force/torque measurements\r\n        # ZMP_x = (M_y + F_z * h) / F_x  (simplified)\r\n        # ZMP_y = (-M_x + F_z * h) / F_y (simplified)\r\n\r\n        # Get forces from both feet\r\n        left_force = np.array(ft_data[\'left_foot\'])\r\n        right_force = np.array(ft_data[\'right_foot\'])\r\n\r\n        # Calculate total forces and moments\r\n        total_force = left_force + right_force\r\n\r\n        # Simplified ZMP calculation\r\n        # In practice, use full moment calculations from both feet\r\n        if total_force[2] != 0:  # Fz should not be zero\r\n            zmp_x = -(left_force[4] + right_force[4]) / total_force[2]  # Moment_y / Force_z\r\n            zmp_y = (left_force[3] + right_force[3]) / total_force[2]   # Moment_x / Force_z\r\n        else:\r\n            zmp_x, zmp_y = 0, 0  # Default to origin if no vertical force\r\n\r\n        self.zmp = np.array([zmp_x, zmp_y, 0.0])\r\n\r\n    def update_support_polygon(self):\r\n        """\r\n        Update support polygon based on foot contact\r\n        """\r\n        # Calculate support polygon from foot positions\r\n        # This is simplified - in practice, consider foot geometry and contact points\r\n\r\n        left_pos = self.foot_positions[\'left\']\r\n        right_pos = self.foot_positions[\'right\']\r\n\r\n        # Create bounding box as support polygon\r\n        min_x = min(left_pos[0], right_pos[0]) - 0.05  # Add small margin\r\n        max_x = max(left_pos[0], right_pos[0]) + 0.05\r\n        min_y = min(left_pos[1], right_pos[1]) - 0.1   # Larger margin in y\r\n        max_y = max(left_pos[1], right_pos[1]) + 0.1\r\n\r\n        self.support_polygon = {\r\n            \'min_x\': min_x,\r\n            \'max_x\': max_x,\r\n            \'min_y\': min_y,\r\n            \'max_y\': max_y\r\n        }\r\n\r\n    def update_foot_positions(self, joint_positions):\r\n        """\r\n        Update foot positions from joint configuration\r\n        """\r\n        # Calculate foot positions using forward kinematics\r\n        # This is simplified - in practice, use full FK\r\n        self.foot_positions[\'left\'] = self.calculate_foot_position(joint_positions, \'left\')\r\n        self.foot_positions[\'right\'] = self.calculate_foot_position(joint_positions, \'right\')\r\n\r\n    def calculate_foot_position(self, joint_positions, foot_side):\r\n        """\r\n        Calculate foot position using simplified forward kinematics\r\n        """\r\n        # This would use full robot kinematics in practice\r\n        # For this example, use a simplified model\r\n\r\n        if foot_side == \'left\':\r\n            # Use left leg joint positions (indices are example)\r\n            leg_joints = joint_positions[21:27]  # Assuming left leg joints are 21-26\r\n        else:  # right\r\n            leg_joints = joint_positions[27:33]  # Assuming right leg joints are 27-32\r\n\r\n        # Simplified calculation of foot position based on leg joints\r\n        # In practice, use full DH parameters or kinematic model\r\n        foot_pos = np.array([0.0, 0.1 if foot_side == \'left\' else -0.1, 0.0])\r\n\r\n        # Add influence from joint angles\r\n        for i, angle in enumerate(leg_joints):\r\n            foot_pos[0] += 0.05 * math.sin(angle + i * 0.5)  # Simplified influence\r\n            foot_pos[1] += 0.02 * math.cos(angle + i * 0.3)  # Simplified influence\r\n            foot_pos[2] -= 0.08  # Foot is below hip (negative z)\r\n\r\n        return foot_pos\r\n\r\n    def calculate_initial_support_polygon(self):\r\n        """\r\n        Calculate initial support polygon\r\n        """\r\n        return {\r\n            \'min_x\': -0.1,\r\n            \'max_x\': 0.1,\r\n            \'min_y\': -0.2,\r\n            \'max_y\': 0.2\r\n        }\r\n\r\n    def initialize_filter(self, state_dim, taps):\r\n        """\r\n        Initialize filter for state estimation\r\n        """\r\n        return np.ones(taps) / taps  # Simple moving average\r\n\r\n    def is_balanced(self):\r\n        """\r\n        Check if robot is balanced based on CoM and ZMP\r\n        """\r\n        # Check if ZMP is within support polygon\r\n        zmp_in_polygon = (self.support_polygon[\'min_x\'] <= self.zmp[0] <= self.support_polygon[\'max_x\'] and\r\n                         self.support_polygon[\'min_y\'] <= self.zmp[1] <= self.support_polygon[\'max_y\'])\r\n\r\n        # Check CoM position relative to feet\r\n        com_stable = abs(self.com_position[0]) < 0.1 and abs(self.com_position[1]) < 0.15\r\n\r\n        return zmp_in_polygon and com_stable\r\n\r\n    def calculate_capture_point(self):\r\n        """\r\n        Calculate capture point for balance recovery\r\n        """\r\n        # Capture point: where to step to stop the CoM\r\n        capture_point = self.com_position[:2] + self.com_velocity[:2] / self.omega\r\n        return capture_point\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-6-create-social-behavior-manager",children:"Step 6: Create Social Behavior Manager"}),"\n",(0,o.jsxs)(n.p,{children:["Create the social behavior manager in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/behaviors/social_behavior.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\n"""\r\nSocial Behavior Manager for Humanoid Robot\r\nManages social interactions, expressions, and communication\r\n"""\r\n\r\nimport numpy as np\r\nimport math\r\nimport time\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\n\r\nclass SocialBehaviorManager:\r\n    def __init__(self, parent_node):\r\n        self.parent = parent_node\r\n\r\n        # Social behavior parameters\r\n        self.social_space_radius = 1.2  # Personal space radius (m)\r\n        self.greeting_distance = 1.0    # Distance for greeting (m)\r\n        self.attention_span = 5.0       # Time to pay attention (seconds)\r\n\r\n        # Behavior state\r\n        self.current_behavior = \'idle\'\r\n        self.behavior_start_time = time.time()\r\n        self.attended_person = None\r\n        self.interaction_intensity = 0.5  # 0-1 scale\r\n\r\n        # Expression and gesture libraries\r\n        self.expressions = {\r\n            \'neutral\': [0, 0, 0, 0],      # [brow, eyes, mouth, cheeks]\r\n            \'happy\': [0, 1, 1, 1],        # Raised brows, smiling eyes, smile, raised cheeks\r\n            \'sad\': [-1, -1, -1, 0],       # Lowered brows, sad eyes, frown, neutral cheeks\r\n            \'surprised\': [1, 1, 0, 0],    # Raised brows, wide eyes, neutral mouth\r\n            \'attentive\': [0, 0.5, 0, 0]   # Slightly raised brows for attention\r\n        }\r\n\r\n        self.gestures = {\r\n            \'wave\': self.wave_gesture,\r\n            \'point\': self.point_gesture,\r\n            \'nod\': self.nod_gesture,\r\n            \'shake_head\': self.shake_head_gesture,\r\n            \'beckon\': self.beckon_gesture\r\n        }\r\n\r\n    def execute_behaviors(self, current_state):\r\n        """\r\n        Execute social behaviors based on current state\r\n        """\r\n        # Check if there are humans nearby\r\n        humans_nearby = self.detect_humans_in_field_of_view(current_state)\r\n\r\n        if humans_nearby:\r\n            # Select appropriate behavior based on context\r\n            self.select_and_execute_behavior(humans_nearby, current_state)\r\n        else:\r\n            # Return to idle behavior\r\n            self.current_behavior = \'idle\'\r\n            self.set_expression(\'neutral\')\r\n\r\n    def detect_humans_in_field_of_view(self, current_state):\r\n        """\r\n        Detect humans in robot\'s field of view (simplified)\r\n        """\r\n        # This would use vision processing in practice\r\n        # For this example, return a mock detection\r\n        camera_image = self.parent.camera_image\r\n        if camera_image is not None:\r\n            # Mock detection - in practice, use object detection\r\n            return [{\'position\': np.array([1.0, 0.0, 0.0]), \'distance\': 1.0}]\r\n        return []\r\n\r\n    def select_and_execute_behavior(self, humans, current_state):\r\n        """\r\n        Select and execute appropriate social behavior\r\n        """\r\n        closest_human = min(humans, key=lambda h: h[\'distance\'])\r\n        distance = closest_human[\'distance\']\r\n\r\n        if distance <= self.greeting_distance and self.current_behavior != \'greeting\':\r\n            # Close enough for greeting\r\n            self.execute_greeting_behavior(closest_human)\r\n        elif distance <= self.social_space_radius:\r\n            # In personal space, be attentive\r\n            self.maintain_attention_behavior(closest_human)\r\n        else:\r\n            # Outside personal space, acknowledge presence\r\n            self.acknowledge_presence_behavior(closest_human)\r\n\r\n    def execute_greeting_behavior(self, human):\r\n        """\r\n        Execute greeting behavior\r\n        """\r\n        if self.current_behavior != \'greeting\':\r\n            self.current_behavior = \'greeting\'\r\n            self.behavior_start_time = time.time()\r\n\r\n            # Wave gesture\r\n            self.execute_wave_behavior(target=human[\'position\'])\r\n\r\n            # Friendly expression\r\n            self.set_expression(\'happy\')\r\n\r\n            # Say greeting\r\n            self.speak_greeting()\r\n\r\n    def maintain_attention_behavior(self, human):\r\n        """\r\n        Maintain attention to nearby human\r\n        """\r\n        if self.current_behavior != \'attentive\':\r\n            self.current_behavior = \'attentive\'\r\n            self.behavior_start_time = time.time()\r\n\r\n            # Attentive expression\r\n            self.set_expression(\'attentive\')\r\n\r\n            # Maintain gaze\r\n            self.maintain_gaze_on_human(human)\r\n\r\n    def acknowledge_presence_behavior(self, human):\r\n        """\r\n        Acknowledge human presence from distance\r\n        """\r\n        if self.current_behavior != \'acknowledge\':\r\n            self.current_behavior = \'acknowledge\'\r\n            self.behavior_start_time = time.time()\r\n\r\n            # Brief nod gesture\r\n            self.execute_nod_behavior()\r\n\r\n            # Neutral expression\r\n            self.set_expression(\'neutral\')\r\n\r\n    def execute_wave_behavior(self, target=None, duration=2.0):\r\n        """\r\n        Execute waving gesture\r\n        """\r\n        # Plan wave trajectory\r\n        wave_trajectory = self.plan_wave_trajectory(target)\r\n\r\n        # Execute wave motion\r\n        start_time = time.time()\r\n        while time.time() - start_time < duration:\r\n            # Follow wave trajectory\r\n            self.follow_arm_trajectory(\'right\', wave_trajectory)\r\n            time.sleep(0.01)  # 100Hz\r\n\r\n    def plan_wave_trajectory(self, target=None):\r\n        """\r\n        Plan waving trajectory\r\n        """\r\n        # Simplified wave trajectory\r\n        trajectory = []\r\n        amplitude = 0.1  # 10cm amplitude\r\n\r\n        for t in np.linspace(0, 2*np.pi, 20):  # 20 points for wave\r\n            x_offset = 0.3  # Extend arm\r\n            y_offset = amplitude * math.sin(t)  # Vertical oscillation\r\n            z_offset = 0.05 * math.cos(t)  # Forward-back oscillation\r\n\r\n            position = np.array([x_offset, y_offset, z_offset])\r\n            trajectory.append(position)\r\n\r\n        return trajectory\r\n\r\n    def execute_nod_behavior(self, count=1):\r\n        """\r\n        Execute nodding gesture\r\n        """\r\n        for i in range(count):\r\n            # Nod down\r\n            self.move_head([0.2, 0, 0])  # Pitch down\r\n            time.sleep(0.3)\r\n\r\n            # Nod up\r\n            self.move_head([0, 0, 0])  # Return to neutral\r\n            time.sleep(0.3)\r\n\r\n    def set_expression(self, expression_name):\r\n        """\r\n        Set facial expression\r\n        """\r\n        if expression_name in self.expressions:\r\n            expression_values = self.expressions[expression_name]\r\n            # In practice, send these values to facial actuation system\r\n            print(f"Setting expression: {expression_name} with values {expression_values}")\r\n\r\n    def speak_greeting(self):\r\n        """\r\n        Speak greeting message\r\n        """\r\n        greeting_messages = [\r\n            "Hello! Nice to meet you!",\r\n            "Hi there! How can I help you?",\r\n            "Greetings! I\'m your humanoid assistant."\r\n        ]\r\n\r\n        # In practice, use text-to-speech system\r\n        import random\r\n        message = random.choice(greeting_messages)\r\n        print(f"Speaking: {message}")\r\n\r\n    def maintain_gaze_on_human(self, human):\r\n        """\r\n        Maintain gaze on human\r\n        """\r\n        # Calculate where to look\r\n        human_pos = human[\'position\']\r\n\r\n        # In practice, control head/eye movements\r\n        print(f"Maintaining gaze on human at {human_pos}")\r\n\r\n    def move_head(self, joint_angles):\r\n        """\r\n        Move head to specified joint angles\r\n        """\r\n        # In practice, send commands to head joints\r\n        print(f"Moving head to angles: {joint_angles}")\r\n\r\n    def follow_arm_trajectory(self, arm_side, trajectory):\r\n        """\r\n        Follow arm trajectory\r\n        """\r\n        # In practice, use trajectory controller\r\n        print(f"Following {arm_side} arm trajectory")\r\n\r\n    def get_social_commands(self, current_state):\r\n        """\r\n        Get social behavior commands to combine with other controllers\r\n        """\r\n        # Return commands that complement balance and manipulation\r\n        social_commands = {\r\n            \'head_orientation\': self.get_head_orientation_command(),\r\n            \'facial_expression\': self.get_current_expression(),\r\n            \'gesture_commands\': self.get_active_gesture_commands()\r\n        }\r\n\r\n        return social_commands\r\n\r\n    def get_head_orientation_command(self):\r\n        """\r\n        Get head orientation command for social interaction\r\n        """\r\n        # If attending to someone, orient head toward them\r\n        if self.attended_person:\r\n            # Calculate direction to attended person\r\n            direction_to_person = self.attended_person[\'position\'] - self.parent.state_estimator.com_position\r\n            direction_xy = direction_to_person[:2]  # Only x,y for head orientation\r\n\r\n            if np.linalg.norm(direction_xy) > 0.1:  # Avoid division by zero\r\n                direction_xy = direction_xy / np.linalg.norm(direction_xy)\r\n\r\n                # Convert to head joint commands\r\n                # This would map to actual head joint angles in practice\r\n                head_yaw = math.atan2(direction_xy[1], direction_xy[0])\r\n                head_pitch = 0  # Keep level for now\r\n\r\n                return {\'yaw\': head_yaw, \'pitch\': head_pitch}\r\n\r\n        # Default: look forward\r\n        return {\'yaw\': 0, \'pitch\': 0}\r\n\r\n    def get_current_expression(self):\r\n        """\r\n        Get current facial expression\r\n        """\r\n        return self.current_behavior\r\n\r\n    def get_active_gesture_commands(self):\r\n        """\r\n        Get commands for any active gestures\r\n        """\r\n        # Return any ongoing gesture commands\r\n        return []\r\n\r\n    def execute_social_interaction(self, interaction_type, target_person=None):\r\n        """\r\n        Execute specific type of social interaction\r\n        """\r\n        if interaction_type == \'greeting\':\r\n            self.execute_greeting_interaction(target_person)\r\n        elif interaction_type == \'farewell\':\r\n            self.execute_farewell_interaction(target_person)\r\n        elif interaction_type == \'assistance_request\':\r\n            self.execute_assistance_interaction(target_person)\r\n        elif interaction_type == \'conversation\':\r\n            self.execute_conversation_interaction(target_person)\r\n\r\n    def execute_greeting_interaction(self, target_person):\r\n        """\r\n        Execute full greeting interaction\r\n        """\r\n        print("Executing greeting interaction...")\r\n\r\n        # Move to appropriate distance\r\n        self.move_to_interaction_distance(target_person, self.greeting_distance)\r\n\r\n        # Make eye contact\r\n        self.maintain_gaze_on_human(target_person)\r\n\r\n        # Wave\r\n        self.execute_wave_behavior(target=target_person[\'position\'])\r\n\r\n        # Smile\r\n        self.set_expression(\'happy\')\r\n\r\n        # Speak\r\n        self.speak_greeting()\r\n\r\n    def move_to_interaction_distance(self, target_person, desired_distance):\r\n        """\r\n        Move to appropriate distance for interaction\r\n        """\r\n        # Calculate current distance\r\n        current_pos = self.parent.state_estimator.com_position\r\n        target_pos = target_person[\'position\']\r\n        current_distance = np.linalg.norm(target_pos[:2] - current_pos[:2])\r\n\r\n        # If too far, move closer (simplified)\r\n        if current_distance > desired_distance + 0.2:  # 20cm tolerance\r\n            print(f"Moving closer to person, current: {current_distance:.2f}m, desired: {desired_distance:.2f}m")\r\n            # In practice, use locomotion controller to move\r\n\r\n    def execute_farewell_interaction(self, target_person):\r\n        """\r\n        Execute farewell interaction\r\n        """\r\n        print("Executing farewell interaction...")\r\n\r\n        # Wave goodbye\r\n        self.execute_wave_behavior(target=target_person[\'position\'])\r\n\r\n        # Set sad/sympathetic expression\r\n        self.set_expression(\'happy\')  # Actually happy to say goodbye in a positive way\r\n\r\n        # Speak farewell\r\n        farewell_messages = [\r\n            "Goodbye! Have a great day!",\r\n            "See you later!",\r\n            "Take care!"\r\n        ]\r\n        import random\r\n        message = random.choice(farewell_messages)\r\n        print(f"Speaking: {message}")\r\n\r\n    def execute_assistance_interaction(self, target_person):\r\n        """\r\n        Execute interaction for requesting/providing assistance\r\n        """\r\n        print("Executing assistance interaction...")\r\n\r\n        # Lean forward slightly to show attention\r\n        # In practice, adjust posture\r\n\r\n        # Make direct eye contact\r\n        self.maintain_gaze_on_human(target_person)\r\n\r\n        # Set attentive expression\r\n        self.set_expression(\'attentive\')\r\n\r\n        # Speak assistance message\r\n        assistance_messages = [\r\n            "How can I assist you?",\r\n            "I\'m here to help. What do you need?",\r\n            "Please let me know if you need any help."\r\n        ]\r\n        import random\r\n        message = random.choice(assistance_messages)\r\n        print(f"Speaking: {message}")\r\n\r\n    def execute_conversation_interaction(self, target_person):\r\n        """\r\n        Execute conversation-like interaction\r\n        """\r\n        print("Executing conversation interaction...")\r\n\r\n        # Maintain natural gaze\r\n        self.maintain_gaze_on_human(target_person)\r\n\r\n        # Use conversational expressions (vary over time)\r\n        conversation_expressions = [\'attentive\', \'happy\', \'neutral\']\r\n        import random\r\n        self.set_expression(random.choice(conversation_expressions))\r\n\r\n        # Nod occasionally to show understanding\r\n        if time.time() - self.behavior_start_time > 3:  # Every 3 seconds\r\n            self.execute_nod_behavior(count=1)\r\n            self.behavior_start_time = time.time()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-7-create-the-main-launch-file",children:"Step 7: Create the Main Launch File"}),"\n",(0,o.jsxs)(n.p,{children:["Create the launch file in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/launch/humanoid_controller.launch.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\r\nfrom launch.event_handlers import OnProcessStart\r\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\r\nfrom launch_ros.actions import Node\r\nfrom launch_ros.substitutions import FindPackageShare\r\nfrom ament_index_python.packages import get_package_share_directory\r\nimport os\r\n\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    use_sim_time = DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='true',\r\n        description='Use simulation (Gazebo) clock if true'\r\n    )\r\n\r\n    # Humanoid controller node\r\n    humanoid_controller = Node(\r\n        package='humanoid_controller',\r\n        executable='humanoid_controller_node',\r\n        name='humanoid_controller',\r\n        parameters=[\r\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    # Additional nodes could be added here:\r\n    # - Vision processing node\r\n    # - Audio processing node\r\n    # - High-level task planner\r\n\r\n    return LaunchDescription([\r\n        use_sim_time,\r\n        humanoid_controller\r\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-8-update-setuppy",children:"Step 8: Update setup.py"}),"\n",(0,o.jsxs)(n.p,{children:["Update the setup.py file in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/setup.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\r\n\r\npackage_name = 'humanoid_controller'\r\n\r\nsetup(\r\n    name=package_name,\r\n    version='0.0.0',\r\n    packages=[package_name,\r\n              f'{package_name}.controllers',\r\n              f'{package_name}.sensors',\r\n              f'{package_name}.utils',\r\n              f'{package_name}.behaviors'],\r\n    data_files=[\r\n        ('share/ament_index/resource_index/packages',\r\n            ['resource/' + package_name]),\r\n        ('share/' + package_name, ['package.xml']),\r\n        ('share/' + package_name + '/launch', ['launch/humanoid_controller.launch.py']),\r\n    ],\r\n    install_requires=['setuptools'],\r\n    zip_safe=True,\r\n    maintainer='Your Name',\r\n    maintainer_email='your.email@example.com',\r\n    description='Humanoid Robot Controller integrating balance, manipulation, and social interaction',\r\n    license='Apache License 2.0',\r\n    tests_require=['pytest'],\r\n    entry_points={\r\n        'console_scripts': [\r\n            'humanoid_controller_node = humanoid_controller.humanoid_controller_node:main',\r\n        ],\r\n    },\r\n)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-9-create-a-simple-test-script",children:"Step 9: Create a Simple Test Script"}),"\n",(0,o.jsxs)(n.p,{children:["Create a test script in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/test_controller.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\n\"\"\"\r\nTest script for Humanoid Controller\r\nDemonstrates basic functionality\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Point\r\nimport time\r\n\r\n\r\nclass ControllerTester(Node):\r\n    def __init__(self):\r\n        super().__init__('controller_tester')\r\n\r\n        # Create subscriber to controller status\r\n        self.status_sub = self.create_subscription(\r\n            String,\r\n            '/controller_status',\r\n            self.status_callback,\r\n            10\r\n        )\r\n\r\n        # Create subscriber to CoM position\r\n        self.com_sub = self.create_subscription(\r\n            Point,\r\n            '/center_of_mass',\r\n            self.com_callback,\r\n            10\r\n        )\r\n\r\n        # Create subscriber to ZMP\r\n        self.zmp_sub = self.create_subscription(\r\n            Point,\r\n            '/zero_moment_point',\r\n            self.zmp_callback,\r\n            10\r\n        )\r\n\r\n        # Store latest values\r\n        self.latest_status = None\r\n        self.latest_com = None\r\n        self.latest_zmp = None\r\n\r\n        self.get_logger().info('Controller Tester initialized')\r\n\r\n    def status_callback(self, msg):\r\n        self.latest_status = msg.data\r\n        self.get_logger().info(f'Controller Status: {msg.data}')\r\n\r\n    def com_callback(self, msg):\r\n        self.latest_com = [msg.x, msg.y, msg.z]\r\n        # Don't log every message to avoid spam\r\n\r\n    def zmp_callback(self, msg):\r\n        self.latest_zmp = [msg.x, msg.y, msg.z]\r\n        # Don't log every message to avoid spam\r\n\r\n    def test_basic_functions(self):\r\n        \"\"\"Test basic controller functions\"\"\"\r\n        self.get_logger().info('Testing basic controller functions...')\r\n\r\n        # Wait a bit for messages to arrive\r\n        time.sleep(2.0)\r\n\r\n        if self.latest_status:\r\n            self.get_logger().info(f'Current status: {self.latest_status}')\r\n\r\n        if self.latest_com:\r\n            self.get_logger().info(f'Current CoM: [{self.latest_com[0]:.3f}, {self.latest_com[1]:.3f}, {self.latest_com[2]:.3f}]')\r\n\r\n        if self.latest_zmp:\r\n            self.get_logger().info(f'Current ZMP: [{self.latest_zmp[0]:.3f}, {self.latest_zmp[1]:.3f}, {self.latest_zmp[2]:.3f}]')\r\n\r\n        # Test mode switching (this would require service calls in practice)\r\n        self.get_logger().info('Test completed successfully!')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    tester = ControllerTester()\r\n\r\n    # Run basic tests\r\n    tester.test_basic_functions()\r\n\r\n    # Keep node alive briefly to receive messages\r\n    time.sleep(5.0)\r\n\r\n    tester.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-10-build-and-test-the-controller",children:"Step 10: Build and Test the Controller"}),"\n",(0,o.jsx)(n.p,{children:"Now let's build the package and test it:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Build the package\r\ncd ~/humanoid_robot_controller\r\ncolcon build --packages-select humanoid_controller\r\nsource install/setup.bash\r\n\r\n# Terminal 2: Run the controller (in simulation environment)\r\nros2 launch humanoid_controller humanoid_controller.launch.py\r\n\r\n# Terminal 3: Test the controller\r\nros2 run humanoid_controller test_controller\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-11-advanced-testing-scenarios",children:"Step 11: Advanced Testing Scenarios"}),"\n",(0,o.jsxs)(n.p,{children:["Create an advanced testing script in ",(0,o.jsx)(n.code,{children:"~/humanoid_robot_controller/test_advanced.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\n\"\"\"\r\nAdvanced test script for Humanoid Controller\r\nTests complex scenarios including balance recovery, manipulation, and interaction\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Point, Twist\r\nfrom sensor_msgs.msg import JointState\r\nfrom std_msgs.msg import Float64MultiArray\r\nimport time\r\nimport numpy as np\r\n\r\n\r\nclass AdvancedControllerTester(Node):\r\n    def __init__(self):\r\n        super().__init__('advanced_controller_tester')\r\n\r\n        # Publishers for testing\r\n        self.mode_cmd_pub = self.create_publisher(String, '/controller_mode_cmd', 10)\r\n        self.test_cmd_pub = self.create_publisher(String, '/controller_test_cmd', 10)\r\n\r\n        # Subscribers\r\n        self.status_sub = self.create_subscription(\r\n            String,\r\n            '/controller_status',\r\n            self.status_callback,\r\n            10\r\n        )\r\n\r\n        self.com_sub = self.create_subscription(\r\n            Point,\r\n            '/center_of_mass',\r\n            self.com_callback,\r\n            10\r\n        )\r\n\r\n        self.zmp_sub = self.create_subscription(\r\n            Point,\r\n            '/zero_moment_point',\r\n            self.zmp_callback,\r\n            10\r\n        )\r\n\r\n        # Test state\r\n        self.test_results = {}\r\n        self.current_test = None\r\n\r\n        self.get_logger().info('Advanced Controller Tester initialized')\r\n\r\n    def status_callback(self, msg):\r\n        if self.current_test:\r\n            print(f'Test {self.current_test}: Status - {msg.data}')\r\n\r\n    def com_callback(self, msg):\r\n        if self.current_test and 'balance' in self.current_test:\r\n            com = np.array([msg.x, msg.y, msg.z])\r\n            self.test_results[self.current_test]['com_history'].append(com)\r\n\r\n    def zmp_callback(self, msg):\r\n        if self.current_test and 'balance' in self.current_test:\r\n            zmp = np.array([msg.x, msg.y, msg.z])\r\n            self.test_results[self.current_test]['zmp_history'].append(zmp)\r\n\r\n    def run_comprehensive_test(self):\r\n        \"\"\"Run comprehensive tests of all controller capabilities\"\"\"\r\n        self.get_logger().info('Starting comprehensive controller test...')\r\n\r\n        # Test 1: Balance control\r\n        self.run_balance_test()\r\n\r\n        # Test 2: Basic manipulation\r\n        self.run_manipulation_test()\r\n\r\n        # Test 3: Social interaction\r\n        self.run_interaction_test()\r\n\r\n        # Test 4: Integrated behavior\r\n        self.run_integration_test()\r\n\r\n        self.print_test_summary()\r\n\r\n    def run_balance_test(self):\r\n        \"\"\"Test balance control capabilities\"\"\"\r\n        self.current_test = 'balance_basic'\r\n        self.test_results[self.current_test] = {\r\n            'passed': False,\r\n            'com_history': [],\r\n            'zmp_history': [],\r\n            'start_time': time.time()\r\n        }\r\n\r\n        self.get_logger().info('Running basic balance test...')\r\n\r\n        # Let it run for 10 seconds\r\n        time.sleep(10.0)\r\n\r\n        # Analyze results\r\n        com_history = self.test_results[self.current_test]['com_history']\r\n        zmp_history = self.test_results[self.current_test]['zmp_history']\r\n\r\n        if len(com_history) > 50 and len(zmp_history) > 50:\r\n            # Calculate stability metrics\r\n            com_stability = np.std([c[0:2] for c in com_history[-20:]])  # Last 20 CoM positions\r\n            zmp_stability = np.std([z[0:2] for z in zmp_history[-20:]])  # Last 20 ZMP positions\r\n\r\n            self.get_logger().info(f'Balance test - CoM stability: {np.mean(com_stability):.3f}, ZMP stability: {np.mean(zmp_stability):.3f}')\r\n\r\n            # Check if stable (thresholds are arbitrary for demo)\r\n            if np.mean(com_stability) < 0.05 and np.mean(zmp_stability) < 0.05:\r\n                self.test_results[self.current_test]['passed'] = True\r\n                self.get_logger().info('\u2713 Balance test PASSED')\r\n            else:\r\n                self.get_logger().info('\u2717 Balance test FAILED')\r\n        else:\r\n            self.get_logger().info('\u2717 Balance test INCONCLUSIVE - insufficient data')\r\n\r\n    def run_manipulation_test(self):\r\n        \"\"\"Test manipulation capabilities\"\"\"\r\n        self.current_test = 'manipulation_basic'\r\n        self.test_results[self.current_test] = {\r\n            'passed': False,\r\n            'start_time': time.time()\r\n        }\r\n\r\n        self.get_logger().info('Running basic manipulation test...')\r\n\r\n        # In a real test, you would command specific manipulation tasks\r\n        # For this demo, just verify the controller can switch modes\r\n        mode_cmd = String()\r\n        mode_cmd.data = 'manipulate'\r\n\r\n        # Publish mode command (in real scenario, this would use services)\r\n        for i in range(5):\r\n            self.mode_cmd_pub.publish(mode_cmd)\r\n            time.sleep(0.5)\r\n\r\n        # Wait to see if mode switches\r\n        time.sleep(2.0)\r\n\r\n        # For this basic test, assume it passes if no errors occurred\r\n        self.test_results[self.current_test]['passed'] = True\r\n        self.get_logger().info('\u2713 Manipulation test completed')\r\n\r\n    def run_interaction_test(self):\r\n        \"\"\"Test social interaction capabilities\"\"\"\r\n        self.current_test = 'interaction_basic'\r\n        self.test_results[self.current_test] = {\r\n            'passed': False,\r\n            'start_time': time.time()\r\n        }\r\n\r\n        self.get_logger().info('Running basic interaction test...')\r\n\r\n        # Switch to interaction mode\r\n        mode_cmd = String()\r\n        mode_cmd.data = 'interact'\r\n\r\n        for i in range(5):\r\n            self.mode_cmd_pub.publish(mode_cmd)\r\n            time.sleep(0.5)\r\n\r\n        # Wait to observe interaction behaviors\r\n        time.sleep(5.0)\r\n\r\n        # For this basic test, assume it passes\r\n        self.test_results[self.current_test]['passed'] = True\r\n        self.get_logger().info('\u2713 Interaction test completed')\r\n\r\n    def run_integration_test(self):\r\n        \"\"\"Test integrated behavior - balance + manipulation + interaction\"\"\"\r\n        self.current_test = 'integration_full'\r\n        self.test_results[self.current_test] = {\r\n            'passed': False,\r\n            'start_time': time.time()\r\n        }\r\n\r\n        self.get_logger().info('Running full integration test...')\r\n\r\n        # This would test the controller coordinating all capabilities\r\n        # For demo purposes, cycle through different modes\r\n        modes = ['balance', 'manipulate', 'interact', 'balance']\r\n\r\n        for mode in modes:\r\n            mode_cmd = String()\r\n            mode_cmd.data = mode\r\n            self.mode_cmd_pub.publish(mode_cmd)\r\n            self.get_logger().info(f'Switched to mode: {mode}')\r\n            time.sleep(3.0)\r\n\r\n        # For this basic test, assume it passes\r\n        self.test_results[self.current_test]['passed'] = True\r\n        self.get_logger().info('\u2713 Integration test completed')\r\n\r\n    def print_test_summary(self):\r\n        \"\"\"Print summary of all tests\"\"\"\r\n        self.get_logger().info('\\n' + '='*50)\r\n        self.get_logger().info('CONTROLLER TEST SUMMARY')\r\n        self.get_logger().info('='*50)\r\n\r\n        total_tests = len(self.test_results)\r\n        passed_tests = sum(1 for result in self.test_results.values() if result['passed'])\r\n\r\n        for test_name, result in self.test_results.items():\r\n            status = \"PASS\" if result['passed'] else \"FAIL\"\r\n            duration = time.time() - result['start_time']\r\n            self.get_logger().info(f'{test_name:<20} | {status:<4} | {duration:>5.1f}s')\r\n\r\n        self.get_logger().info('-'*50)\r\n        self.get_logger().info(f'Total: {total_tests}, Passed: {passed_tests}, Failed: {total_tests - passed_tests}')\r\n        self.get_logger().info(f'Success Rate: {(passed_tests/total_tests)*100:.1f}%' if total_tests > 0 else '0%')\r\n        self.get_logger().info('='*50)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    tester = AdvancedControllerTester()\r\n\r\n    try:\r\n        tester.run_comprehensive_test()\r\n    except KeyboardInterrupt:\r\n        tester.get_logger().info('Testing interrupted by user')\r\n    finally:\r\n        tester.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"expected-results",children:"Expected Results"}),"\n",(0,o.jsx)(n.p,{children:"When you run the complete humanoid controller system:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Balance Control"}),": The robot should maintain stable balance with CoM and ZMP within appropriate bounds"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": The robot should be able to execute basic reaching and grasping motions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Social Interaction"}),": The robot should detect humans and respond with appropriate behaviors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration"}),": All systems should work together without conflicts"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(n.p,{children:"If you encounter issues:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Build Errors"}),": Ensure all dependencies are installed and paths are correct"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Runtime Errors"}),": Check that the simulation environment is properly configured"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance Issues"}),": The controller may need tuning for your specific robot model"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration Problems"}),": Verify that all ROS 2 topics and services are properly connected"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"extensions",children:"Extensions"}),"\n",(0,o.jsx)(n.p,{children:"To enhance this controller:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Add more sophisticated behaviors"})," like emotion recognition"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Implement learning algorithms"})," to adapt to users"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Add more complex manipulation tasks"})," like bimanual coordination"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Include advanced safety features"})," like collision avoidance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Add localization and mapping"})," for autonomous navigation"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This controller provides a solid foundation for humanoid robot control that integrates balance, manipulation, and social interaction capabilities in a unified framework."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(_,{...e})}):_(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>a});var t=r(6540);const o={},s=t.createContext(o);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);