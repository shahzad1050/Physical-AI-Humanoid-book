"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[8937],{8016:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>_,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module3/reinforcement-learning","title":"Reinforcement Learning for Control","description":"Introduction to Reinforcement Learning in Robotics","source":"@site/docs/module3/reinforcement-learning.md","sourceDirName":"module3","slug":"/module3/reinforcement-learning","permalink":"/Physical-AI-Humanoid-book/docs/module3/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/user/physical-ai-humanoid-robotics/tree/main/docs/module3/reinforcement-learning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation and Grasping","permalink":"/Physical-AI-Humanoid-book/docs/module3/manipulation-grasping"},"next":{"title":"Sim-to-Real Transfer Techniques","permalink":"/Physical-AI-Humanoid-book/docs/module3/sim-to-real"}}');var a=t(4848),s=t(8453);const o={},r="Reinforcement Learning for Control",l={},c=[{value:"Introduction to Reinforcement Learning in Robotics",id:"introduction-to-reinforcement-learning-in-robotics",level:2},{value:"RL Fundamentals for Robotics",id:"rl-fundamentals-for-robotics",level:2},{value:"The RL Framework",id:"the-rl-framework",level:3},{value:"Markov Decision Process (MDP) Formulation",id:"markov-decision-process-mdp-formulation",level:3},{value:"Deep Reinforcement Learning Algorithms",id:"deep-reinforcement-learning-algorithms",level:2},{value:"Deep Q-Network (DQN) for Discrete Actions",id:"deep-q-network-dqn-for-discrete-actions",level:3},{value:"Deep Deterministic Policy Gradient (DDPG) for Continuous Actions",id:"deep-deterministic-policy-gradient-ddpg-for-continuous-actions",level:3},{value:"Twin Delayed DDPG (TD3) - Advanced Continuous Control",id:"twin-delayed-ddpg-td3---advanced-continuous-control",level:3},{value:"Isaac Sim Integration for RL Training",id:"isaac-sim-integration-for-rl-training",level:2},{value:"Creating RL Environments in Isaac Sim",id:"creating-rl-environments-in-isaac-sim",level:3},{value:"Training Loop Implementation",id:"training-loop-implementation",level:3},{value:"Advanced RL Techniques for Robotics",id:"advanced-rl-techniques-for-robotics",level:2},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:3},{value:"Hindsight Experience Replay (HER)",id:"hindsight-experience-replay-her",level:3},{value:"Domain Randomization and Sim-to-Real Transfer",id:"domain-randomization-and-sim-to-real-transfer",level:2},{value:"Domain Randomization in Isaac Sim",id:"domain-randomization-in-isaac-sim",level:3},{value:"Multi-Task and Transfer Learning",id:"multi-task-and-transfer-learning",level:2},{value:"Multi-Task RL Agent",id:"multi-task-rl-agent",level:3},{value:"Practical RL Applications in Robotics",id:"practical-rl-applications-in-robotics",level:2},{value:"Navigation with RL",id:"navigation-with-rl",level:3},{value:"Manipulation with RL",id:"manipulation-with-rl",level:3},{value:"Training Best Practices",id:"training-best-practices",level:2},{value:"Curriculum Learning",id:"curriculum-learning",level:3},{value:"Reward Shaping",id:"reward-shaping",level:3},{value:"Troubleshooting and Debugging",id:"troubleshooting-and-debugging",level:2},{value:"Common RL Issues and Solutions",id:"common-rl-issues-and-solutions",level:3},{value:"Best Practices for RL in Robotics",id:"best-practices-for-rl-in-robotics",level:2},{value:"1. Simulation Design",id:"1-simulation-design",level:3},{value:"2. Reward Engineering",id:"2-reward-engineering",level:3},{value:"3. Exploration Strategies",id:"3-exploration-strategies",level:3},{value:"4. Safety Considerations",id:"4-safety-considerations",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"reinforcement-learning-for-control",children:"Reinforcement Learning for Control"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-reinforcement-learning-in-robotics",children:"Introduction to Reinforcement Learning in Robotics"}),"\n",(0,a.jsx)(n.p,{children:"Reinforcement Learning (RL) has emerged as a powerful paradigm for learning complex robotic behaviors directly from interaction with the environment. Unlike traditional control methods that rely on explicit mathematical models, RL enables robots to learn optimal control policies through trial and error, making it particularly suitable for complex, high-dimensional tasks where analytical solutions are difficult to obtain."}),"\n",(0,a.jsx)(n.h2,{id:"rl-fundamentals-for-robotics",children:"RL Fundamentals for Robotics"}),"\n",(0,a.jsx)(n.h3,{id:"the-rl-framework",children:"The RL Framework"}),"\n",(0,a.jsx)(n.p,{children:"In robotics, the RL framework consists of:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Agent"}),": The robot or control system"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environment"}),": The physical world or simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State (s)"}),": Robot configuration, sensor readings, task context"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action (a)"}),": Motor commands, joint torques, velocities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reward (r)"}),": Feedback signal indicating task success"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Policy (\u03c0)"}),": Mapping from states to actions"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Environment \u2192 State (s) \u2192 Agent \u2192 Action (a) \u2192 Environment \u2192 Reward (r)\n     \u2191                                           \u2193\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(n.h3,{id:"markov-decision-process-mdp-formulation",children:"Markov Decision Process (MDP) Formulation"}),"\n",(0,a.jsx)(n.p,{children:"The robotic control problem can be formulated as an MDP:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State Space (S)"}),": Continuous or discrete states representing robot configuration and environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Space (A)"}),": Continuous (torques, velocities) or discrete (motion primitives) actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Transition Dynamics (P)"}),": Probability of transitioning to next state given current state and action"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reward Function (R)"}),": Scalar feedback signal"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Discount Factor (\u03b3)"}),": Trade-off between immediate and future rewards"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"deep-reinforcement-learning-algorithms",children:"Deep Reinforcement Learning Algorithms"}),"\n",(0,a.jsx)(n.h3,{id:"deep-q-network-dqn-for-discrete-actions",children:"Deep Q-Network (DQN) for Discrete Actions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\n\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(DQN, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n\n    def forward(self, state):\n        return self.network(state)\n\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n\n        # Neural networks\n        self.q_network = DQN(state_dim, action_dim)\n        self.target_network = DQN(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n\n        # Replay buffer\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 32\n\n        # Update target network\n        self.update_target_network()\n\n    def update_target_network(self):\n        """Copy weights from main network to target network"""\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def remember(self, state, action, reward, next_state, done):\n        """Store experience in replay buffer"""\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        """Choose action using epsilon-greedy policy"""\n        if np.random.random() <= self.epsilon:\n            return random.randrange(self.action_dim)\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.cpu().data.numpy())\n\n    def replay(self):\n        """Train the network on a batch of experiences"""\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.LongTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch])\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.BoolTensor([e[4] for e in batch])\n\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n'})}),"\n",(0,a.jsx)(n.h3,{id:"deep-deterministic-policy-gradient-ddpg-for-continuous-actions",children:"Deep Deterministic Policy Gradient (DDPG) for Continuous Actions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom collections import deque\nimport random\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):\n        super(Actor, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()\n        )\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        return self.max_action * self.network(state)\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(Critic, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, state, action):\n        return self.network(torch.cat([state, action], dim=1))\n\nclass DDPGAgent:\n    def __init__(self, state_dim, action_dim, max_action, lr_actor=1e-4, lr_critic=1e-3,\n                 gamma=0.99, tau=0.005, noise_std=0.2):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.gamma = gamma\n        self.tau = tau\n        self.noise_std = noise_std\n\n        # Networks\n        self.actor = Actor(state_dim, action_dim, max_action)\n        self.actor_target = Actor(state_dim, action_dim, max_action)\n        self.critic = Critic(state_dim, action_dim)\n        self.critic_target = Critic(state_dim, action_dim)\n\n        # Optimizers\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n\n        # Replay buffer\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 100\n\n        # Initialize target networks\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n    def select_action(self, state, add_noise=True):\n        """Select action with optional noise for exploration"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action = self.actor(state_tensor).cpu().data.numpy().flatten()\n\n        if add_noise:\n            noise = np.random.normal(0, self.noise_std, size=self.action_dim)\n            action = action + noise\n            action = np.clip(action, -self.max_action, self.max_action)\n\n        return action\n\n    def remember(self, state, action, reward, next_state, done):\n        """Store experience in replay buffer"""\n        self.memory.append((state, action, reward, next_state, done))\n\n    def update(self):\n        """Update actor and critic networks"""\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.FloatTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch]).unsqueeze(1)\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.BoolTensor([e[4] for e in batch]).unsqueeze(1)\n\n        # Critic update\n        next_actions = self.actor_target(next_states)\n        next_q_values = self.critic_target(next_states, next_actions)\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        current_q_values = self.critic(states, actions)\n        critic_loss = nn.MSELoss()(current_q_values, target_q_values.detach())\n\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # Actor update\n        predicted_actions = self.actor(states)\n        actor_loss = -self.critic(states, predicted_actions).mean()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # Soft update target networks\n        self.soft_update(self.actor_target, self.actor, self.tau)\n        self.soft_update(self.critic_target, self.critic, self.tau)\n\n    def soft_update(self, target_network, source_network, tau):\n        """Soft update of target network parameters"""\n        for target_param, param in zip(target_network.parameters(), source_network.parameters()):\n            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"twin-delayed-ddpg-td3---advanced-continuous-control",children:"Twin Delayed DDPG (TD3) - Advanced Continuous Control"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class TD3Critic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(TD3Critic, self).__init__()\n\n        # Two Q-networks for double Q-learning\n        self.q1 = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        self.q2 = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, state, action):\n        sa = torch.cat([state, action], dim=1)\n        q1 = self.q1(sa)\n        q2 = self.q2(sa)\n        return q1, q2\n\n    def Q1(self, state, action):\n        sa = torch.cat([state, action], dim=1)\n        return self.q1(sa)\n\nclass TD3Agent:\n    def __init__(self, state_dim, action_dim, max_action, lr_actor=1e-4, lr_critic=1e-3,\n                 gamma=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.gamma = gamma\n        self.tau = tau\n        self.policy_noise = policy_noise\n        self.noise_clip = noise_clip\n        self.policy_freq = policy_freq\n        self.total_it = 0\n\n        # Networks\n        self.actor = Actor(state_dim, action_dim, max_action)\n        self.actor_target = Actor(state_dim, action_dim, max_action)\n        self.critic = TD3Critic(state_dim, action_dim)\n        self.critic_target = TD3Critic(state_dim, action_dim)\n\n        # Optimizers\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n\n        # Initialize target networks\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n    def select_action(self, state, add_noise=True):\n        """Select action with target policy smoothing"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action = self.actor(state_tensor).cpu().data.numpy().flatten()\n\n        if add_noise:\n            # Add clipped noise for exploration\n            noise = np.random.normal(0, self.policy_noise, size=self.action_dim)\n            noise = np.clip(noise, -self.noise_clip, self.noise_clip)\n            action = action + noise\n            action = np.clip(action, -self.max_action, self.max_action)\n\n        return action\n\n    def update(self, replay_buffer, batch_size=100):\n        """Update TD3 networks"""\n        self.total_it += 1\n\n        # Sample batch from replay buffer\n        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n\n        with torch.no_grad():\n            # Select action according to policy and add clipped noise\n            noise = torch.FloatTensor(action).data.normal_(0, self.policy_noise)\n            noise = noise.clamp(-self.noise_clip, self.noise_clip)\n\n            next_action = (self.actor_target(next_state) + noise).clamp(\n                -self.max_action, self.max_action\n            )\n\n            # Compute target Q-value\n            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n            target_Q = torch.min(target_Q1, target_Q2)\n            target_Q = reward + not_done * self.gamma * target_Q\n\n        # Get current Q estimates\n        current_Q1, current_Q2 = self.critic(state, action)\n\n        # Compute critic loss\n        critic_loss = nn.MSELoss()(current_Q1, target_Q) + nn.MSELoss()(current_Q2, target_Q)\n\n        # Optimize critic\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # Delayed policy updates\n        if self.total_it % self.policy_freq == 0:\n            # Compute actor loss\n            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n\n            # Optimize actor\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            # Soft update target networks\n            self.soft_update(self.actor_target, self.actor, self.tau)\n            self.soft_update(self.critic_target, self.critic, self.tau)\n\n    def soft_update(self, target_network, source_network, tau):\n        """Soft update of target network parameters"""\n        for target_param, param in zip(target_network.parameters(), source_network.parameters()):\n            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-sim-integration-for-rl-training",children:"Isaac Sim Integration for RL Training"}),"\n",(0,a.jsx)(n.h3,{id:"creating-rl-environments-in-isaac-sim",children:"Creating RL Environments in Isaac Sim"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom ommi.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.objects import DynamicCuboid\nimport numpy as np\nimport torch\n\nclass IsaacRLEnvironment:\n    def __init__(self, robot_usd_path, scene_config):\n        # Initialize Isaac Sim world\n        self.world = World(stage_units_in_meters=1.0)\n        self.world.scene.add_default_ground_plane()\n\n        # Add robot\n        self.robot = self.world.scene.add(\n            Robot(\n                prim_path="/World/Robot",\n                name="rl_robot",\n                usd_path=robot_usd_path\n            )\n        )\n\n        # Initialize environment parameters\n        self.scene_config = scene_config\n        self.max_episode_steps = 1000\n        self.current_step = 0\n\n        # Reset environment\n        self.reset()\n\n    def reset(self):\n        """Reset the environment to initial state"""\n        self.world.reset()\n        self.current_step = 0\n\n        # Reset robot to initial configuration\n        initial_joint_positions = self.scene_config.get("initial_joint_positions", [0] * 7)\n        self.robot.set_joint_positions(np.array(initial_joint_positions))\n\n        # Reset objects in scene\n        self.reset_objects()\n\n        # Return initial observation\n        return self.get_observation()\n\n    def step(self, action):\n        """Execute one step in the environment"""\n        # Apply action to robot\n        self.apply_action(action)\n\n        # Step the simulation\n        self.world.step(render=True)\n\n        # Get next observation\n        next_obs = self.get_observation()\n\n        # Calculate reward\n        reward = self.calculate_reward()\n\n        # Check if episode is done\n        done = self.is_episode_done()\n\n        # Increment step counter\n        self.current_step += 1\n\n        return next_obs, reward, done, {}\n\n    def get_observation(self):\n        """Get current observation from environment"""\n        # Get robot state\n        joint_positions = self.robot.get_joint_positions()\n        joint_velocities = self.robot.get_joint_velocities()\n\n        # Get end-effector pose\n        ee_pose = self.robot.get_end_effector_pose()\n\n        # Get object information (if applicable)\n        object_info = self.get_object_info()\n\n        # Combine into observation vector\n        observation = np.concatenate([\n            joint_positions,\n            joint_velocities,\n            ee_pose.position,\n            ee_pose.orientation,\n            object_info\n        ])\n\n        return observation\n\n    def apply_action(self, action):\n        """Apply action to robot"""\n        # Convert action to joint commands\n        joint_commands = self.process_action(action)\n\n        # Apply commands to robot\n        self.robot.apply_action(joint_commands)\n\n    def calculate_reward(self):\n        """Calculate reward based on current state"""\n        # Example reward function - customize based on task\n        ee_pos = self.robot.get_end_effector_position()\n        target_pos = self.scene_config.get("target_position", np.array([0, 0, 0]))\n\n        # Distance to target\n        distance_to_target = np.linalg.norm(ee_pos - target_pos)\n\n        # Reward based on distance (negative distance)\n        reward = -distance_to_target\n\n        # Add bonus for reaching target\n        if distance_to_target < 0.1:  # 10cm threshold\n            reward += 100  # Large bonus for reaching target\n\n        # Add penalty for joint limits\n        joint_positions = self.robot.get_joint_positions()\n        joint_limits = self.scene_config.get("joint_limits", [])\n        if joint_limits:\n            for i, (pos, limits) in enumerate(zip(joint_positions, joint_limits)):\n                if pos < limits[0] or pos > limits[1]:\n                    reward -= 10  # Penalty for exceeding joint limits\n\n        return reward\n\n    def is_episode_done(self):\n        """Check if episode is done"""\n        # Check step limit\n        if self.current_step >= self.max_episode_steps:\n            return True\n\n        # Check for collisions (if applicable)\n        if self.has_collision():\n            return True\n\n        # Check task completion\n        if self.is_task_completed():\n            return True\n\n        return False\n\n    def has_collision(self):\n        """Check if robot has collided with environment"""\n        # Implementation depends on collision detection setup\n        return False\n\n    def is_task_completed(self):\n        """Check if task has been completed"""\n        # Example: check if end-effector is close to target\n        ee_pos = self.robot.get_end_effector_position()\n        target_pos = self.scene_config.get("target_position", np.array([0, 0, 0]))\n        distance = np.linalg.norm(ee_pos - target_pos)\n        return distance < 0.05  # 5cm threshold\n'})}),"\n",(0,a.jsx)(n.h3,{id:"training-loop-implementation",children:"Training Loop Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class RLTrainer:\n    def __init__(self, agent, environment, max_episodes=1000, max_steps_per_episode=1000):\n        self.agent = agent\n        self.env = environment\n        self.max_episodes = max_episodes\n        self.max_steps_per_episode = max_steps_per_episode\n        self.episode_rewards = []\n\n    def train(self):\n        """Main training loop"""\n        for episode in range(self.max_episodes):\n            # Reset environment for new episode\n            state = self.env.reset()\n\n            episode_reward = 0\n            done = False\n\n            for step in range(self.max_steps_per_episode):\n                # Select action using current policy\n                action = self.agent.select_action(state)\n\n                # Execute action in environment\n                next_state, reward, done, info = self.env.step(action)\n\n                # Store experience in replay buffer\n                self.agent.remember(state, action, reward, next_state, done)\n\n                # Update agent\n                self.agent.update()\n\n                # Update current state\n                state = next_state\n                episode_reward += reward\n\n                if done:\n                    break\n\n            # Log episode results\n            self.episode_rewards.append(episode_reward)\n            print(f"Episode {episode}: Reward = {episode_reward:.2f}, Steps = {step+1}")\n\n            # Save model periodically\n            if episode % 100 == 0:\n                self.save_model(f"rl_model_episode_{episode}.pth")\n\n    def save_model(self, filepath):\n        """Save trained model"""\n        torch.save({\n            \'actor_state_dict\': self.agent.actor.state_dict(),\n            \'critic_state_dict\': self.agent.critic.state_dict() if hasattr(self.agent, \'critic\') else None,\n            \'episode_rewards\': self.episode_rewards\n        }, filepath)\n\n    def load_model(self, filepath):\n        """Load trained model"""\n        checkpoint = torch.load(filepath)\n        self.agent.actor.load_state_dict(checkpoint[\'actor_state_dict\'])\n        if checkpoint[\'critic_state_dict\'] is not None:\n            self.agent.critic.load_state_dict(checkpoint[\'critic_state_dict\'])\n        self.episode_rewards = checkpoint[\'episode_rewards\']\n'})}),"\n",(0,a.jsx)(n.h2,{id:"advanced-rl-techniques-for-robotics",children:"Advanced RL Techniques for Robotics"}),"\n",(0,a.jsx)(n.h3,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch.nn as nn\nimport torch.optim as optim\n\nclass PPOActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(PPOActorCritic, self).__init__()\n\n        # Shared feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Actor head (for continuous actions)\n        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))\n\n        # Critic head\n        self.critic = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state):\n        features = self.feature_extractor(state)\n\n        # Actor output\n        action_mean = self.actor_mean(features)\n        action_log_std = self.actor_log_std.expand_as(action_mean)\n\n        # Critic output\n        value = self.critic(features)\n\n        return action_mean, action_log_std, value\n\n    def get_action(self, state):\n        """Sample action from policy"""\n        action_mean, action_log_std, _ = self.forward(state)\n        action_std = torch.exp(action_log_std)\n\n        # Sample from normal distribution\n        normal = torch.distributions.Normal(action_mean, action_std)\n        action = normal.sample()\n        log_prob = normal.log_prob(action).sum(dim=-1)\n\n        return action, log_prob\n\n    def evaluate(self, state, action):\n        """Evaluate action probability and value"""\n        action_mean, action_log_std, value = self.forward(state)\n        action_std = torch.exp(action_log_std)\n\n        normal = torch.distributions.Normal(action_mean, action_std)\n        log_prob = normal.log_prob(action).sum(dim=-1)\n        entropy = normal.entropy().sum(dim=-1)\n\n        return log_prob, entropy, value\n\nclass PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2,\n                 k_epochs=80, entropy_coef=0.01):\n        self.lr = lr\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.k_epochs = k_epochs\n        self.entropy_coef = entropy_coef\n\n        self.policy = PPOActorCritic(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n\n        self.buffer = RolloutBuffer()\n\n    def update(self):\n        """Update policy using PPO"""\n        # Monte Carlo estimate of state rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # Normalizing the rewards\n        rewards = torch.tensor(rewards, dtype=torch.float32)\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n\n        # Convert list to tensor\n        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach()\n        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach()\n        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach()\n\n        # Optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # Evaluating old actions and values\n            logprobs, entropy, state_values = self.policy.evaluate(old_states, old_actions)\n\n            # Finding the ratio (pi_theta / pi_theta__old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # Finding Surrogate Loss\n            advantages = rewards - state_values.detach()\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n\n            # Final loss of clipped objective PPO\n            loss = -torch.min(surr1, surr2) + 0.5 * nn.MSELoss()(state_values, rewards) - self.entropy_coef * entropy\n\n            # Take gradient step\n            self.optimizer.zero_grad()\n            loss.mean().backward()\n            self.optimizer.step()\n\n        # Clear buffer\n        self.buffer.clear()\n\nclass RolloutBuffer:\n    def __init__(self):\n        self.actions = []\n        self.states = []\n        self.logprobs = []\n        self.rewards = []\n        self.state_values = []\n        self.is_terminals = []\n\n    def clear(self):\n        del self.actions[:]\n        del self.states[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.state_values[:]\n        del self.is_terminals[:]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"hindsight-experience-replay-her",children:"Hindsight Experience Replay (HER)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class HERBuffer:\n    def __init__(self, buffer_size, k_future=4, reward_func=None):\n        self.buffer_size = buffer_size\n        self.k_future = k_future\n        self.reward_func = reward_func\n        self.buffer = deque(maxlen=buffer_size)\n\n    def store_episode(self, episode_transitions):\n        \"\"\"Store an entire episode and generate HER transitions\"\"\"\n        # Store original episode\n        for transition in episode_transitions:\n            self.buffer.append(transition)\n\n        # Generate HER transitions\n        episode_length = len(episode_transitions)\n\n        for t in range(episode_length):\n            # Get future time steps for goal relabeling\n            future_timesteps = np.random.choice(\n                range(t, episode_length),\n                size=min(self.k_future, episode_length - t),\n                replace=False\n            )\n\n            for future_time in future_timesteps:\n                # Create new transition with relabeled goal\n                transition = episode_transitions[t]\n                future_transition = episode_transitions[future_time]\n\n                # Relabel goal with achieved state from future\n                new_goal = future_transition['achieved_state']\n\n                # Calculate new reward with relabeled goal\n                new_reward = self.reward_func(\n                    transition['achieved_state'],\n                    new_goal,\n                    transition['action']\n                )\n\n                # Create HER transition\n                her_transition = {\n                    'state': transition['state'],\n                    'action': transition['action'],\n                    'reward': new_reward,\n                    'next_state': transition['next_state'],\n                    'done': transition['done'],\n                    'goal': new_goal\n                }\n\n                self.buffer.append(her_transition)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"domain-randomization-and-sim-to-real-transfer",children:"Domain Randomization and Sim-to-Real Transfer"}),"\n",(0,a.jsx)(n.h3,{id:"domain-randomization-in-isaac-sim",children:"Domain Randomization in Isaac Sim"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class DomainRandomization:\n    def __init__(self, environment):\n        self.env = environment\n        self.randomization_params = {\n            'lighting': {'min': 0.5, 'max': 2.0},\n            'textures': ['metal', 'wood', 'plastic'],\n            'object_sizes': {'min': 0.8, 'max': 1.2},\n            'friction': {'min': 0.1, 'max': 0.9},\n            'mass': {'min': 0.5, 'max': 2.0}\n        }\n\n    def randomize_environment(self):\n        \"\"\"Randomize environment parameters\"\"\"\n        # Randomize lighting conditions\n        self.randomize_lighting()\n\n        # Randomize object properties\n        self.randomize_object_properties()\n\n        # Randomize physics parameters\n        self.randomize_physics()\n\n        # Randomize sensor noise\n        self.randomize_sensor_noise()\n\n    def randomize_lighting(self):\n        \"\"\"Randomize lighting conditions\"\"\"\n        # Get lighting prim\n        light_prim = get_prim_at_path(\"/World/Light\")\n\n        if light_prim.IsValid():\n            # Randomize intensity\n            intensity = np.random.uniform(\n                self.randomization_params['lighting']['min'],\n                self.randomization_params['lighting']['max']\n            ) * 1000  # Base intensity\n\n            # Apply to light\n            light_prim.GetAttribute(\"intensity\").Set(intensity)\n\n    def randomize_object_properties(self):\n        \"\"\"Randomize object properties\"\"\"\n        # Get all objects in scene\n        objects = self.env.get_all_objects()\n\n        for obj in objects:\n            # Randomize size\n            size_factor = np.random.uniform(\n                self.randomization_params['object_sizes']['min'],\n                self.randomization_params['object_sizes']['max']\n            )\n\n            # Randomize material properties\n            friction = np.random.uniform(\n                self.randomization_params['friction']['min'],\n                self.randomization_params['friction']['max']\n            )\n\n            # Apply randomizations\n            obj.set_scale(size_factor)\n            obj.set_friction(friction)\n\n    def randomize_physics(self):\n        \"\"\"Randomize physics parameters\"\"\"\n        # Randomize gravity\n        gravity_magnitude = np.random.uniform(9.0, 10.0)\n        self.env.set_gravity([0, -gravity_magnitude, 0])\n\n        # Randomize time step\n        timestep = np.random.uniform(0.001, 0.01)\n        self.env.set_physics_timestep(timestep)\n\n    def randomize_sensor_noise(self):\n        \"\"\"Randomize sensor noise parameters\"\"\"\n        # Add random noise to sensor readings\n        self.env.camera.add_noise(\n            mean=0,\n            std=np.random.uniform(0.001, 0.01)\n        )\n"})}),"\n",(0,a.jsx)(n.h2,{id:"multi-task-and-transfer-learning",children:"Multi-Task and Transfer Learning"}),"\n",(0,a.jsx)(n.h3,{id:"multi-task-rl-agent",children:"Multi-Task RL Agent"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class MultiTaskAgent:\n    def __init__(self, state_dim, action_dim, num_tasks, hidden_dim=256):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.num_tasks = num_tasks\n\n        # Shared feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Task-specific heads\n        self.task_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, action_dim)\n            ) for _ in range(num_tasks)\n        ])\n\n        # Task classifier (for unsupervised task identification)\n        self.task_classifier = nn.Linear(hidden_dim, num_tasks)\n\n        self.optimizer = optim.Adam(list(self.feature_extractor.parameters()) +\n                                   list(self.task_heads.parameters()) +\n                                   list(self.task_classifier.parameters()))\n\n    def forward(self, state, task_id=None):\n        """Forward pass with optional task specification"""\n        features = self.feature_extractor(state)\n\n        if task_id is not None:\n            # Use specific task head\n            action = self.task_heads[task_id](features)\n        else:\n            # Use all task heads and select based on classification\n            task_probs = torch.softmax(self.task_classifier(features), dim=-1)\n            actions = torch.stack([head(features) for head in self.task_heads], dim=1)\n            action = torch.sum(actions * task_probs.unsqueeze(-1), dim=1)\n\n        return action\n'})}),"\n",(0,a.jsx)(n.h2,{id:"practical-rl-applications-in-robotics",children:"Practical RL Applications in Robotics"}),"\n",(0,a.jsx)(n.h3,{id:"navigation-with-rl",children:"Navigation with RL"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class RLNavigationAgent:\n    def __init__(self, state_dim, action_dim, max_linear_vel=1.0, max_angular_vel=1.0):\n        self.action_dim = action_dim\n        self.max_linear_vel = max_linear_vel\n        self.max_angular_vel = max_angular_vel\n\n        # Use DDPG for continuous navigation control\n        self.agent = DDPGAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            max_action=1.0\n        )\n\n    def get_navigation_action(self, observation):\n        """Get navigation action from RL agent"""\n        # Process observation (laser scan, goal direction, etc.)\n        processed_obs = self.process_navigation_observation(observation)\n\n        # Get action from agent\n        action = self.agent.select_action(processed_obs)\n\n        # Convert to velocity commands\n        linear_vel = action[0] * self.max_linear_vel\n        angular_vel = action[1] * self.max_angular_vel\n\n        return linear_vel, angular_vel\n\n    def process_navigation_observation(self, raw_obs):\n        """Process raw navigation observation"""\n        # Example: laser scan + goal direction + robot pose\n        laser_scan = raw_obs[\'laser_scan\']\n        goal_direction = raw_obs[\'goal_direction\']\n        robot_pose = raw_obs[\'robot_pose\']\n\n        # Normalize laser scan\n        laser_scan = np.clip(laser_scan / 10.0, 0, 1)  # Assuming max range 10m\n\n        # Combine all observations\n        processed_obs = np.concatenate([laser_scan, goal_direction, robot_pose])\n\n        return processed_obs\n'})}),"\n",(0,a.jsx)(n.h3,{id:"manipulation-with-rl",children:"Manipulation with RL"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class RLManipulationAgent:\n    def __init__(self, robot_state_dim, action_dim, max_joint_vel=1.0):\n        self.max_joint_vel = max_joint_vel\n\n        # Use TD3 for manipulation control\n        self.agent = TD3Agent(\n            state_dim=robot_state_dim,\n            action_dim=action_dim,\n            max_action=1.0\n        )\n\n    def get_manipulation_action(self, observation):\n        """Get manipulation action from RL agent"""\n        # Process observation (joint states, end-effector pose, object pose)\n        processed_obs = self.process_manipulation_observation(observation)\n\n        # Get action from agent\n        action = self.agent.select_action(processed_obs)\n\n        # Convert to joint velocity commands\n        joint_velocities = action * self.max_joint_vel\n\n        return joint_velocities\n\n    def process_manipulation_observation(self, raw_obs):\n        """Process raw manipulation observation"""\n        # Example: joint positions, velocities, end-effector pose, object pose\n        joint_positions = raw_obs[\'joint_positions\']\n        joint_velocities = raw_obs[\'joint_velocities\']\n        ee_pose = raw_obs[\'end_effector_pose\']\n        object_pose = raw_obs[\'object_pose\']\n\n        # Normalize joint positions to [-1, 1]\n        normalized_joints = np.tanh(joint_positions)  # Assuming joint limits\n\n        # Combine all observations\n        processed_obs = np.concatenate([\n            normalized_joints,\n            joint_velocities,\n            ee_pose,\n            object_pose\n        ])\n\n        return processed_obs\n'})}),"\n",(0,a.jsx)(n.h2,{id:"training-best-practices",children:"Training Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"curriculum-learning",children:"Curriculum Learning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class CurriculumLearner:\n    def __init__(self, tasks, success_threshold=0.8):\n        self.tasks = tasks  # Ordered list of tasks from easy to hard\n        self.success_threshold = success_threshold\n        self.current_task_idx = 0\n        self.task_performance = [0.0] * len(tasks)\n\n    def update_task_performance(self, task_idx, success):\n        """Update performance for a task"""\n        # Simple moving average of success rate\n        self.task_performance[task_idx] = 0.9 * self.task_performance[task_idx] + 0.1 * success\n\n    def should_advance_task(self):\n        """Check if we should advance to the next task"""\n        current_performance = self.task_performance[self.current_task_idx]\n        return current_performance >= self.success_threshold\n\n    def advance_task(self):\n        """Advance to the next task in curriculum"""\n        if self.current_task_idx < len(self.tasks) - 1 and self.should_advance_task():\n            self.current_task_idx += 1\n            print(f"Advancing to task {self.current_task_idx + 1}: {self.tasks[self.current_task_idx]}")\n\n    def get_current_task(self):\n        """Get the current task configuration"""\n        return self.tasks[self.current_task_idx]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"reward-shaping",children:"Reward Shaping"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ShapedRewardFunction:\n    def __init__(self, task_config):\n        self.task_config = task_config\n        self.weights = task_config.get(\'reward_weights\', {\n            \'progress\': 1.0,\n            \'efficiency\': 0.1,\n            \'safety\': 0.5,\n            \'completion\': 10.0\n        })\n\n    def calculate_reward(self, state, action, next_state, done):\n        """Calculate shaped reward"""\n        reward = 0.0\n\n        # Progress reward - encourage movement toward goal\n        progress_reward = self.calculate_progress_reward(state, next_state)\n        reward += self.weights[\'progress\'] * progress_reward\n\n        # Efficiency reward - penalize unnecessary movements\n        efficiency_reward = self.calculate_efficiency_reward(action)\n        reward += self.weights[\'efficiency\'] * efficiency_reward\n\n        # Safety reward - penalize dangerous states\n        safety_reward = self.calculate_safety_reward(next_state)\n        reward += self.weights[\'safety\'] * safety_reward\n\n        # Completion reward - large reward for task completion\n        if done and self.is_task_successful(next_state):\n            reward += self.weights[\'completion\']\n\n        return reward\n\n    def calculate_progress_reward(self, state, next_state):\n        """Calculate reward based on progress toward goal"""\n        # Example: distance to goal\n        current_dist = self.get_distance_to_goal(state)\n        next_dist = self.get_distance_to_goal(next_state)\n\n        # Positive if we moved closer to goal\n        return max(0, current_dist - next_dist)\n\n    def calculate_efficiency_reward(self, action):\n        """Calculate reward based on action efficiency"""\n        # Penalize large actions to encourage smooth movements\n        action_norm = np.linalg.norm(action)\n        return -0.01 * action_norm  # Small penalty for large actions\n\n    def calculate_safety_reward(self, state):\n        """Calculate reward based on safety"""\n        # Penalize being too close to obstacles\n        min_distance = self.get_min_obstacle_distance(state)\n        if min_distance < 0.2:  # 20cm threshold\n            return -1.0\n        return 0.0\n'})}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting-and-debugging",children:"Troubleshooting and Debugging"}),"\n",(0,a.jsx)(n.h3,{id:"common-rl-issues-and-solutions",children:"Common RL Issues and Solutions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class RLDiagnostic:\n    def __init__(self, agent):\n        self.agent = agent\n        self.metrics = {\n            \'rewards\': [],\n            \'losses\': [],\n            \'exploration\': [],\n            \'success_rate\': []\n        }\n\n    def diagnose_training(self):\n        """Diagnose common RL training issues"""\n        issues = []\n\n        # Check for reward stagnation\n        if self.is_reward_stagnating():\n            issues.append("Reward stagnation detected - consider reward reshaping or exploration adjustment")\n\n        # Check for loss instability\n        if self.is_loss_unstable():\n            issues.append("Loss instability detected - consider learning rate reduction or gradient clipping")\n\n        # Check for poor exploration\n        if self.is_exploration_poor():\n            issues.append("Poor exploration detected - consider increasing exploration noise")\n\n        return issues\n\n    def is_reward_stagnating(self, window=100):\n        """Check if rewards are stagnating"""\n        if len(self.metrics[\'rewards\']) < window:\n            return False\n\n        recent_rewards = self.metrics[\'rewards\'][-window:]\n        return np.std(recent_rewards) < 0.01  # Very low variance indicates stagnation\n\n    def is_loss_unstable(self, threshold=10.0):\n        """Check if losses are unstable"""\n        if not self.metrics[\'losses\']:\n            return False\n\n        return np.any(np.abs(self.metrics[\'losses\'][-10:]) > threshold)\n\n    def is_exploration_poor(self, threshold=0.1):\n        """Check if exploration is poor"""\n        if not self.metrics[\'exploration\']:\n            return True\n\n        return np.mean(self.metrics[\'exploration\'][-50:]) < threshold\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-for-rl-in-robotics",children:"Best Practices for RL in Robotics"}),"\n",(0,a.jsx)(n.h3,{id:"1-simulation-design",children:"1. Simulation Design"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Create diverse simulation environments"}),"\n",(0,a.jsx)(n.li,{children:"Implement proper domain randomization"}),"\n",(0,a.jsx)(n.li,{children:"Ensure simulation fidelity matches reality"}),"\n",(0,a.jsx)(n.li,{children:"Include realistic sensor noise models"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-reward-engineering",children:"2. Reward Engineering"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Design sparse rewards for final goals"}),"\n",(0,a.jsx)(n.li,{children:"Add shaped rewards for intermediate progress"}),"\n",(0,a.jsx)(n.li,{children:"Balance different reward components"}),"\n",(0,a.jsx)(n.li,{children:"Test reward function thoroughly"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-exploration-strategies",children:"3. Exploration Strategies"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use appropriate exploration noise"}),"\n",(0,a.jsx)(n.li,{children:"Implement curriculum learning"}),"\n",(0,a.jsx)(n.li,{children:"Consider intrinsic motivation"}),"\n",(0,a.jsx)(n.li,{children:"Monitor exploration statistics"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-safety-considerations",children:"4. Safety Considerations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement action clipping and limits"}),"\n",(0,a.jsx)(n.li,{children:"Add safety constraints to reward"}),"\n",(0,a.jsx)(n.li,{children:"Use simulation for dangerous tasks"}),"\n",(0,a.jsx)(n.li,{children:"Plan for graceful failure"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Reinforcement Learning provides a powerful framework for learning complex robotic behaviors directly from environmental interaction. The NVIDIA Isaac Platform, combined with Isaac Sim, offers an ideal environment for training RL agents with high-fidelity simulation and synthetic data generation."}),"\n",(0,a.jsx)(n.p,{children:"Key aspects of successful RL implementation in robotics include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Proper choice of RL algorithm based on action space"}),"\n",(0,a.jsx)(n.li,{children:"Effective reward function design"}),"\n",(0,a.jsx)(n.li,{children:"Domain randomization for sim-to-real transfer"}),"\n",(0,a.jsx)(n.li,{children:"Curriculum learning for complex tasks"}),"\n",(0,a.jsx)(n.li,{children:"Proper safety considerations and constraints"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"In the next section, we'll explore sim-to-real transfer techniques that enable successful deployment of simulation-trained robots to real-world applications."})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);