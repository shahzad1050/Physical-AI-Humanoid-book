"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[3898],{3871:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4/lab-humanoid-controller","title":"Practical Lab: Building a Humanoid Robot Controller","description":"Objective","source":"@site/docs/module4/lab-humanoid-controller.md","sourceDirName":"module4","slug":"/module4/lab-humanoid-controller","permalink":"/Physical-AI-Humanoid-book/docs/module4/lab-humanoid-controller","draft":false,"unlisted":false,"editUrl":"https://github.com/user/physical-ai-humanoid-robotics/tree/main/docs/module4/lab-humanoid-controller.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Robot Kinematics","permalink":"/Physical-AI-Humanoid-book/docs/module4/kinematics"},"next":{"title":"Manipulation and Grasping","permalink":"/Physical-AI-Humanoid-book/docs/module4/manipulation"}}');var s=t(4848),i=t(8453);const r={},a="Practical Lab: Building a Humanoid Robot Controller",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Project Setup and Architecture",id:"step-1-project-setup-and-architecture",level:2},{value:"Step 2: Create the Main Controller Node",id:"step-2-create-the-main-controller-node",level:2},{value:"Step 3: Create the Balance Controller",id:"step-3-create-the-balance-controller",level:2},{value:"Step 4: Create the Manipulation Controller",id:"step-4-create-the-manipulation-controller",level:2},{value:"Step 5: Create the State Estimator",id:"step-5-create-the-state-estimator",level:2},{value:"Step 6: Create Social Behavior Manager",id:"step-6-create-social-behavior-manager",level:2},{value:"Step 7: Create the Main Launch File",id:"step-7-create-the-main-launch-file",level:2},{value:"Step 8: Update setup.py",id:"step-8-update-setuppy",level:2},{value:"Step 9: Create a Simple Test Script",id:"step-9-create-a-simple-test-script",level:2},{value:"Step 10: Build and Test the Controller",id:"step-10-build-and-test-the-controller",level:2},{value:"Step 11: Advanced Testing Scenarios",id:"step-11-advanced-testing-scenarios",level:2},{value:"Expected Results",id:"expected-results",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Extensions",id:"extensions",level:2}];function _(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"practical-lab-building-a-humanoid-robot-controller",children:"Practical Lab: Building a Humanoid Robot Controller"})}),"\n",(0,s.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"In this lab, you will implement a complete humanoid robot controller that integrates all the concepts learned in Module 4. You will create a system that can maintain balance, perform basic manipulation tasks, and interact socially with humans. The controller will demonstrate coordination between balance, locomotion, manipulation, and human-robot interaction capabilities."}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed all previous modules (ROS 2, Simulation, Isaac Platform, Humanoid Robotics fundamentals)"}),"\n",(0,s.jsx)(n.li,{children:"Access to a humanoid robot simulator (Gazebo, Isaac Sim, or similar)"}),"\n",(0,s.jsx)(n.li,{children:"Python 3.8+ with required packages installed"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of control theory, kinematics, and dynamics"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"step-1-project-setup-and-architecture",children:"Step 1: Project Setup and Architecture"}),"\n",(0,s.jsx)(n.p,{children:"First, let's create the project structure for our humanoid controller:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create project directory\nmkdir -p ~/humanoid_robot_controller/src\ncd ~/humanoid_robot_controller/src\n\n# Create ROS 2 package\nros2 pkg create --build-type ament_python humanoid_controller --dependencies rclpy std_msgs sensor_msgs geometry_msgs builtin_interfaces message_filters cv_bridge tf2_ros tf2_geometry_msgs\n\n# Create directory structure\nmkdir -p ~/humanoid_robot_controller/src/humanoid_controller/{controllers,sensors,utils,behaviors}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-2-create-the-main-controller-node",children:"Step 2: Create the Main Controller Node"}),"\n",(0,s.jsxs)(n.p,{children:["Create the main controller node in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/humanoid_controller_node.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\n\"\"\"\nHumanoid Robot Controller\nIntegrates balance, manipulation, and social interaction capabilities\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Imu, Image\nfrom geometry_msgs.msg import Twist, Pose, Point, Vector3\nfrom std_msgs.msg import String, Float64MultiArray\nfrom builtin_interfaces.msg import Duration\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nfrom tf2_ros import TransformBroadcaster\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nimport time\nfrom collections import deque\n\n# Import controller modules\nfrom .controllers.balance_controller import BalanceController\nfrom .controllers.manipulation_controller import ManipulationController\nfrom .controllers.locomotion_controller import LocomotionController\nfrom .sensors.state_estimator import StateEstimator\nfrom .behaviors.social_behavior import SocialBehaviorManager\n\n\nclass HumanoidController(Node):\n    def __init__(self):\n        super().__init__('humanoid_controller')\n\n        # Initialize components\n        self.balance_controller = BalanceController(self)\n        self.manipulation_controller = ManipulationController(self)\n        self.locomotion_controller = LocomotionController(self)\n        self.state_estimator = StateEstimator(self)\n        self.social_behavior_manager = SocialBehaviorManager(self)\n\n        self.cv_bridge = CvBridge()\n\n        # Robot state variables\n        self.joint_positions = np.zeros(28)  # Example: 28 DOF humanoid\n        self.joint_velocities = np.zeros(28)\n        self.imu_data = None\n        self.camera_image = None\n        self.force_torque_data = {'left_foot': [0,0,0], 'right_foot': [0,0,0]}\n\n        # Control state\n        self.current_mode = 'idle'  # idle, balance, walk, manipulate, interact\n        self.desired_com_position = np.array([0.0, 0.0, 0.8])  # Desired CoM height: 80cm\n        self.desired_com_velocity = np.zeros(3)\n\n        # Create subscribers\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        self.camera_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.camera_callback,\n            10\n        )\n\n        self.ft_sensor_sub = self.create_subscription(\n            Float64MultiArray,\n            '/force_torque_sensors',\n            self.force_torque_callback,\n            10\n        )\n\n        # Create publishers\n        self.joint_cmd_pub = self.create_publisher(\n            Float64MultiArray,\n            '/joint_group_position_controller/commands',\n            10\n        )\n\n        self.com_pub = self.create_publisher(\n            Point,\n            '/center_of_mass',\n            10\n        )\n\n        self.zmp_pub = self.create_publisher(\n            Point,\n            '/zero_moment_point',\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            '/controller_status',\n            10\n        )\n\n        # Create service clients\n        self.service_clients = {}\n\n        # Timer for main control loop\n        self.control_timer = self.create_timer(0.01, self.main_control_loop)  # 100Hz\n\n        # TF broadcaster and listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Initialize robot state\n        self.initialize_robot()\n\n        self.get_logger().info('Humanoid Controller initialized')\n\n    def initialize_robot(self):\n        \"\"\"Initialize robot to safe configuration\"\"\"\n        # Move to neutral standing position\n        neutral_pos = self.get_neutral_standing_position()\n\n        cmd_msg = Float64MultiArray()\n        cmd_msg.data = neutral_pos.tolist()\n        self.joint_cmd_pub.publish(cmd_msg)\n\n        # Wait for robot to reach position\n        time.sleep(2.0)\n\n        # Switch to balance mode\n        self.current_mode = 'balance'\n        self.get_logger().info('Robot initialized to balance mode')\n\n    def get_neutral_standing_position(self):\n        \"\"\"Get neutral standing joint configuration\"\"\"\n        # Example neutral standing position for a typical humanoid\n        # This would be specific to your robot model\n        neutral_pos = np.array([\n            0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # Head/Neck joints\n            0.0, 0.2, -0.4, 0.2, 0.0, 0.0, 0.0,  # Left arm\n            0.0, -0.2, 0.4, -0.2, 0.0, 0.0, 0.0,  # Right arm\n            0.0, 0.0, -0.3, 0.6, -0.3, 0.0,  # Left leg\n            0.0, 0.0, -0.3, 0.6, -0.3, 0.0   # Right leg\n        ])\n\n        return neutral_pos\n\n    def joint_state_callback(self, msg):\n        \"\"\"Process joint state messages\"\"\"\n        if len(msg.position) == len(self.joint_positions):\n            self.joint_positions = np.array(msg.position)\n\n        if len(msg.velocity) == len(self.joint_velocities):\n            self.joint_velocities = np.array(msg.velocity)\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data\"\"\"\n        self.imu_data = {\n            'linear_acceleration': np.array([msg.linear_acceleration.x,\n                                           msg.linear_acceleration.y,\n                                           msg.linear_acceleration.z]),\n            'angular_velocity': np.array([msg.angular_velocity.x,\n                                        msg.angular_velocity.y,\n                                        msg.angular_velocity.z]),\n            'orientation': np.array([msg.orientation.w, msg.orientation.x,\n                                   msg.orientation.y, msg.orientation.z])\n        }\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera images\"\"\"\n        try:\n            self.camera_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\n        except Exception as e:\n            self.get_logger().error(f'Failed to convert image: {e}')\n\n    def force_torque_callback(self, msg):\n        \"\"\"Process force/torque sensor data\"\"\"\n        if len(msg.data) >= 6:  # At least 6 values for both feet\n            self.force_torque_data = {\n                'left_foot': msg.data[0:3],   # Fx, Fy, Fz\n                'right_foot': msg.data[3:6]   # Fx, Fy, Fz\n            }\n\n    def main_control_loop(self):\n        \"\"\"Main control loop that orchestrates all subsystems\"\"\"\n        # Update state estimation\n        current_state = self.state_estimator.estimate_state(\n            self.joint_positions,\n            self.joint_velocities,\n            self.imu_data,\n            self.force_torque_data\n        )\n\n        # Calculate control commands based on current mode\n        control_commands = self.compute_control_commands(current_state)\n\n        # Publish control commands\n        self.publish_control_commands(control_commands)\n\n        # Update status\n        self.publish_status()\n\n        # Execute social behaviors if in interaction mode\n        if self.current_mode == 'interact':\n            self.social_behavior_manager.execute_behaviors(current_state)\n\n    def compute_control_commands(self, current_state):\n        \"\"\"Compute control commands based on current mode\"\"\"\n        if self.current_mode == 'balance':\n            return self.balance_controller.compute_balance_control(current_state)\n        elif self.current_mode == 'walk':\n            return self.locomotion_controller.compute_locomotion_control(current_state)\n        elif self.current_mode == 'manipulate':\n            return self.manipulation_controller.compute_manipulation_control(current_state)\n        elif self.current_mode == 'interact':\n            # Combine balance, manipulation, and social behaviors\n            balance_cmds = self.balance_controller.compute_balance_control(current_state)\n            manipulation_cmds = self.manipulation_controller.compute_manipulation_control(current_state)\n            social_cmds = self.social_behavior_manager.get_social_commands(current_state)\n\n            # Combine commands with priorities\n            return self.combine_commands(balance_cmds, manipulation_cmds, social_cmds)\n        else:  # idle mode\n            # Maintain current position\n            return self.joint_positions\n\n    def combine_commands(self, balance_cmds, manipulation_cmds, social_cmds):\n        \"\"\"Combine different command types with priorities\"\"\"\n        # This is a simplified combination - in practice, you'd use null-space projections\n        # or task-priority based control\n\n        combined_commands = balance_cmds.copy()\n\n        # Add manipulation commands in null space of balance\n        # Add social behavior commands as secondary objectives\n\n        return combined_commands\n\n    def publish_control_commands(self, commands):\n        \"\"\"Publish control commands to robot\"\"\"\n        cmd_msg = Float64MultiArray()\n        cmd_msg.data = commands.tolist()\n        self.joint_cmd_pub.publish(cmd_msg)\n\n    def publish_status(self):\n        \"\"\"Publish controller status\"\"\"\n        status_msg = String()\n        status_msg.data = f\"Mode: {self.current_mode}, CoM: {self.state_estimator.com_position}\"\n        self.status_pub.publish(status_msg)\n\n        # Publish CoM and ZMP for visualization\n        com_msg = Point()\n        com_msg.x = float(self.state_estimator.com_position[0])\n        com_msg.y = float(self.state_estimator.com_position[1])\n        com_msg.z = float(self.state_estimator.com_position[2])\n        self.com_pub.publish(com_msg)\n\n        zmp_msg = Point()\n        zmp = self.state_estimator.calculate_zmp()\n        zmp_msg.x = float(zmp[0])\n        zmp_msg.y = float(zmp[1])\n        zmp_msg.z = 0.0  # ZMP is on ground plane\n        self.zmp_pub.publish(zmp_msg)\n\n    def switch_mode(self, new_mode):\n        \"\"\"Switch controller mode\"\"\"\n        if new_mode in ['idle', 'balance', 'walk', 'manipulate', 'interact']:\n            old_mode = self.current_mode\n            self.current_mode = new_mode\n\n            # Perform mode transition actions\n            if old_mode == 'balance' and new_mode != 'balance':\n                # Transitioning out of balance mode\n                pass\n            elif new_mode == 'balance' and old_mode != 'balance':\n                # Transitioning into balance mode\n                pass\n\n            self.get_logger().info(f'Switched from {old_mode} to {new_mode}')\n        else:\n            self.get_logger().warn(f'Invalid mode: {new_mode}')\n\n    def execute_simple_task(self, task_type, **kwargs):\n        \"\"\"Execute a simple task\"\"\"\n        if task_type == 'wave':\n            self.switch_mode('interact')\n            return self.social_behavior_manager.execute_wave_behavior(**kwargs)\n        elif task_type == 'reach':\n            self.switch_mode('manipulate')\n            return self.manipulation_controller.execute_reach(**kwargs)\n        elif task_type == 'step':\n            self.switch_mode('balance')\n            return self.balance_controller.execute_recovery_step(**kwargs)\n        else:\n            self.get_logger().warn(f'Unknown task type: {task_type}')\n            return False\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = HumanoidController()\n\n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        controller.get_logger().info('Shutting down Humanoid Controller...')\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-3-create-the-balance-controller",children:"Step 3: Create the Balance Controller"}),"\n",(0,s.jsxs)(n.p,{children:["Create the balance controller in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/controllers/balance_controller.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nBalance Controller for Humanoid Robot\nImplements LIPM-based balance control with ZMP tracking\n"""\n\nimport numpy as np\nimport math\nfrom scipy.spatial.transform import Rotation as R\nimport time\n\n\nclass BalanceController:\n    def __init__(self, parent_node):\n        self.parent = parent_node\n        self.com_height = 0.8  # Center of mass height (m)\n        self.gravity = 9.81\n        self.omega = math.sqrt(self.gravity / self.com_height)\n\n        # Control gains\n        self.Kp_com = np.diag([100, 100, 0])  # Position gains for CoM\n        self.Kd_com = np.diag([20, 20, 0])    # Velocity gains for CoM\n        self.Kp_zmp = np.array([50, 50])      # ZMP tracking gains\n\n        # State variables\n        self.previous_com_error = np.zeros(2)\n        self.integral_com_error = np.zeros(2)\n        self.zmp_reference = np.zeros(2)\n        self.support_polygon = self.calculate_default_support_polygon()\n\n        # Walking parameters (for stepping control)\n        self.step_width = 0.2  # Distance between feet (m)\n        self.step_length = 0.3  # Step length (m)\n        self.is_left_support = True  # Which foot is currently supporting\n\n    def compute_balance_control(self, current_state):\n        """\n        Compute balance control commands\n        """\n        # Extract current state\n        current_com = current_state[\'com_position\'][:2]  # Only x,y for balance\n        current_com_vel = current_state[\'com_velocity\'][:2]\n        current_zmp = current_state[\'zmp\'][:2]\n        support_polygon = current_state[\'support_polygon\']\n\n        # Calculate CoM tracking error\n        com_error = self.zmp_reference - current_com\n        com_vel_error = np.zeros(2) - current_com_vel  # Desired velocity is 0\n\n        # PID control for CoM\n        self.integral_com_error += com_error * 0.01  # dt = 0.01s\n        derivative_com_error = (com_error - self.previous_com_error) / 0.01\n\n        com_control = (self.Kp_com[:2, :2] @ com_error +\n                      self.Kd_com[:2, :2] @ com_vel_error +\n                      1.0 * self.integral_com_error)  # Integral gain of 1.0\n\n        # Calculate ZMP error\n        zmp_error = self.zmp_reference - current_zmp\n        zmp_control = self.Kp_zmp * zmp_error\n\n        # Combine controls\n        total_control = com_control + zmp_control\n\n        # Convert to joint torques using inverse dynamics\n        joint_commands = self.compute_joint_commands_from_balance_control(\n            total_control, current_state\n        )\n\n        # Update state for next iteration\n        self.previous_com_error = com_error.copy()\n\n        return joint_commands\n\n    def compute_joint_commands_from_balance_control(self, balance_control, current_state):\n        """\n        Convert balance control signals to joint commands\n        """\n        # This is a simplified approach - in practice, use whole-body control\n        current_joints = current_state[\'joint_positions\']\n\n        # Calculate required joint modifications to achieve balance\n        # This would typically use inverse kinematics or operational space control\n\n        # For this example, adjust ankle joints for balance\n        ankle_adjustments = self.calculate_ankle_adjustments(balance_control, current_joints)\n\n        # Apply adjustments to current joint positions\n        new_joints = current_joints.copy()\n\n        # Modify ankle joints (indices are example - depend on your robot)\n        # Assuming ankle joints are at indices 22-23 (left) and 24-25 (right)\n        new_joints[22] += ankle_adjustments[0]  # Left ankle roll\n        new_joints[23] += ankle_adjustments[1]  # Left ankle pitch\n        new_joints[24] += ankle_adjustments[2]  # Right ankle roll\n        new_joints[25] += ankle_adjustments[3]  # Right ankle pitch\n\n        # Add hip adjustments for larger balance corrections\n        hip_adjustments = self.calculate_hip_adjustments(balance_control, current_joints)\n        new_joints[16] += hip_adjustments[0]  # Left hip roll\n        new_joints[17] += hip_adjustments[1]  # Left hip pitch\n        new_joints[18] += hip_adjustments[2]  # Left hip yaw\n        new_joints[26] += hip_adjustments[3]  # Right hip roll\n        new_joints[27] += hip_adjustments[4]  # Right hip pitch\n        new_joints[28] += hip_adjustments[5]  # Right hip yaw\n\n        return new_joints\n\n    def calculate_ankle_adjustments(self, balance_control, current_joints):\n        """\n        Calculate ankle joint adjustments for balance control\n        """\n        # Balance control contains [x_control, y_control]\n        x_control, y_control = balance_control\n\n        # Convert balance commands to ankle adjustments\n        # This mapping depends on your robot\'s kinematics\n        ankle_roll_adjustment = 0.1 * y_control  # Y control -> roll\n        ankle_pitch_adjustment = -0.1 * x_control  # X control -> pitch\n\n        # Return adjustments for both ankles\n        return np.array([\n            ankle_roll_adjustment,   # Left ankle roll\n            ankle_pitch_adjustment,  # Left ankle pitch\n            -ankle_roll_adjustment,  # Right ankle roll (opposite)\n            -ankle_pitch_adjustment  # Right ankle pitch (opposite)\n        ])\n\n    def calculate_hip_adjustments(self, balance_control, current_joints):\n        """\n        Calculate hip joint adjustments for larger balance corrections\n        """\n        x_control, y_control = balance_control\n\n        # For larger disturbances, use hip joints\n        hip_roll_adjustment = 0.05 * y_control\n        hip_pitch_adjustment = -0.05 * x_control\n        hip_yaw_adjustment = 0.02 * (x_control + y_control)  # For turning\n\n        # Return adjustments for both hips\n        return np.array([\n            hip_roll_adjustment,   # Left hip roll\n            hip_pitch_adjustment,  # Left hip pitch\n            hip_yaw_adjustment,    # Left hip yaw\n            -hip_roll_adjustment,  # Right hip roll (opposite)\n            -hip_pitch_adjustment, # Right hip pitch (opposite)\n            -hip_yaw_adjustment    # Right hip yaw (opposite)\n        ])\n\n    def update_zmp_reference(self, new_zmp_ref):\n        """\n        Update the ZMP reference trajectory\n        """\n        self.zmp_reference = new_zmp_ref\n\n    def calculate_default_support_polygon(self):\n        """\n        Calculate default support polygon based on foot positions\n        """\n        # Simplified: assume feet are 20cm apart\n        foot_separation = 0.2\n        foot_length = 0.15\n        foot_width = 0.08\n\n        # Support polygon vertices (simplified as rectangle)\n        return {\n            \'min_x\': -foot_length/2,\n            \'max_x\': foot_length/2,\n            \'min_y\': -foot_separation/2 - foot_width/2,\n            \'max_y\': foot_separation/2 + foot_width/2\n        }\n\n    def is_zmp_stable(self, zmp_pos, support_polygon):\n        """\n        Check if ZMP is within support polygon\n        """\n        return (support_polygon[\'min_x\'] <= zmp_pos[0] <= support_polygon[\'max_x\'] and\n                support_polygon[\'min_y\'] <= zmp_pos[1] <= support_polygon[\'max_y\'])\n\n    def execute_recovery_step(self, current_state, direction=\'forward\'):\n        """\n        Execute a recovery step to regain balance\n        """\n        # Calculate capture point\n        current_com = current_state[\'com_position\'][:2]\n        current_com_vel = current_state[\'com_velocity\'][:2]\n\n        capture_point = current_com + current_com_vel / self.omega\n\n        # Determine step location based on direction\n        step_offset = np.array([0.0, 0.0])\n        if direction == \'forward\':\n            step_offset[0] = 0.1  # Step forward 10cm\n        elif direction == \'backward\':\n            step_offset[0] = -0.1  # Step backward 10cm\n        elif direction == \'left\':\n            step_offset[1] = 0.1  # Step left 10cm\n        elif direction == \'right\':\n            step_offset[1] = -0.1  # Step right 10cm\n\n        # Target step location\n        step_target = capture_point + step_offset\n\n        # Execute step using manipulation controller\n        # This is a simplified approach - in practice, use stepping controller\n        self.parent.manipulation_controller.execute_foot_placement(step_target)\n\n        # Update support polygon\n        self.update_support_polygon_after_step(step_target)\n\n        return True\n\n    def update_support_polygon_after_step(self, new_foot_pos):\n        """\n        Update support polygon after a step is taken\n        """\n        # This would update the support polygon based on new foot placement\n        # For now, just update the reference ZMP to the new foot location\n        self.zmp_reference = new_foot_pos\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-4-create-the-manipulation-controller",children:"Step 4: Create the Manipulation Controller"}),"\n",(0,s.jsxs)(n.p,{children:["Create the manipulation controller in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/controllers/manipulation_controller.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nManipulation Controller for Humanoid Robot\nHandles arm control, grasping, and object manipulation\n"""\n\nimport numpy as np\nimport math\nfrom scipy.spatial.transform import Rotation as R\nimport time\n\n\nclass ManipulationController:\n    def __init__(self, parent_node):\n        self.parent = parent_node\n\n        # Arm configuration\n        self.arm_dof = 7  # 7 DOF arms\n        self.left_arm_indices = slice(7, 14)   # Joints 7-13 for left arm\n        self.right_arm_indices = slice(14, 21) # Joints 14-20 for right arm\n\n        # Control parameters\n        self.kp_pos = 100.0  # Position gain\n        self.kd_pos = 20.0   # Velocity gain\n        self.kp_ori = 10.0   # Orientation gain\n        self.kd_ori = 5.0    # Angular velocity gain\n\n        # Jacobian and inverse kinematics parameters\n        self.ik_lambda = 0.01  # Damping factor for Jacobian pseudoinverse\n        self.ik_max_iter = 100\n        self.ik_tolerance = 1e-4\n\n        # State variables\n        self.left_ee_pose = np.eye(4)  # Left end-effector pose\n        self.right_ee_pose = np.eye(4)  # Right end-effector pose\n        self.left_ee_vel = np.zeros(6)  # Left end-effector velocity\n        self.right_ee_vel = np.zeros(6)  # Right end-effector velocity\n\n        # Task state\n        self.active_left_task = None\n        self.active_right_task = None\n\n    def compute_manipulation_control(self, current_state):\n        """\n        Compute manipulation control commands\n        """\n        current_joints = current_state[\'joint_positions\']\n        current_velocities = current_state[\'joint_velocities\']\n\n        # Calculate desired joint positions for manipulation tasks\n        left_arm_cmd = self.compute_left_arm_control(current_state)\n        right_arm_cmd = self.compute_right_arm_control(current_state)\n\n        # Combine with other joints\n        new_joints = current_joints.copy()\n        new_joints[self.left_arm_indices] = left_arm_cmd\n        new_joints[self.right_arm_indices] = right_arm_cmd\n\n        return new_joints\n\n    def compute_left_arm_control(self, current_state):\n        """\n        Compute control for left arm\n        """\n        if self.active_left_task is None:\n            # Return current position (no active task)\n            return current_state[\'joint_positions\'][self.left_arm_indices]\n\n        task = self.active_left_task\n        current_joints = current_state[\'joint_positions\'][self.left_arm_indices]\n\n        if task[\'type\'] == \'reach\':\n            return self.ik_reach_target(\n                current_joints,\n                task[\'target_position\'],\n                task[\'target_orientation\']\n            )\n        elif task[\'type\'] == \'grasp\':\n            return self.execute_grasp_task(current_joints, task)\n        elif task[\'type\'] == \'hold\':\n            return self.hold_object(current_joints, task)\n        else:\n            return current_joints  # No active task\n\n    def compute_right_arm_control(self, current_state):\n        """\n        Compute control for right arm\n        """\n        if self.active_right_task is None:\n            # Return current position (no active task)\n            return current_state[\'joint_positions\'][self.right_arm_indices]\n\n        task = self.active_right_task\n        current_joints = current_state[\'joint_positions\'][self.right_arm_indices]\n\n        if task[\'type\'] == \'reach\':\n            return self.ik_reach_target(\n                current_joints,\n                task[\'target_position\'],\n                task[\'target_orientation\']\n            )\n        elif task[\'type\'] == \'grasp\':\n            return self.execute_grasp_task(current_joints, task)\n        elif task[\'type\'] == \'hold\':\n            return self.hold_object(current_joints, task)\n        else:\n            return current_joints  # No active task\n\n    def ik_reach_target(self, current_joints, target_pos, target_ori=None):\n        """\n        Inverse kinematics to reach target position and orientation\n        """\n        if target_ori is None:\n            # Use current orientation if not specified\n            current_ee_pose = self.forward_kinematics(current_joints)\n            target_ori = current_ee_pose[:3, :3]\n\n        target_pose = np.eye(4)\n        target_pose[:3, 3] = target_pos\n        target_pose[:3, :3] = target_ori if isinstance(target_ori, np.ndarray) else R.from_quat(target_ori).as_matrix()\n\n        # Use iterative inverse kinematics\n        new_joints = self.iterative_ik(current_joints, target_pose)\n\n        return new_joints\n\n    def iterative_ik(self, current_joints, target_pose):\n        """\n        Iterative inverse kinematics using Jacobian transpose/pseudoinverse\n        """\n        current_joints = current_joints.copy()\n\n        for iteration in range(self.ik_max_iter):\n            # Calculate current end-effector pose\n            current_pose = self.forward_kinematics(current_joints)\n\n            # Calculate error\n            pos_error = target_pose[:3, 3] - current_pose[:3, 3]\n            ori_error = self.rotation_error(current_pose[:3, :3], target_pose[:3, :3])\n\n            # Check convergence\n            if np.linalg.norm(pos_error) < self.ik_tolerance and np.linalg.norm(ori_error) < self.ik_tolerance:\n                break\n\n            # Calculate Jacobian\n            jacobian = self.calculate_jacobian(current_joints)\n\n            # Combine position and orientation errors\n            error = np.concatenate([pos_error, ori_error])\n\n            # Calculate joint updates using damped least squares\n            I = np.eye(len(current_joints))\n            jtj_lambda = jacobian.T @ jacobian + self.ik_lambda * I\n            joint_delta = np.linalg.solve(jtj_lambda, jacobian.T @ error)\n\n            # Apply updates\n            current_joints += 0.5 * joint_delta  # 0.5 for stability\n\n            # Apply joint limits\n            current_joints = self.apply_joint_limits(current_joints)\n\n        return current_joints\n\n    def forward_kinematics(self, joint_angles):\n        """\n        Simplified forward kinematics - in practice, use robot-specific FK\n        """\n        # This is a placeholder - implement robot-specific forward kinematics\n        # For this example, return a simple transformation\n        pose = np.eye(4)\n\n        # Simplified FK based on joint angles\n        # In practice, use DH parameters or other kinematic model\n        x = 0.3 + 0.1 * math.sin(joint_angles[0])\n        y = 0.2 + 0.1 * math.cos(joint_angles[1])\n        z = 1.0 + 0.1 * math.sin(joint_angles[2])\n\n        pose[0, 3] = x\n        pose[1, 3] = y\n        pose[2, 3] = z\n\n        # Simple orientation (identity for now)\n        return pose\n\n    def calculate_jacobian(self, joint_angles):\n        """\n        Calculate geometric Jacobian - in practice, use robot-specific method\n        """\n        # This is a simplified Jacobian calculation\n        # In practice, use analytical or numerical differentiation\n        n_joints = len(joint_angles)\n        jacobian = np.zeros((6, n_joints))  # 6 DOF (pos + ori)\n\n        # Simplified Jacobian - in practice, calculate properly based on kinematics\n        for i in range(n_joints):\n            # Position part of Jacobian\n            jacobian[0:3, i] = self.calculate_position_jacobian_column(i, joint_angles)\n            # Orientation part of Jacobian\n            jacobian[3:6, i] = self.calculate_orientation_jacobian_column(i, joint_angles)\n\n        return jacobian\n\n    def calculate_position_jacobian_column(self, joint_idx, joint_angles):\n        """\n        Calculate position part of Jacobian column\n        """\n        # Simplified calculation - in practice, use proper kinematic derivation\n        return np.array([0.1, 0.1, 0.1])  # Placeholder\n\n    def calculate_orientation_jacobian_column(self, joint_idx, joint_angles):\n        """\n        Calculate orientation part of Jacobian column\n        """\n        # Simplified calculation - in practice, use proper kinematic derivation\n        return np.array([0.01, 0.01, 0.01])  # Placeholder\n\n    def rotation_error(self, current_rotation, target_rotation):\n        """\n        Calculate rotational error as angle-axis representation\n        """\n        relative_rotation = target_rotation @ current_rotation.T\n        rotation_vector = R.from_matrix(relative_rotation).as_rotvec()\n        return rotation_vector\n\n    def apply_joint_limits(self, joints):\n        """\n        Apply joint limits to joint angles\n        """\n        # Example joint limits (these should match your robot)\n        min_limits = np.array([-2.0] * len(joints))\n        max_limits = np.array([2.0] * len(joints))\n\n        return np.clip(joints, min_limits, max_limits)\n\n    def execute_grasp_task(self, current_joints, task):\n        """\n        Execute grasp task\n        """\n        # Move to pre-grasp position\n        pre_grasp_pos = task[\'object_position\'] + np.array([0, 0, 0.1])  # 10cm above object\n        pre_grasp_ori = task[\'grasp_orientation\']\n\n        # Move to pre-grasp\n        joints = self.ik_reach_target(current_joints, pre_grasp_pos, pre_grasp_ori)\n\n        # Move down to object\n        grasp_pos = task[\'object_position\']\n        joints = self.ik_reach_target(joints, grasp_pos, pre_grasp_ori)\n\n        # Close gripper (simplified)\n        # In practice, control gripper separately\n        joints = self.execute_gripper_control(joints, \'close\')\n\n        # Lift object\n        lift_pos = grasp_pos + np.array([0, 0, 0.1])  # Lift 10cm\n        joints = self.ik_reach_target(joints, lift_pos, pre_grasp_ori)\n\n        return joints\n\n    def execute_gripper_control(self, joints, command):\n        """\n        Control gripper (simplified)\n        """\n        # In practice, this would control actual gripper joints\n        # For this example, we\'ll just return the joints unchanged\n        return joints\n\n    def hold_object(self, current_joints, task):\n        """\n        Hold object at specified location\n        """\n        # Maintain grasp while possibly moving to new location\n        target_pos = task.get(\'hold_position\', self.get_current_ee_position(current_joints))\n        target_ori = task.get(\'hold_orientation\', self.get_current_ee_orientation(current_joints))\n\n        return self.ik_reach_target(current_joints, target_pos, target_ori)\n\n    def get_current_ee_position(self, joints):\n        """\n        Get current end-effector position\n        """\n        pose = self.forward_kinematics(joints)\n        return pose[:3, 3]\n\n    def get_current_ee_orientation(self, joints):\n        """\n        Get current end-effector orientation\n        """\n        pose = self.forward_kinematics(joints)\n        return pose[:3, :3]\n\n    def execute_reach(self, target_position, arm=\'right\', orientation=None):\n        """\n        Execute reach task with specified arm\n        """\n        task = {\n            \'type\': \'reach\',\n            \'target_position\': np.array(target_position),\n            \'target_orientation\': orientation\n        }\n\n        if arm == \'left\':\n            self.active_left_task = task\n        else:  # right\n            self.active_right_task = task\n\n    def execute_grasp(self, object_position, grasp_type=\'top\', arm=\'right\'):\n        """\n        Execute grasp task\n        """\n        # Determine grasp orientation based on grasp type\n        if grasp_type == \'top\':\n            grasp_orientation = R.from_euler(\'xyz\', [0, 0, 0]).as_matrix()  # Grasp from top\n        elif grasp_type == \'side\':\n            grasp_orientation = R.from_euler(\'xyz\', [0, np.pi/2, 0]).as_matrix()  # Grasp from side\n        else:\n            grasp_orientation = R.from_euler(\'xyz\', [0, 0, 0]).as_matrix()  # Default\n\n        task = {\n            \'type\': \'grasp\',\n            \'object_position\': np.array(object_position),\n            \'grasp_orientation\': grasp_orientation\n        }\n\n        if arm == \'left\':\n            self.active_left_task = task\n        else:  # right\n            self.active_right_task = task\n\n    def execute_hold(self, hold_position, arm=\'right\'):\n        """\n        Execute hold task\n        """\n        task = {\n            \'type\': \'hold\',\n            \'hold_position\': np.array(hold_position)\n        }\n\n        if arm == \'left\':\n            self.active_left_task = task\n        else:  # right\n            self.active_right_task = task\n\n    def execute_foot_placement(self, target_position):\n        """\n        Execute foot placement for stepping (simplified)\n        """\n        # This would coordinate with balance controller for stepping\n        # For this example, just return True\n        return True\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-5-create-the-state-estimator",children:"Step 5: Create the State Estimator"}),"\n",(0,s.jsxs)(n.p,{children:["Create the state estimator in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/sensors/state_estimator.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nState Estimator for Humanoid Robot\nEstimates robot state including CoM, ZMP, joint states, etc.\n"""\n\nimport numpy as np\nimport math\nfrom scipy.spatial.transform import Rotation as R\nfrom collections import deque\nimport time\n\n\nclass StateEstimator:\n    def __init__(self, parent_node):\n        self.parent = parent_node\n\n        # State variables\n        self.com_position = np.array([0.0, 0.0, 0.8])  # Initial CoM at 80cm height\n        self.com_velocity = np.zeros(3)\n        self.com_acceleration = np.zeros(3)\n\n        self.zmp = np.zeros(3)  # Zero Moment Point\n        self.support_polygon = self.calculate_initial_support_polygon()\n\n        self.foot_positions = {\n            \'left\': np.array([0.0, 0.1, 0.0]),   # Left foot position\n            \'right\': np.array([0.0, -0.1, 0.0])  # Right foot position\n        }\n\n        # Estimation parameters\n        self.com_height = 0.8  # Fixed CoM height assumption for LIPM\n        self.gravity = 9.81\n        self.omega = math.sqrt(self.gravity / self.com_height)\n\n        # Filtering parameters\n        self.com_velocity_filter = self.initialize_filter(3, 5)  # 3D, 5 taps\n        self.com_position_history = deque(maxlen=10)  # Keep last 10 positions\n\n        # Sensor fusion weights\n        self.kf_process_noise = np.diag([0.01, 0.01, 0.1])  # Process noise\n        self.kf_measurement_noise = np.diag([0.001, 0.001, 0.01])  # Measurement noise\n\n    def estimate_state(self, joint_positions, joint_velocities, imu_data, ft_data):\n        """\n        Estimate robot state from sensor data\n        """\n        # Update CoM position and velocity\n        self.update_com_state(joint_positions)\n\n        # Update ZMP from force/torque sensors\n        self.update_zmp(ft_data)\n\n        # Update support polygon\n        self.update_support_polygon()\n\n        # Update foot positions\n        self.update_foot_positions(joint_positions)\n\n        # Package state for controller\n        state = {\n            \'com_position\': self.com_position.copy(),\n            \'com_velocity\': self.com_velocity.copy(),\n            \'com_acceleration\': self.com_acceleration.copy(),\n            \'zmp\': self.zmp.copy(),\n            \'support_polygon\': self.support_polygon.copy(),\n            \'foot_positions\': self.foot_positions.copy(),\n            \'joint_positions\': joint_positions,\n            \'joint_velocities\': joint_velocities,\n            \'imu_data\': imu_data,\n            \'force_torque_data\': ft_data\n        }\n\n        return state\n\n    def update_com_state(self, joint_positions):\n        """\n        Update CoM position and velocity from joint positions\n        """\n        # Calculate CoM position from joint configuration\n        # This is a simplified approach - in practice, use full kinematic model\n        new_com_pos = self.calculate_com_from_joints(joint_positions)\n\n        # Update position history\n        self.com_position_history.append(new_com_pos)\n\n        # Calculate velocity from position differences\n        if len(self.com_position_history) >= 2:\n            dt = 0.01  # Assuming 100Hz control\n            pos_diff = new_com_pos - self.com_position_history[-2]\n            new_com_vel = pos_diff / dt\n\n            # Apply simple filtering to velocity\n            if np.linalg.norm(self.com_velocity) > 0:\n                self.com_velocity = 0.7 * self.com_velocity + 0.3 * new_com_vel\n            else:\n                self.com_velocity = new_com_vel\n\n        # Update CoM position (apply some smoothing)\n        alpha = 0.9  # Smoothing factor\n        self.com_position = alpha * self.com_position + (1 - alpha) * new_com_pos\n\n    def calculate_com_from_joints(self, joint_positions):\n        """\n        Calculate CoM from joint positions using simplified model\n        """\n        # This would use the full kinematic model in practice\n        # For this example, use a simplified calculation\n\n        # Simplified CoM calculation based on joint positions\n        # In practice, use mass distribution and full forward kinematics\n        base_com = np.array([0.0, 0.0, 0.8])  # Base CoM position\n\n        # Add influence from joint positions\n        # This is a very simplified model\n        joint_influence = np.zeros(3)\n\n        # Influence from leg joints (affects x,y,z)\n        leg_joints = joint_positions[21:29]  # Assuming leg joints are at indices 21-28\n        for i, joint in enumerate(leg_joints):\n            # Each leg joint has some influence on CoM position\n            influence = 0.01 * math.sin(joint)  # Simplified influence\n            joint_influence[0] += influence * ((i % 3) == 0)  # x influence\n            joint_influence[1] += influence * ((i % 3) == 1)  # y influence\n            joint_influence[2] += influence * ((i % 3) == 2)  # z influence\n\n        return base_com + joint_influence\n\n    def update_zmp(self, ft_data):\n        """\n        Update Zero Moment Point from force/torque sensors\n        """\n        # Calculate ZMP from foot force/torque measurements\n        # ZMP_x = (M_y + F_z * h) / F_x  (simplified)\n        # ZMP_y = (-M_x + F_z * h) / F_y (simplified)\n\n        # Get forces from both feet\n        left_force = np.array(ft_data[\'left_foot\'])\n        right_force = np.array(ft_data[\'right_foot\'])\n\n        # Calculate total forces and moments\n        total_force = left_force + right_force\n\n        # Simplified ZMP calculation\n        # In practice, use full moment calculations from both feet\n        if total_force[2] != 0:  # Fz should not be zero\n            zmp_x = -(left_force[4] + right_force[4]) / total_force[2]  # Moment_y / Force_z\n            zmp_y = (left_force[3] + right_force[3]) / total_force[2]   # Moment_x / Force_z\n        else:\n            zmp_x, zmp_y = 0, 0  # Default to origin if no vertical force\n\n        self.zmp = np.array([zmp_x, zmp_y, 0.0])\n\n    def update_support_polygon(self):\n        """\n        Update support polygon based on foot contact\n        """\n        # Calculate support polygon from foot positions\n        # This is simplified - in practice, consider foot geometry and contact points\n\n        left_pos = self.foot_positions[\'left\']\n        right_pos = self.foot_positions[\'right\']\n\n        # Create bounding box as support polygon\n        min_x = min(left_pos[0], right_pos[0]) - 0.05  # Add small margin\n        max_x = max(left_pos[0], right_pos[0]) + 0.05\n        min_y = min(left_pos[1], right_pos[1]) - 0.1   # Larger margin in y\n        max_y = max(left_pos[1], right_pos[1]) + 0.1\n\n        self.support_polygon = {\n            \'min_x\': min_x,\n            \'max_x\': max_x,\n            \'min_y\': min_y,\n            \'max_y\': max_y\n        }\n\n    def update_foot_positions(self, joint_positions):\n        """\n        Update foot positions from joint configuration\n        """\n        # Calculate foot positions using forward kinematics\n        # This is simplified - in practice, use full FK\n        self.foot_positions[\'left\'] = self.calculate_foot_position(joint_positions, \'left\')\n        self.foot_positions[\'right\'] = self.calculate_foot_position(joint_positions, \'right\')\n\n    def calculate_foot_position(self, joint_positions, foot_side):\n        """\n        Calculate foot position using simplified forward kinematics\n        """\n        # This would use full robot kinematics in practice\n        # For this example, use a simplified model\n\n        if foot_side == \'left\':\n            # Use left leg joint positions (indices are example)\n            leg_joints = joint_positions[21:27]  # Assuming left leg joints are 21-26\n        else:  # right\n            leg_joints = joint_positions[27:33]  # Assuming right leg joints are 27-32\n\n        # Simplified calculation of foot position based on leg joints\n        # In practice, use full DH parameters or kinematic model\n        foot_pos = np.array([0.0, 0.1 if foot_side == \'left\' else -0.1, 0.0])\n\n        # Add influence from joint angles\n        for i, angle in enumerate(leg_joints):\n            foot_pos[0] += 0.05 * math.sin(angle + i * 0.5)  # Simplified influence\n            foot_pos[1] += 0.02 * math.cos(angle + i * 0.3)  # Simplified influence\n            foot_pos[2] -= 0.08  # Foot is below hip (negative z)\n\n        return foot_pos\n\n    def calculate_initial_support_polygon(self):\n        """\n        Calculate initial support polygon\n        """\n        return {\n            \'min_x\': -0.1,\n            \'max_x\': 0.1,\n            \'min_y\': -0.2,\n            \'max_y\': 0.2\n        }\n\n    def initialize_filter(self, state_dim, taps):\n        """\n        Initialize filter for state estimation\n        """\n        return np.ones(taps) / taps  # Simple moving average\n\n    def is_balanced(self):\n        """\n        Check if robot is balanced based on CoM and ZMP\n        """\n        # Check if ZMP is within support polygon\n        zmp_in_polygon = (self.support_polygon[\'min_x\'] <= self.zmp[0] <= self.support_polygon[\'max_x\'] and\n                         self.support_polygon[\'min_y\'] <= self.zmp[1] <= self.support_polygon[\'max_y\'])\n\n        # Check CoM position relative to feet\n        com_stable = abs(self.com_position[0]) < 0.1 and abs(self.com_position[1]) < 0.15\n\n        return zmp_in_polygon and com_stable\n\n    def calculate_capture_point(self):\n        """\n        Calculate capture point for balance recovery\n        """\n        # Capture point: where to step to stop the CoM\n        capture_point = self.com_position[:2] + self.com_velocity[:2] / self.omega\n        return capture_point\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-6-create-social-behavior-manager",children:"Step 6: Create Social Behavior Manager"}),"\n",(0,s.jsxs)(n.p,{children:["Create the social behavior manager in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/src/humanoid_controller/behaviors/social_behavior.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nSocial Behavior Manager for Humanoid Robot\nManages social interactions, expressions, and communication\n"""\n\nimport numpy as np\nimport math\nimport time\nfrom scipy.spatial.transform import Rotation as R\n\n\nclass SocialBehaviorManager:\n    def __init__(self, parent_node):\n        self.parent = parent_node\n\n        # Social behavior parameters\n        self.social_space_radius = 1.2  # Personal space radius (m)\n        self.greeting_distance = 1.0    # Distance for greeting (m)\n        self.attention_span = 5.0       # Time to pay attention (seconds)\n\n        # Behavior state\n        self.current_behavior = \'idle\'\n        self.behavior_start_time = time.time()\n        self.attended_person = None\n        self.interaction_intensity = 0.5  # 0-1 scale\n\n        # Expression and gesture libraries\n        self.expressions = {\n            \'neutral\': [0, 0, 0, 0],      # [brow, eyes, mouth, cheeks]\n            \'happy\': [0, 1, 1, 1],        # Raised brows, smiling eyes, smile, raised cheeks\n            \'sad\': [-1, -1, -1, 0],       # Lowered brows, sad eyes, frown, neutral cheeks\n            \'surprised\': [1, 1, 0, 0],    # Raised brows, wide eyes, neutral mouth\n            \'attentive\': [0, 0.5, 0, 0]   # Slightly raised brows for attention\n        }\n\n        self.gestures = {\n            \'wave\': self.wave_gesture,\n            \'point\': self.point_gesture,\n            \'nod\': self.nod_gesture,\n            \'shake_head\': self.shake_head_gesture,\n            \'beckon\': self.beckon_gesture\n        }\n\n    def execute_behaviors(self, current_state):\n        """\n        Execute social behaviors based on current state\n        """\n        # Check if there are humans nearby\n        humans_nearby = self.detect_humans_in_field_of_view(current_state)\n\n        if humans_nearby:\n            # Select appropriate behavior based on context\n            self.select_and_execute_behavior(humans_nearby, current_state)\n        else:\n            # Return to idle behavior\n            self.current_behavior = \'idle\'\n            self.set_expression(\'neutral\')\n\n    def detect_humans_in_field_of_view(self, current_state):\n        """\n        Detect humans in robot\'s field of view (simplified)\n        """\n        # This would use vision processing in practice\n        # For this example, return a mock detection\n        camera_image = self.parent.camera_image\n        if camera_image is not None:\n            # Mock detection - in practice, use object detection\n            return [{\'position\': np.array([1.0, 0.0, 0.0]), \'distance\': 1.0}]\n        return []\n\n    def select_and_execute_behavior(self, humans, current_state):\n        """\n        Select and execute appropriate social behavior\n        """\n        closest_human = min(humans, key=lambda h: h[\'distance\'])\n        distance = closest_human[\'distance\']\n\n        if distance <= self.greeting_distance and self.current_behavior != \'greeting\':\n            # Close enough for greeting\n            self.execute_greeting_behavior(closest_human)\n        elif distance <= self.social_space_radius:\n            # In personal space, be attentive\n            self.maintain_attention_behavior(closest_human)\n        else:\n            # Outside personal space, acknowledge presence\n            self.acknowledge_presence_behavior(closest_human)\n\n    def execute_greeting_behavior(self, human):\n        """\n        Execute greeting behavior\n        """\n        if self.current_behavior != \'greeting\':\n            self.current_behavior = \'greeting\'\n            self.behavior_start_time = time.time()\n\n            # Wave gesture\n            self.execute_wave_behavior(target=human[\'position\'])\n\n            # Friendly expression\n            self.set_expression(\'happy\')\n\n            # Say greeting\n            self.speak_greeting()\n\n    def maintain_attention_behavior(self, human):\n        """\n        Maintain attention to nearby human\n        """\n        if self.current_behavior != \'attentive\':\n            self.current_behavior = \'attentive\'\n            self.behavior_start_time = time.time()\n\n            # Attentive expression\n            self.set_expression(\'attentive\')\n\n            # Maintain gaze\n            self.maintain_gaze_on_human(human)\n\n    def acknowledge_presence_behavior(self, human):\n        """\n        Acknowledge human presence from distance\n        """\n        if self.current_behavior != \'acknowledge\':\n            self.current_behavior = \'acknowledge\'\n            self.behavior_start_time = time.time()\n\n            # Brief nod gesture\n            self.execute_nod_behavior()\n\n            # Neutral expression\n            self.set_expression(\'neutral\')\n\n    def execute_wave_behavior(self, target=None, duration=2.0):\n        """\n        Execute waving gesture\n        """\n        # Plan wave trajectory\n        wave_trajectory = self.plan_wave_trajectory(target)\n\n        # Execute wave motion\n        start_time = time.time()\n        while time.time() - start_time < duration:\n            # Follow wave trajectory\n            self.follow_arm_trajectory(\'right\', wave_trajectory)\n            time.sleep(0.01)  # 100Hz\n\n    def plan_wave_trajectory(self, target=None):\n        """\n        Plan waving trajectory\n        """\n        # Simplified wave trajectory\n        trajectory = []\n        amplitude = 0.1  # 10cm amplitude\n\n        for t in np.linspace(0, 2*np.pi, 20):  # 20 points for wave\n            x_offset = 0.3  # Extend arm\n            y_offset = amplitude * math.sin(t)  # Vertical oscillation\n            z_offset = 0.05 * math.cos(t)  # Forward-back oscillation\n\n            position = np.array([x_offset, y_offset, z_offset])\n            trajectory.append(position)\n\n        return trajectory\n\n    def execute_nod_behavior(self, count=1):\n        """\n        Execute nodding gesture\n        """\n        for i in range(count):\n            # Nod down\n            self.move_head([0.2, 0, 0])  # Pitch down\n            time.sleep(0.3)\n\n            # Nod up\n            self.move_head([0, 0, 0])  # Return to neutral\n            time.sleep(0.3)\n\n    def set_expression(self, expression_name):\n        """\n        Set facial expression\n        """\n        if expression_name in self.expressions:\n            expression_values = self.expressions[expression_name]\n            # In practice, send these values to facial actuation system\n            print(f"Setting expression: {expression_name} with values {expression_values}")\n\n    def speak_greeting(self):\n        """\n        Speak greeting message\n        """\n        greeting_messages = [\n            "Hello! Nice to meet you!",\n            "Hi there! How can I help you?",\n            "Greetings! I\'m your humanoid assistant."\n        ]\n\n        # In practice, use text-to-speech system\n        import random\n        message = random.choice(greeting_messages)\n        print(f"Speaking: {message}")\n\n    def maintain_gaze_on_human(self, human):\n        """\n        Maintain gaze on human\n        """\n        # Calculate where to look\n        human_pos = human[\'position\']\n\n        # In practice, control head/eye movements\n        print(f"Maintaining gaze on human at {human_pos}")\n\n    def move_head(self, joint_angles):\n        """\n        Move head to specified joint angles\n        """\n        # In practice, send commands to head joints\n        print(f"Moving head to angles: {joint_angles}")\n\n    def follow_arm_trajectory(self, arm_side, trajectory):\n        """\n        Follow arm trajectory\n        """\n        # In practice, use trajectory controller\n        print(f"Following {arm_side} arm trajectory")\n\n    def get_social_commands(self, current_state):\n        """\n        Get social behavior commands to combine with other controllers\n        """\n        # Return commands that complement balance and manipulation\n        social_commands = {\n            \'head_orientation\': self.get_head_orientation_command(),\n            \'facial_expression\': self.get_current_expression(),\n            \'gesture_commands\': self.get_active_gesture_commands()\n        }\n\n        return social_commands\n\n    def get_head_orientation_command(self):\n        """\n        Get head orientation command for social interaction\n        """\n        # If attending to someone, orient head toward them\n        if self.attended_person:\n            # Calculate direction to attended person\n            direction_to_person = self.attended_person[\'position\'] - self.parent.state_estimator.com_position\n            direction_xy = direction_to_person[:2]  # Only x,y for head orientation\n\n            if np.linalg.norm(direction_xy) > 0.1:  # Avoid division by zero\n                direction_xy = direction_xy / np.linalg.norm(direction_xy)\n\n                # Convert to head joint commands\n                # This would map to actual head joint angles in practice\n                head_yaw = math.atan2(direction_xy[1], direction_xy[0])\n                head_pitch = 0  # Keep level for now\n\n                return {\'yaw\': head_yaw, \'pitch\': head_pitch}\n\n        # Default: look forward\n        return {\'yaw\': 0, \'pitch\': 0}\n\n    def get_current_expression(self):\n        """\n        Get current facial expression\n        """\n        return self.current_behavior\n\n    def get_active_gesture_commands(self):\n        """\n        Get commands for any active gestures\n        """\n        # Return any ongoing gesture commands\n        return []\n\n    def execute_social_interaction(self, interaction_type, target_person=None):\n        """\n        Execute specific type of social interaction\n        """\n        if interaction_type == \'greeting\':\n            self.execute_greeting_interaction(target_person)\n        elif interaction_type == \'farewell\':\n            self.execute_farewell_interaction(target_person)\n        elif interaction_type == \'assistance_request\':\n            self.execute_assistance_interaction(target_person)\n        elif interaction_type == \'conversation\':\n            self.execute_conversation_interaction(target_person)\n\n    def execute_greeting_interaction(self, target_person):\n        """\n        Execute full greeting interaction\n        """\n        print("Executing greeting interaction...")\n\n        # Move to appropriate distance\n        self.move_to_interaction_distance(target_person, self.greeting_distance)\n\n        # Make eye contact\n        self.maintain_gaze_on_human(target_person)\n\n        # Wave\n        self.execute_wave_behavior(target=target_person[\'position\'])\n\n        # Smile\n        self.set_expression(\'happy\')\n\n        # Speak\n        self.speak_greeting()\n\n    def move_to_interaction_distance(self, target_person, desired_distance):\n        """\n        Move to appropriate distance for interaction\n        """\n        # Calculate current distance\n        current_pos = self.parent.state_estimator.com_position\n        target_pos = target_person[\'position\']\n        current_distance = np.linalg.norm(target_pos[:2] - current_pos[:2])\n\n        # If too far, move closer (simplified)\n        if current_distance > desired_distance + 0.2:  # 20cm tolerance\n            print(f"Moving closer to person, current: {current_distance:.2f}m, desired: {desired_distance:.2f}m")\n            # In practice, use locomotion controller to move\n\n    def execute_farewell_interaction(self, target_person):\n        """\n        Execute farewell interaction\n        """\n        print("Executing farewell interaction...")\n\n        # Wave goodbye\n        self.execute_wave_behavior(target=target_person[\'position\'])\n\n        # Set sad/sympathetic expression\n        self.set_expression(\'happy\')  # Actually happy to say goodbye in a positive way\n\n        # Speak farewell\n        farewell_messages = [\n            "Goodbye! Have a great day!",\n            "See you later!",\n            "Take care!"\n        ]\n        import random\n        message = random.choice(farewell_messages)\n        print(f"Speaking: {message}")\n\n    def execute_assistance_interaction(self, target_person):\n        """\n        Execute interaction for requesting/providing assistance\n        """\n        print("Executing assistance interaction...")\n\n        # Lean forward slightly to show attention\n        # In practice, adjust posture\n\n        # Make direct eye contact\n        self.maintain_gaze_on_human(target_person)\n\n        # Set attentive expression\n        self.set_expression(\'attentive\')\n\n        # Speak assistance message\n        assistance_messages = [\n            "How can I assist you?",\n            "I\'m here to help. What do you need?",\n            "Please let me know if you need any help."\n        ]\n        import random\n        message = random.choice(assistance_messages)\n        print(f"Speaking: {message}")\n\n    def execute_conversation_interaction(self, target_person):\n        """\n        Execute conversation-like interaction\n        """\n        print("Executing conversation interaction...")\n\n        # Maintain natural gaze\n        self.maintain_gaze_on_human(target_person)\n\n        # Use conversational expressions (vary over time)\n        conversation_expressions = [\'attentive\', \'happy\', \'neutral\']\n        import random\n        self.set_expression(random.choice(conversation_expressions))\n\n        # Nod occasionally to show understanding\n        if time.time() - self.behavior_start_time > 3:  # Every 3 seconds\n            self.execute_nod_behavior(count=1)\n            self.behavior_start_time = time.time()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-7-create-the-main-launch-file",children:"Step 7: Create the Main Launch File"}),"\n",(0,s.jsxs)(n.p,{children:["Create the launch file in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/launch/humanoid_controller.launch.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.event_handlers import OnProcessStart\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='true',\n        description='Use simulation (Gazebo) clock if true'\n    )\n\n    # Humanoid controller node\n    humanoid_controller = Node(\n        package='humanoid_controller',\n        executable='humanoid_controller_node',\n        name='humanoid_controller',\n        parameters=[\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\n        ],\n        output='screen'\n    )\n\n    # Additional nodes could be added here:\n    # - Vision processing node\n    # - Audio processing node\n    # - High-level task planner\n\n    return LaunchDescription([\n        use_sim_time,\n        humanoid_controller\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-8-update-setuppy",children:"Step 8: Update setup.py"}),"\n",(0,s.jsxs)(n.p,{children:["Update the setup.py file in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/setup.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\n\npackage_name = 'humanoid_controller'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name,\n              f'{package_name}.controllers',\n              f'{package_name}.sensors',\n              f'{package_name}.utils',\n              f'{package_name}.behaviors'],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        ('share/' + package_name + '/launch', ['launch/humanoid_controller.launch.py']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Your Name',\n    maintainer_email='your.email@example.com',\n    description='Humanoid Robot Controller integrating balance, manipulation, and social interaction',\n    license='Apache License 2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'humanoid_controller_node = humanoid_controller.humanoid_controller_node:main',\n        ],\n    },\n)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-9-create-a-simple-test-script",children:"Step 9: Create a Simple Test Script"}),"\n",(0,s.jsxs)(n.p,{children:["Create a test script in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/test_controller.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\n\"\"\"\nTest script for Humanoid Controller\nDemonstrates basic functionality\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point\nimport time\n\n\nclass ControllerTester(Node):\n    def __init__(self):\n        super().__init__('controller_tester')\n\n        # Create subscriber to controller status\n        self.status_sub = self.create_subscription(\n            String,\n            '/controller_status',\n            self.status_callback,\n            10\n        )\n\n        # Create subscriber to CoM position\n        self.com_sub = self.create_subscription(\n            Point,\n            '/center_of_mass',\n            self.com_callback,\n            10\n        )\n\n        # Create subscriber to ZMP\n        self.zmp_sub = self.create_subscription(\n            Point,\n            '/zero_moment_point',\n            self.zmp_callback,\n            10\n        )\n\n        # Store latest values\n        self.latest_status = None\n        self.latest_com = None\n        self.latest_zmp = None\n\n        self.get_logger().info('Controller Tester initialized')\n\n    def status_callback(self, msg):\n        self.latest_status = msg.data\n        self.get_logger().info(f'Controller Status: {msg.data}')\n\n    def com_callback(self, msg):\n        self.latest_com = [msg.x, msg.y, msg.z]\n        # Don't log every message to avoid spam\n\n    def zmp_callback(self, msg):\n        self.latest_zmp = [msg.x, msg.y, msg.z]\n        # Don't log every message to avoid spam\n\n    def test_basic_functions(self):\n        \"\"\"Test basic controller functions\"\"\"\n        self.get_logger().info('Testing basic controller functions...')\n\n        # Wait a bit for messages to arrive\n        time.sleep(2.0)\n\n        if self.latest_status:\n            self.get_logger().info(f'Current status: {self.latest_status}')\n\n        if self.latest_com:\n            self.get_logger().info(f'Current CoM: [{self.latest_com[0]:.3f}, {self.latest_com[1]:.3f}, {self.latest_com[2]:.3f}]')\n\n        if self.latest_zmp:\n            self.get_logger().info(f'Current ZMP: [{self.latest_zmp[0]:.3f}, {self.latest_zmp[1]:.3f}, {self.latest_zmp[2]:.3f}]')\n\n        # Test mode switching (this would require service calls in practice)\n        self.get_logger().info('Test completed successfully!')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    tester = ControllerTester()\n\n    # Run basic tests\n    tester.test_basic_functions()\n\n    # Keep node alive briefly to receive messages\n    time.sleep(5.0)\n\n    tester.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-10-build-and-test-the-controller",children:"Step 10: Build and Test the Controller"}),"\n",(0,s.jsx)(n.p,{children:"Now let's build the package and test it:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Build the package\ncd ~/humanoid_robot_controller\ncolcon build --packages-select humanoid_controller\nsource install/setup.bash\n\n# Terminal 2: Run the controller (in simulation environment)\nros2 launch humanoid_controller humanoid_controller.launch.py\n\n# Terminal 3: Test the controller\nros2 run humanoid_controller test_controller\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-11-advanced-testing-scenarios",children:"Step 11: Advanced Testing Scenarios"}),"\n",(0,s.jsxs)(n.p,{children:["Create an advanced testing script in ",(0,s.jsx)(n.code,{children:"~/humanoid_robot_controller/test_advanced.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\n\"\"\"\nAdvanced test script for Humanoid Controller\nTests complex scenarios including balance recovery, manipulation, and interaction\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point, Twist\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Float64MultiArray\nimport time\nimport numpy as np\n\n\nclass AdvancedControllerTester(Node):\n    def __init__(self):\n        super().__init__('advanced_controller_tester')\n\n        # Publishers for testing\n        self.mode_cmd_pub = self.create_publisher(String, '/controller_mode_cmd', 10)\n        self.test_cmd_pub = self.create_publisher(String, '/controller_test_cmd', 10)\n\n        # Subscribers\n        self.status_sub = self.create_subscription(\n            String,\n            '/controller_status',\n            self.status_callback,\n            10\n        )\n\n        self.com_sub = self.create_subscription(\n            Point,\n            '/center_of_mass',\n            self.com_callback,\n            10\n        )\n\n        self.zmp_sub = self.create_subscription(\n            Point,\n            '/zero_moment_point',\n            self.zmp_callback,\n            10\n        )\n\n        # Test state\n        self.test_results = {}\n        self.current_test = None\n\n        self.get_logger().info('Advanced Controller Tester initialized')\n\n    def status_callback(self, msg):\n        if self.current_test:\n            print(f'Test {self.current_test}: Status - {msg.data}')\n\n    def com_callback(self, msg):\n        if self.current_test and 'balance' in self.current_test:\n            com = np.array([msg.x, msg.y, msg.z])\n            self.test_results[self.current_test]['com_history'].append(com)\n\n    def zmp_callback(self, msg):\n        if self.current_test and 'balance' in self.current_test:\n            zmp = np.array([msg.x, msg.y, msg.z])\n            self.test_results[self.current_test]['zmp_history'].append(zmp)\n\n    def run_comprehensive_test(self):\n        \"\"\"Run comprehensive tests of all controller capabilities\"\"\"\n        self.get_logger().info('Starting comprehensive controller test...')\n\n        # Test 1: Balance control\n        self.run_balance_test()\n\n        # Test 2: Basic manipulation\n        self.run_manipulation_test()\n\n        # Test 3: Social interaction\n        self.run_interaction_test()\n\n        # Test 4: Integrated behavior\n        self.run_integration_test()\n\n        self.print_test_summary()\n\n    def run_balance_test(self):\n        \"\"\"Test balance control capabilities\"\"\"\n        self.current_test = 'balance_basic'\n        self.test_results[self.current_test] = {\n            'passed': False,\n            'com_history': [],\n            'zmp_history': [],\n            'start_time': time.time()\n        }\n\n        self.get_logger().info('Running basic balance test...')\n\n        # Let it run for 10 seconds\n        time.sleep(10.0)\n\n        # Analyze results\n        com_history = self.test_results[self.current_test]['com_history']\n        zmp_history = self.test_results[self.current_test]['zmp_history']\n\n        if len(com_history) > 50 and len(zmp_history) > 50:\n            # Calculate stability metrics\n            com_stability = np.std([c[0:2] for c in com_history[-20:]])  # Last 20 CoM positions\n            zmp_stability = np.std([z[0:2] for z in zmp_history[-20:]])  # Last 20 ZMP positions\n\n            self.get_logger().info(f'Balance test - CoM stability: {np.mean(com_stability):.3f}, ZMP stability: {np.mean(zmp_stability):.3f}')\n\n            # Check if stable (thresholds are arbitrary for demo)\n            if np.mean(com_stability) < 0.05 and np.mean(zmp_stability) < 0.05:\n                self.test_results[self.current_test]['passed'] = True\n                self.get_logger().info('\u2713 Balance test PASSED')\n            else:\n                self.get_logger().info('\u2717 Balance test FAILED')\n        else:\n            self.get_logger().info('\u2717 Balance test INCONCLUSIVE - insufficient data')\n\n    def run_manipulation_test(self):\n        \"\"\"Test manipulation capabilities\"\"\"\n        self.current_test = 'manipulation_basic'\n        self.test_results[self.current_test] = {\n            'passed': False,\n            'start_time': time.time()\n        }\n\n        self.get_logger().info('Running basic manipulation test...')\n\n        # In a real test, you would command specific manipulation tasks\n        # For this demo, just verify the controller can switch modes\n        mode_cmd = String()\n        mode_cmd.data = 'manipulate'\n\n        # Publish mode command (in real scenario, this would use services)\n        for i in range(5):\n            self.mode_cmd_pub.publish(mode_cmd)\n            time.sleep(0.5)\n\n        # Wait to see if mode switches\n        time.sleep(2.0)\n\n        # For this basic test, assume it passes if no errors occurred\n        self.test_results[self.current_test]['passed'] = True\n        self.get_logger().info('\u2713 Manipulation test completed')\n\n    def run_interaction_test(self):\n        \"\"\"Test social interaction capabilities\"\"\"\n        self.current_test = 'interaction_basic'\n        self.test_results[self.current_test] = {\n            'passed': False,\n            'start_time': time.time()\n        }\n\n        self.get_logger().info('Running basic interaction test...')\n\n        # Switch to interaction mode\n        mode_cmd = String()\n        mode_cmd.data = 'interact'\n\n        for i in range(5):\n            self.mode_cmd_pub.publish(mode_cmd)\n            time.sleep(0.5)\n\n        # Wait to observe interaction behaviors\n        time.sleep(5.0)\n\n        # For this basic test, assume it passes\n        self.test_results[self.current_test]['passed'] = True\n        self.get_logger().info('\u2713 Interaction test completed')\n\n    def run_integration_test(self):\n        \"\"\"Test integrated behavior - balance + manipulation + interaction\"\"\"\n        self.current_test = 'integration_full'\n        self.test_results[self.current_test] = {\n            'passed': False,\n            'start_time': time.time()\n        }\n\n        self.get_logger().info('Running full integration test...')\n\n        # This would test the controller coordinating all capabilities\n        # For demo purposes, cycle through different modes\n        modes = ['balance', 'manipulate', 'interact', 'balance']\n\n        for mode in modes:\n            mode_cmd = String()\n            mode_cmd.data = mode\n            self.mode_cmd_pub.publish(mode_cmd)\n            self.get_logger().info(f'Switched to mode: {mode}')\n            time.sleep(3.0)\n\n        # For this basic test, assume it passes\n        self.test_results[self.current_test]['passed'] = True\n        self.get_logger().info('\u2713 Integration test completed')\n\n    def print_test_summary(self):\n        \"\"\"Print summary of all tests\"\"\"\n        self.get_logger().info('\\n' + '='*50)\n        self.get_logger().info('CONTROLLER TEST SUMMARY')\n        self.get_logger().info('='*50)\n\n        total_tests = len(self.test_results)\n        passed_tests = sum(1 for result in self.test_results.values() if result['passed'])\n\n        for test_name, result in self.test_results.items():\n            status = \"PASS\" if result['passed'] else \"FAIL\"\n            duration = time.time() - result['start_time']\n            self.get_logger().info(f'{test_name:<20} | {status:<4} | {duration:>5.1f}s')\n\n        self.get_logger().info('-'*50)\n        self.get_logger().info(f'Total: {total_tests}, Passed: {passed_tests}, Failed: {total_tests - passed_tests}')\n        self.get_logger().info(f'Success Rate: {(passed_tests/total_tests)*100:.1f}%' if total_tests > 0 else '0%')\n        self.get_logger().info('='*50)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    tester = AdvancedControllerTester()\n\n    try:\n        tester.run_comprehensive_test()\n    except KeyboardInterrupt:\n        tester.get_logger().info('Testing interrupted by user')\n    finally:\n        tester.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"expected-results",children:"Expected Results"}),"\n",(0,s.jsx)(n.p,{children:"When you run the complete humanoid controller system:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Balance Control"}),": The robot should maintain stable balance with CoM and ZMP within appropriate bounds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manipulation"}),": The robot should be able to execute basic reaching and grasping motions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social Interaction"}),": The robot should detect humans and respond with appropriate behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration"}),": All systems should work together without conflicts"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.p,{children:"If you encounter issues:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build Errors"}),": Ensure all dependencies are installed and paths are correct"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Runtime Errors"}),": Check that the simulation environment is properly configured"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Issues"}),": The controller may need tuning for your specific robot model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration Problems"}),": Verify that all ROS 2 topics and services are properly connected"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"extensions",children:"Extensions"}),"\n",(0,s.jsx)(n.p,{children:"To enhance this controller:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add more sophisticated behaviors"})," like emotion recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement learning algorithms"})," to adapt to users"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add more complex manipulation tasks"})," like bimanual coordination"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Include advanced safety features"})," like collision avoidance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add localization and mapping"})," for autonomous navigation"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This controller provides a solid foundation for humanoid robot control that integrates balance, manipulation, and social interaction capabilities in a unified framework."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(_,{...e})}):_(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var o=t(6540);const s={},i=o.createContext(s);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);