"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[7271],{5275:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>_,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module3/sim-to-real","title":"Sim-to-Real Transfer Techniques","description":"Introduction to Sim-to-Real Transfer","source":"@site/docs/module3/sim-to-real.md","sourceDirName":"module3","slug":"/module3/sim-to-real","permalink":"/Physical-AI-Humanoid-book/docs/module3/sim-to-real","draft":false,"unlisted":false,"editUrl":"https://github.com/user/physical-ai-humanoid-robotics/tree/main/docs/module3/sim-to-real.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning for Control","permalink":"/Physical-AI-Humanoid-book/docs/module3/reinforcement-learning"},"next":{"title":"Balance and Postural Control","permalink":"/Physical-AI-Humanoid-book/docs/module4/balance-control"}}');var t=a(4848),s=a(8453);const r={},o="Sim-to-Real Transfer Techniques",l={},c=[{value:"Introduction to Sim-to-Real Transfer",id:"introduction-to-sim-to-real-transfer",level:2},{value:"The Reality Gap Problem",id:"the-reality-gap-problem",level:2},{value:"Sources of the Reality Gap",id:"sources-of-the-reality-gap",level:3},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Visual Domain Randomization",id:"visual-domain-randomization",level:3},{value:"Physics Domain Randomization",id:"physics-domain-randomization",level:3},{value:"Domain Adaptation Techniques",id:"domain-adaptation-techniques",level:2},{value:"Unsupervised Domain Adaptation",id:"unsupervised-domain-adaptation",level:3},{value:"SimGAN (Simulation-to-Reality with GANs)",id:"simgan-simulation-to-reality-with-gans",level:3},{value:"System Identification and System Modeling",id:"system-identification-and-system-modeling",level:2},{value:"System Identification for Dynamics Matching",id:"system-identification-for-dynamics-matching",level:3},{value:"Adaptive Control and Online Learning",id:"adaptive-control-and-online-learning",level:2},{value:"Online Domain Adaptation",id:"online-domain-adaptation",level:3},{value:"Sensor Fusion and Calibration",id:"sensor-fusion-and-calibration",level:2},{value:"Multi-Sensor Calibration for Transfer",id:"multi-sensor-calibration-for-transfer",level:3},{value:"Robust Control Techniques",id:"robust-control-techniques",level:2},{value:"Robust Control for Domain Transfer",id:"robust-control-for-domain-transfer",level:3},{value:"Transfer Learning Approaches",id:"transfer-learning-approaches",level:2},{value:"Fine-tuning for Real-World Deployment",id:"fine-tuning-for-real-world-deployment",level:3},{value:"Validation and Testing Strategies",id:"validation-and-testing-strategies",level:2},{value:"Sim-to-Real Validation Framework",id:"sim-to-real-validation-framework",level:3},{value:"Best Practices for Successful Transfer",id:"best-practices-for-successful-transfer",level:2},{value:"1. Gradual Deployment Strategy",id:"1-gradual-deployment-strategy",level:3},{value:"2. Safety-First Approach",id:"2-safety-first-approach",level:3},{value:"Troubleshooting Common Transfer Issues",id:"troubleshooting-common-transfer-issues",level:2},{value:"1. Performance Degradation",id:"1-performance-degradation",level:3},{value:"2. Instability and Oscillation",id:"2-instability-and-oscillation",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"sim-to-real-transfer-techniques",children:"Sim-to-Real Transfer Techniques"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-sim-to-real-transfer",children:"Introduction to Sim-to-Real Transfer"}),"\n",(0,t.jsx)(n.p,{children:"Sim-to-Real transfer, also known as domain transfer, is the process of taking models, policies, or systems trained in simulation and successfully deploying them on real robots. This is a critical challenge in robotics because simulations, while valuable for training and testing, inevitably differ from reality due to modeling inaccuracies, sensor noise, actuator dynamics, and environmental factors."}),"\n",(0,t.jsx)(n.h2,{id:"the-reality-gap-problem",children:"The Reality Gap Problem"}),"\n",(0,t.jsx)(n.h3,{id:"sources-of-the-reality-gap",children:"Sources of the Reality Gap"}),"\n",(0,t.jsx)(n.p,{children:"The reality gap encompasses all differences between simulation and real-world performance:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Domain Gap"}),": Differences in appearance, lighting, textures, and camera characteristics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physics Domain Gap"}),": Differences in friction, mass, dynamics, and contact mechanics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Domain Gap"}),": Differences in sensor noise, latency, and accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Actuator Domain Gap"}),": Differences in motor dynamics, delays, and precision"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environmental Domain Gap"}),": Differences in workspace conditions, disturbances, and objects"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Simulation Domain \u2192 Reality Gap \u2192 Real World Domain\n     \u2193                \u2193               \u2193\nPerfect models    Modeling errors   Physical world\nClean data      Sensor noise     Environmental\nNo delays       Actuator delays   disturbances\n"})}),"\n",(0,t.jsx)(n.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,t.jsx)(n.h3,{id:"visual-domain-randomization",children:"Visual Domain Randomization"}),"\n",(0,t.jsx)(n.p,{children:"Domain randomization is one of the most effective techniques for creating robust policies that can handle domain shift:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class VisualDomainRandomizer:\n    def __init__(self):\n        self.randomization_params = {\n            'lighting': {\n                'intensity_range': (0.5, 2.0),\n                'color_temperature_range': (3000, 8000),\n                'position_variance': 0.5\n            },\n            'textures': {\n                'materials': ['metal', 'wood', 'plastic', 'fabric'],\n                'roughness_range': (0.1, 0.9),\n                'metallic_range': (0.0, 1.0)\n            },\n            'camera': {\n                'exposure_range': (-2.0, 2.0),\n                'white_balance_range': (0.8, 1.2),\n                'noise_std_range': (0.001, 0.02)\n            }\n        }\n\n    def randomize_visual_environment(self, scene):\n        \"\"\"Apply visual domain randomization to the scene\"\"\"\n        # Randomize lighting conditions\n        self.randomize_lighting(scene)\n\n        # Randomize material properties\n        self.randomize_materials(scene)\n\n        # Add camera noise\n        self.add_camera_noise(scene)\n\n        # Randomize textures and colors\n        self.randomize_textures(scene)\n\n    def randomize_lighting(self, scene):\n        \"\"\"Randomize lighting parameters\"\"\"\n        for light in scene.get_lights():\n            # Randomize intensity\n            intensity_factor = np.random.uniform(\n                self.randomization_params['lighting']['intensity_range'][0],\n                self.randomization_params['lighting']['intensity_range'][1]\n            )\n            light.set_intensity(light.base_intensity * intensity_factor)\n\n            # Randomize color temperature\n            color_temp = np.random.uniform(\n                self.randomization_params['lighting']['color_temperature_range'][0],\n                self.randomization_params['lighting']['color_temperature_range'][1]\n            )\n            light.set_color_temperature(color_temp)\n\n            # Randomize position\n            pos_variance = self.randomization_params['lighting']['position_variance']\n            random_offset = np.random.uniform(-pos_variance, pos_variance, 3)\n            light.set_position(light.base_position + random_offset)\n\n    def add_camera_noise(self, scene):\n        \"\"\"Add realistic camera noise\"\"\"\n        for camera in scene.get_cameras():\n            # Randomize noise parameters\n            noise_std = np.random.uniform(\n                self.randomization_params['camera']['noise_std_range'][0],\n                self.randomization_params['camera']['noise_std_range'][1]\n            )\n            camera.add_noise(std=noise_std)\n\n            # Randomize exposure\n            exposure = np.random.uniform(\n                self.randomization_params['camera']['exposure_range'][0],\n                self.randomization_params['camera']['exposure_range'][1]\n            )\n            camera.set_exposure(exposure)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"physics-domain-randomization",children:"Physics Domain Randomization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class PhysicsDomainRandomizer:\n    def __init__(self):\n        self.randomization_params = {\n            'friction': (0.1, 1.0),\n            'restitution': (0.0, 0.5),\n            'mass_multiplier': (0.8, 1.2),\n            'damping': (0.95, 1.05),\n            'gravity': (9.7, 9.9)\n        }\n\n    def randomize_physics_properties(self, scene):\n        \"\"\"Randomize physics properties in the simulation\"\"\"\n        # Randomize friction coefficients\n        self.randomize_friction(scene)\n\n        # Randomize restitution (bounciness)\n        self.randomize_restitution(scene)\n\n        # Randomize object masses\n        self.randomize_masses(scene)\n\n        # Randomize damping parameters\n        self.randomize_damping(scene)\n\n        # Randomize gravity\n        self.randomize_gravity(scene)\n\n    def randomize_friction(self, scene):\n        \"\"\"Randomize friction coefficients\"\"\"\n        for obj in scene.get_objects():\n            friction = np.random.uniform(\n                self.randomization_params['friction'][0],\n                self.randomization_params['friction'][1]\n            )\n            obj.set_friction(friction)\n\n    def randomize_masses(self, scene):\n        \"\"\"Randomize object masses\"\"\"\n        for obj in scene.get_objects():\n            mass_multiplier = np.random.uniform(\n                self.randomization_params['mass_multiplier'][0],\n                self.randomization_params['mass_multiplier'][1]\n            )\n            original_mass = obj.get_mass()\n            new_mass = original_mass * mass_multiplier\n            obj.set_mass(new_mass)\n\n    def randomize_damping(self, scene):\n        \"\"\"Randomize damping parameters\"\"\"\n        for obj in scene.get_objects():\n            linear_damping = obj.get_linear_damping() * np.random.uniform(\n                self.randomization_params['damping'][0],\n                self.randomization_params['damping'][1]\n            )\n            angular_damping = obj.get_angular_damping() * np.random.uniform(\n                self.randomization_params['damping'][0],\n                self.randomization_params['damping'][1]\n            )\n            obj.set_damping(linear_damping, angular_damping)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"domain-adaptation-techniques",children:"Domain Adaptation Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"unsupervised-domain-adaptation",children:"Unsupervised Domain Adaptation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DomainAdaptationNetwork(nn.Module):\n    def __init__(self, input_dim, feature_dim=256):\n        super(DomainAdaptationNetwork, self).__init__()\n\n        # Feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, feature_dim),\n            nn.ReLU()\n        )\n\n        # Classifier for source domain\n        self.classifier = nn.Sequential(\n            nn.Linear(feature_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 10)  # Number of classes\n        )\n\n        # Domain discriminator\n        self.domain_discriminator = nn.Sequential(\n            nn.Linear(feature_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)  # Binary classification: source vs target\n        )\n\n    def forward(self, x, domain_label=None):\n        features = self.feature_extractor(x)\n\n        # Classification output\n        class_output = self.classifier(features)\n\n        # Domain classification (only during training)\n        domain_output = None\n        if domain_label is not None:\n            domain_output = self.domain_discriminator(features)\n\n        return class_output, domain_output, features\n\nclass UnsupervisedDomainAdaptation:\n    def __init__(self, model, lr=1e-4):\n        self.model = model\n        self.classifier_criterion = nn.CrossEntropyLoss()\n        self.domain_criterion = nn.BCEWithLogitsLoss()\n\n        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n        self.domain_optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    def train_step(self, source_data, target_data, source_labels):\n        """Training step for domain adaptation"""\n        batch_size = source_data.size(0)\n\n        # Prepare domain labels\n        source_domain_labels = torch.zeros(batch_size, 1).to(source_data.device)\n        target_domain_labels = torch.ones(batch_size, 1).to(target_data.device)\n\n        # Train on source data\n        source_class_pred, source_domain_pred, _ = self.model(source_data, source_domain_labels)\n        source_class_loss = self.classifier_criterion(source_class_pred, source_labels)\n\n        # Train domain discriminator on source\n        source_domain_loss = self.domain_criterion(source_domain_pred, torch.zeros_like(source_domain_labels))\n\n        # Train on target data (unsupervised)\n        _, target_domain_pred, _ = self.model(target_data, target_domain_labels)\n        target_domain_loss = self.domain_criterion(target_domain_pred, torch.ones_like(target_domain_labels))\n\n        # Total loss for domain discriminator\n        domain_loss = source_domain_loss + target_domain_loss\n\n        # Gradient reversal for feature alignment\n        target_features = self.model.feature_extractor(target_data)\n        target_domain_pred_gr = self.model.domain_discriminator(target_features.detach())\n        domain_adversarial_loss = self.domain_criterion(target_domain_pred_gr, torch.zeros_like(target_domain_labels))\n\n        # Combined loss\n        total_loss = source_class_loss + 0.1 * domain_loss - 0.1 * domain_adversarial_loss\n\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        self.optimizer.step()\n\n        return total_loss.item()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"simgan-simulation-to-reality-with-gans",children:"SimGAN (Simulation-to-Reality with GANs)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SimGAN(nn.Module):\n    def __init__(self, input_channels=3):\n        super(SimGAN, self).__init__()\n\n        # Refiner network (simulator \u2192 realistic)\n        self.refiner = nn.Sequential(\n            # Input: simulated image\n            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, input_channels, kernel_size=3, padding=1),\n            nn.Tanh()\n        )\n\n        # Discriminator network\n        self.discriminator = nn.Sequential(\n            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0),\n            nn.Sigmoid()\n        )\n\n    def forward(self, sim_image):\n        refined_image = self.refiner(sim_image)\n        return refined_image\n\nclass SimGANTrainer:\n    def __init__(self, simgan_model, lr=2e-4):\n        self.model = simgan_model\n        self.optimizer_g = optim.Adam(self.model.refiner.parameters(), lr=lr)\n        self.optimizer_d = optim.Adam(self.model.discriminator.parameters(), lr=lr)\n        self.l1_loss = nn.L1Loss()\n        self.bce_loss = nn.BCELoss()\n\n    def train_step(self, real_images, sim_images):\n        """Training step for SimGAN"""\n        batch_size = sim_images.size(0)\n\n        # Labels for real/fake\n        real_labels = torch.ones(batch_size, 1, 1, 1).to(sim_images.device)\n        fake_labels = torch.zeros(batch_size, 1, 1, 1).to(sim_images.device)\n\n        # Train discriminator\n        self.optimizer_d.zero_grad()\n\n        # Real images\n        d_real = self.model.discriminator(real_images)\n        d_real_loss = self.bce_loss(d_real, real_labels)\n\n        # Simulated images (original)\n        d_sim = self.model.discriminator(sim_images)\n        d_sim_loss = self.bce_loss(d_sim, fake_labels)\n\n        # Refined images\n        refined_images = self.model(sim_images)\n        d_refined = self.model.discriminator(refined_images.detach())\n        d_refined_loss = self.bce_loss(d_refined, fake_labels)\n\n        d_loss = d_real_loss + d_sim_loss + d_refined_loss\n        d_loss.backward()\n        self.optimizer_d.step()\n\n        # Train generator (refiner)\n        self.optimizer_g.zero_grad()\n\n        # Adversarial loss\n        d_refined_for_g = self.model.discriminator(refined_images)\n        g_adv_loss = self.bce_loss(d_refined_for_g, real_labels)\n\n        # Content loss (preserve important features)\n        g_content_loss = self.l1_loss(refined_images, sim_images)\n\n        g_loss = g_adv_loss + 0.1 * g_content_loss\n        g_loss.backward()\n        self.optimizer_g.step()\n\n        return g_loss.item(), d_loss.item()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"system-identification-and-system-modeling",children:"System Identification and System Modeling"}),"\n",(0,t.jsx)(n.h3,{id:"system-identification-for-dynamics-matching",children:"System Identification for Dynamics Matching"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class SystemIdentifier:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.sim_params = {}\n        self.real_params = {}\n        self.correction_factors = {}\n\n    def identify_dynamics_parameters(self):\n        \"\"\"Identify key dynamics parameters\"\"\"\n        # Collect data from real robot\n        real_data = self.collect_real_robot_data()\n\n        # Estimate parameters using system identification\n        self.estimate_mass_parameters(real_data)\n        self.estimate_friction_parameters(real_data)\n        self.estimate_inertia_parameters(real_data)\n        self.estimate_actuator_dynamics(real_data)\n\n    def estimate_mass_parameters(self, data):\n        \"\"\"Estimate mass-related parameters\"\"\"\n        # Use least squares or other system identification methods\n        masses = []\n        for joint in self.robot.get_joints():\n            # Collect data for each joint\n            joint_data = self.filter_data_for_joint(data, joint.name)\n\n            # Estimate mass using inverse dynamics\n            estimated_mass = self.inverse_dynamics_mass_estimation(joint_data)\n            masses.append(estimated_mass)\n\n        self.real_params['masses'] = masses\n\n    def estimate_friction_parameters(self, data):\n        \"\"\"Estimate friction parameters (Coulomb and viscous)\"\"\"\n        friction_coeffs = []\n        for joint in self.robot.get_joints():\n            joint_data = self.filter_data_for_joint(data, joint.name)\n\n            # Estimate friction using regression\n            coulomb, viscous = self.estimate_friction_coefficients(joint_data)\n            friction_coeffs.append({'coulomb': coulomb, 'viscous': viscous})\n\n        self.real_params['friction'] = friction_coeffs\n\n    def collect_real_robot_data(self):\n        \"\"\"Collect experimental data from real robot\"\"\"\n        data = {\n            'joint_positions': [],\n            'joint_velocities': [],\n            'joint_accelerations': [],\n            'torques': [],\n            'timestamps': []\n        }\n\n        # Execute predefined trajectories\n        trajectories = self.generate_excitation_trajectories()\n\n        for traj in trajectories:\n            # Execute trajectory on real robot\n            self.execute_trajectory_safely(traj)\n\n            # Record data\n            recorded_data = self.record_robot_state()\n            data['joint_positions'].extend(recorded_data['positions'])\n            data['joint_velocities'].extend(recorded_data['velocities'])\n            data['torques'].extend(recorded_data['torques'])\n            data['timestamps'].extend(recorded_data['timestamps'])\n\n        return data\n\n    def update_simulation_with_real_params(self):\n        \"\"\"Update simulation with identified real-world parameters\"\"\"\n        for i, joint in enumerate(self.robot.get_joints()):\n            # Apply correction factors to simulation\n            if 'masses' in self.real_params:\n                real_mass = self.real_params['masses'][i]\n                sim_mass = joint.get_mass()\n                correction_factor = real_mass / sim_mass\n                joint.set_mass(real_mass)\n\n            if 'friction' in self.real_params:\n                real_friction = self.real_params['friction'][i]\n                joint.set_friction(real_friction['coulomb'], real_friction['viscous'])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"adaptive-control-and-online-learning",children:"Adaptive Control and Online Learning"}),"\n",(0,t.jsx)(n.h3,{id:"online-domain-adaptation",children:"Online Domain Adaptation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class OnlineDomainAdapter:\n    def __init__(self, base_policy, adaptation_rate=0.01):\n        self.base_policy = base_policy\n        self.adaptation_rate = adaptation_rate\n        self.performance_history = []\n        self.adaptation_parameters = {}\n        self.is_adapting = True\n\n    def adapt_policy_online(self, real_observation, real_action, real_reward, sim_observation, sim_action):\n        \"\"\"Adapt policy based on real-world experience\"\"\"\n        if not self.is_adapting:\n            return\n\n        # Calculate performance difference\n        performance_diff = self.calculate_performance_difference(\n            real_observation, real_action, real_reward,\n            sim_observation, sim_action\n        )\n\n        # Update adaptation parameters\n        self.update_adaptation_parameters(performance_diff)\n\n        # Adjust policy based on adaptation\n        adapted_policy = self.apply_adaptation(self.base_policy, self.adaptation_parameters)\n\n        return adapted_policy\n\n    def calculate_performance_difference(self, real_obs, real_act, real_rew, sim_obs, sim_act):\n        \"\"\"Calculate the difference between real and simulated performance\"\"\"\n        # Calculate state difference\n        state_diff = np.linalg.norm(real_obs - sim_obs)\n\n        # Calculate action difference (if applicable)\n        action_diff = np.linalg.norm(real_act - sim_act) if sim_act is not None else 0\n\n        # Calculate reward difference\n        reward_diff = real_rew  # Real reward as indicator\n\n        return {\n            'state_diff': state_diff,\n            'action_diff': action_diff,\n            'reward_diff': reward_diff\n        }\n\n    def update_adaptation_parameters(self, performance_diff):\n        \"\"\"Update adaptation parameters based on performance\"\"\"\n        # Update based on state difference\n        if performance_diff['state_diff'] > 0.1:  # Threshold for significant difference\n            # Adjust observation preprocessing\n            self.adaptation_parameters['observation_scaling'] = self.adaptation_parameters.get('observation_scaling', 1.0) * (\n                1 - self.adaptation_rate * performance_diff['state_diff']\n            )\n\n        # Update based on reward difference\n        if performance_diff['reward_diff'] < 0:  # Negative reward indicates poor performance\n            # Increase exploration\n            self.adaptation_parameters['exploration_bonus'] = self.adaptation_parameters.get('exploration_bonus', 0.0) + (\n                self.adaptation_rate * abs(performance_diff['reward_diff'])\n            )\n\n    def apply_adaptation(self, base_policy, adaptation_params):\n        \"\"\"Apply adaptation to base policy\"\"\"\n        # This would typically involve adjusting policy parameters\n        # For example, modifying neural network weights or control gains\n        adapted_policy = base_policy.copy()\n\n        if 'observation_scaling' in adaptation_params:\n            adapted_policy.set_observation_scaling(adaptation_params['observation_scaling'])\n\n        if 'exploration_bonus' in adaptation_params:\n            adapted_policy.set_exploration_bonus(adaptation_params['exploration_bonus'])\n\n        return adapted_policy\n"})}),"\n",(0,t.jsx)(n.h2,{id:"sensor-fusion-and-calibration",children:"Sensor Fusion and Calibration"}),"\n",(0,t.jsx)(n.h3,{id:"multi-sensor-calibration-for-transfer",children:"Multi-Sensor Calibration for Transfer"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SensorCalibrator:\n    def __init__(self):\n        self.calibration_data = {}\n        self.transformation_matrices = {}\n        self.uncertainty_models = {}\n\n    def calibrate_camera_to_robot(self, camera, robot):\n        """Calibrate camera-to-robot transformation"""\n        # Collect calibration data using checkerboard or known objects\n        calibration_points = self.collect_calibration_data(camera, robot)\n\n        # Compute transformation matrix\n        transformation = self.compute_camera_robot_transform(calibration_points)\n\n        self.transformation_matrices[\'camera_to_robot\'] = transformation\n\n        # Estimate uncertainty\n        uncertainty = self.estimate_calibration_uncertainty(calibration_points)\n        self.uncertainty_models[\'camera_to_robot\'] = uncertainty\n\n    def collect_calibration_data(self, camera, robot):\n        """Collect calibration data points"""\n        calibration_points = []\n\n        # Move robot to known positions\n        calibration_poses = self.generate_calibration_poses()\n\n        for pose in calibration_poses:\n            # Move robot to calibration pose\n            robot.move_to_joint_position(pose)\n\n            # Capture image and extract features\n            image = camera.capture()\n            image_features = self.extract_features(image)\n\n            # Get robot\'s known position\n            robot_position = robot.get_end_effector_pose()\n\n            calibration_points.append({\n                \'image_features\': image_features,\n                \'robot_position\': robot_position\n            })\n\n        return calibration_points\n\n    def calibrate_lidar_to_camera(self, lidar, camera):\n        """Calibrate LiDAR to camera transformation"""\n        # Collect synchronized data\n        lidar_data, camera_data = self.collect_synchronized_data(lidar, camera)\n\n        # Find common features\n        common_features = self.match_features(lidar_data, camera_data)\n\n        # Compute transformation\n        transformation = self.compute_lidar_camera_transform(common_features)\n\n        self.transformation_matrices[\'lidar_to_camera\'] = transformation\n\n    def apply_sensor_correction(self, sensor_data, sensor_type):\n        """Apply calibration corrections to sensor data"""\n        corrected_data = sensor_data.copy()\n\n        if sensor_type == \'camera\':\n            # Apply camera intrinsics/extrinsics correction\n            corrected_data = self.correct_camera_data(sensor_data)\n\n        elif sensor_type == \'lidar\':\n            # Apply LiDAR calibration\n            corrected_data = self.correct_lidar_data(sensor_data)\n\n        elif sensor_type == \'imu\':\n            # Apply IMU bias and scale corrections\n            corrected_data = self.correct_imu_data(sensor_data)\n\n        return corrected_data\n\n    def correct_camera_data(self, camera_data):\n        """Apply camera calibration corrections"""\n        # Apply distortion correction\n        corrected_image = self.undistort_image(camera_data.image)\n\n        # Apply extrinsic transformation\n        corrected_pose = self.apply_transformation(\n            camera_data.pose,\n            self.transformation_matrices[\'camera_to_robot\']\n        )\n\n        return {\n            \'image\': corrected_image,\n            \'pose\': corrected_pose,\n            \'uncertainty\': self.uncertainty_models[\'camera_to_robot\']\n        }\n'})}),"\n",(0,t.jsx)(n.h2,{id:"robust-control-techniques",children:"Robust Control Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"robust-control-for-domain-transfer",children:"Robust Control for Domain Transfer"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class RobustController:\n    def __init__(self, nominal_model, uncertainty_bounds):\n        self.nominal_model = nominal_model\n        self.uncertainty_bounds = uncertainty_bounds\n        self.controller = self.design_robust_controller()\n\n    def design_robust_controller(self):\n        """Design a robust controller that can handle uncertainties"""\n        # Use H-infinity or mu-synthesis techniques\n        # This is a simplified example - real implementation would be more complex\n\n        # Design controller with integral action for disturbance rejection\n        controller = {\n            \'Kp\': self.calculate_robust_proportional_gain(),\n            \'Ki\': self.calculate_robust_integral_gain(),\n            \'Kd\': self.calculate_robust_derivative_gain()\n        }\n\n        return controller\n\n    def calculate_robust_proportional_gain(self):\n        """Calculate proportional gain considering uncertainties"""\n        # Conservative gain selection based on uncertainty bounds\n        base_gain = self.nominal_model.calculate_nominal_gain()\n\n        # Reduce gain to ensure stability under uncertainties\n        robust_gain = base_gain * (1 - self.uncertainty_bounds[\'max_uncertainty\'])\n\n        return robust_gain\n\n    def apply_robust_control(self, state_error, uncertainty_estimate):\n        """Apply robust control law"""\n        # Use state error and uncertainty estimate to compute control\n        proportional_term = self.controller[\'Kp\'] * state_error\n\n        # Adjust for uncertainty\n        uncertainty_compensation = self.compensate_for_uncertainty(\n            uncertainty_estimate, state_error\n        )\n\n        control_output = proportional_term + uncertainty_compensation\n\n        # Apply safety limits\n        control_output = np.clip(control_output,\n                                -self.nominal_model.max_control,\n                                self.nominal_model.max_control)\n\n        return control_output\n\n    def compensate_for_uncertainty(self, uncertainty, error):\n        """Compensate control for model uncertainty"""\n        # Increase control effort based on uncertainty level\n        compensation_gain = uncertainty * self.controller[\'Kp\'] * 0.1\n\n        # Apply compensation in direction opposite to error\n        compensation = -compensation_gain * np.sign(error)\n\n        return compensation\n'})}),"\n",(0,t.jsx)(n.h2,{id:"transfer-learning-approaches",children:"Transfer Learning Approaches"}),"\n",(0,t.jsx)(n.h3,{id:"fine-tuning-for-real-world-deployment",children:"Fine-tuning for Real-World Deployment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class TransferLearner:\n    def __init__(self, pretrained_model, real_robot_data_size=100):\n        self.pretrained_model = pretrained_model\n        self.real_robot_data_size = real_robot_data_size\n        self.finetuning_phase = True\n\n    def finetune_on_real_data(self, real_data_loader):\n        """Fine-tune pretrained model on real robot data"""\n        # Freeze early layers, fine-tune later layers\n        self.freeze_early_layers()\n\n        # Define fine-tuning optimizer (lower learning rate)\n        optimizer = optim.Adam(\n            filter(lambda p: p.requires_grad, self.pretrained_model.parameters()),\n            lr=1e-5  # Lower learning rate for fine-tuning\n        )\n\n        criterion = nn.MSELoss()\n\n        for epoch in range(10):  # Limited epochs to prevent overfitting\n            for batch_idx, (data, target) in enumerate(real_data_loader):\n                optimizer.zero_grad()\n                output = self.pretrained_model(data)\n                loss = criterion(output, target)\n                loss.backward()\n                optimizer.step()\n\n                if batch_idx % 10 == 0:\n                    print(f\'Fine-tuning Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.6f}\')\n\n    def freeze_early_layers(self):\n        """Freeze early layers of the network"""\n        # Example: freeze first half of the layers\n        layers = list(self.pretrained_model.children())\n        num_layers_to_freeze = len(layers) // 2\n\n        for i in range(num_layers_to_freeze):\n            for param in layers[i].parameters():\n                param.requires_grad = False\n\n    def gradual_unfreezing(self, real_data_loader):\n        """Gradually unfreeze layers during fine-tuning"""\n        layers = list(self.pretrained_model.children())\n        num_layers = len(layers)\n\n        for layer_idx in range(num_layers):\n            # Unfreeze current layer\n            for param in layers[layer_idx].parameters():\n                param.requires_grad = True\n\n            # Fine-tune with current unfrozen layers\n            self.finetune_current_setup(real_data_loader, layer_idx)\n\n    def finetune_current_setup(self, data_loader, layer_idx):\n        """Fine-tune with current layer setup"""\n        optimizer = optim.Adam(\n            filter(lambda p: p.requires_grad, self.pretrained_model.parameters()),\n            lr=1e-5 * (layer_idx + 1)  # Slightly increase learning rate\n        )\n\n        # Run few epochs with current setup\n        for epoch in range(3):\n            total_loss = 0\n            for data, target in data_loader:\n                optimizer.zero_grad()\n                output = self.pretrained_model(data)\n                loss = nn.MSELoss()(output, target)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"validation-and-testing-strategies",children:"Validation and Testing Strategies"}),"\n",(0,t.jsx)(n.h3,{id:"sim-to-real-validation-framework",children:"Sim-to-Real Validation Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SimToRealValidator:\n    def __init__(self, sim_env, real_env):\n        self.sim_env = sim_env\n        self.real_env = real_env\n        self.metrics = {\n            \'success_rate\': [],\n            \'execution_time\': [],\n            \'trajectory_deviation\': [],\n            \'energy_efficiency\': []\n        }\n\n    def validate_transfer(self, policy, num_trials=10):\n        """Validate policy transfer from sim to real"""\n        sim_successes = 0\n        real_successes = 0\n\n        for trial in range(num_trials):\n            # Test in simulation\n            sim_success, sim_time, sim_trajectory = self.test_in_simulation(policy)\n            sim_successes += sim_success\n\n            # Test on real robot\n            real_success, real_time, real_trajectory = self.test_on_real_robot(policy)\n            real_successes += real_success\n\n            # Calculate trajectory deviation\n            deviation = self.calculate_trajectory_deviation(sim_trajectory, real_trajectory)\n\n            # Record metrics\n            self.metrics[\'success_rate\'].append((sim_success, real_success))\n            self.metrics[\'execution_time\'].append((sim_time, real_time))\n            self.metrics[\'trajectory_deviation\'].append(deviation)\n\n        # Calculate transfer success rate\n        sim_success_rate = sim_successes / num_trials\n        real_success_rate = real_successes / num_trials\n\n        transfer_success_rate = real_success_rate / sim_success_rate if sim_success_rate > 0 else 0\n\n        return {\n            \'sim_success_rate\': sim_success_rate,\n            \'real_success_rate\': real_success_rate,\n            \'transfer_success_rate\': transfer_success_rate,\n            \'average_deviation\': np.mean(self.metrics[\'trajectory_deviation\'])\n        }\n\n    def calculate_trajectory_deviation(self, sim_traj, real_traj):\n        """Calculate deviation between simulated and real trajectories"""\n        if len(sim_traj) == 0 or len(real_traj) == 0:\n            return float(\'inf\')\n\n        # Interpolate trajectories to same length\n        sim_interp = self.interpolate_trajectory(sim_traj, 100)\n        real_interp = self.interpolate_trajectory(real_traj, 100)\n\n        # Calculate average distance between trajectories\n        distances = []\n        for s, r in zip(sim_interp, real_interp):\n            dist = np.linalg.norm(np.array(s) - np.array(r))\n            distances.append(dist)\n\n        return np.mean(distances)\n\n    def test_in_simulation(self, policy):\n        """Test policy in simulation environment"""\n        state = self.sim_env.reset()\n        trajectory = []\n        success = False\n        start_time = time.time()\n\n        for step in range(1000):  # Max steps\n            action = policy.select_action(state)\n            next_state, reward, done, info = self.sim_env.step(action)\n\n            trajectory.append(next_state)\n\n            if self.is_task_success(next_state):\n                success = True\n                break\n\n            if done:\n                break\n\n            state = next_state\n\n        execution_time = time.time() - start_time\n        return success, execution_time, trajectory\n\n    def test_on_real_robot(self, policy):\n        """Test policy on real robot (with safety measures)"""\n        # Reset real robot to safe state\n        self.reset_real_robot_safely()\n\n        state = self.get_real_robot_state()\n        trajectory = []\n        success = False\n        start_time = time.time()\n\n        for step in range(1000):  # Max steps\n            # Add safety checks\n            if self.is_robot_in_safe_state():\n                action = policy.select_action(state)\n                next_state, reward, done, info = self.execute_real_action(action)\n\n                trajectory.append(next_state)\n\n                if self.is_task_success_real(next_state):\n                    success = True\n                    break\n\n                if done or self.is_emergency_stop_needed():\n                    break\n\n                state = next_state\n            else:\n                # Emergency stop\n                self.emergency_stop()\n                break\n\n        execution_time = time.time() - start_time\n        return success, execution_time, trajectory\n\n    def is_robot_in_safe_state(self):\n        """Check if robot is in safe operational state"""\n        # Check joint limits, collisions, etc.\n        joint_positions = self.get_real_robot_joint_positions()\n        joint_limits = self.get_robot_joint_limits()\n\n        for pos, limits in zip(joint_positions, joint_limits):\n            if pos < limits[0] or pos > limits[1]:\n                return False\n\n        # Check for collisions\n        if self.detect_real_robot_collision():\n            return False\n\n        return True\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices-for-successful-transfer",children:"Best Practices for Successful Transfer"}),"\n",(0,t.jsx)(n.h3,{id:"1-gradual-deployment-strategy",children:"1. Gradual Deployment Strategy"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class GradualDeployment:\n    def __init__(self, policy, safety_monitor):\n        self.policy = policy\n        self.safety_monitor = safety_monitor\n        self.confidence_levels = [\'low\', \'medium\', \'high\']\n        self.current_confidence = \'low\'\n\n    def deploy_gradually(self):\n        """Deploy policy with increasing confidence levels"""\n        for confidence in self.confidence_levels:\n            print(f"Deploying at {confidence} confidence level")\n\n            if self.test_at_confidence_level(confidence):\n                self.current_confidence = confidence\n                print(f"Success at {confidence} level, proceeding to next")\n            else:\n                print(f"Failed at {confidence} level, stopping deployment")\n                break\n\n    def test_at_confidence_level(self, confidence):\n        """Test policy at specific confidence level"""\n        if confidence == \'low\':\n            # Limited workspace, simple tasks\n            return self.test_simple_task()\n        elif confidence == \'medium\':\n            # Extended workspace, moderate complexity\n            return self.test_moderate_task()\n        elif confidence == \'high\':\n            # Full workspace, complex tasks\n            return self.test_complex_task()\n\n    def test_simple_task(self):\n        """Test on simple, safe tasks"""\n        # Use only safe, well-tested parts of policy\n        return self.safety_monitor.run_safety_test(self.policy, \'simple\')\n\n    def test_moderate_task(self):\n        """Test on moderate complexity tasks"""\n        return self.safety_monitor.run_safety_test(self.policy, \'moderate\')\n\n    def test_complex_task(self):\n        """Test on complex tasks"""\n        return self.safety_monitor.run_safety_test(self.policy, \'complex\')\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-safety-first-approach",children:"2. Safety-First Approach"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SafetyFirstTransfer:\n    def __init__(self, robot_controller):\n        self.controller = robot_controller\n        self.safety_limits = self.define_safety_limits()\n        self.emergency_procedures = self.define_emergency_procedures()\n\n    def define_safety_limits(self):\n        """Define comprehensive safety limits"""\n        return {\n            \'velocity_limits\': [0.5, 0.5, 0.5, 1.0, 1.0, 1.0],  # Cartesian + joint limits\n            \'force_limits\': [50, 50, 50, 5, 5, 5],  # Force limits in each direction\n            \'workspace_limits\': {\n                \'min\': [-1.0, -1.0, 0.0],\n                \'max\': [1.0, 1.0, 2.0]\n            },\n            \'collision_threshold\': 0.1  # Minimum distance to obstacles\n        }\n\n    def execute_with_safety(self, planned_action):\n        """Execute action with safety checks"""\n        # Check velocity limits\n        if not self.check_velocity_limits(planned_action):\n            return self.get_safe_action()\n\n        # Check force limits\n        if not self.check_force_limits():\n            return self.get_safe_action()\n\n        # Check workspace limits\n        if not self.check_workspace_limits(planned_action):\n            return self.get_safe_action()\n\n        # Check for potential collisions\n        if not self.check_collision_safety(planned_action):\n            return self.get_safe_action()\n\n        # If all checks pass, execute planned action\n        return planned_action\n\n    def check_velocity_limits(self, action):\n        """Check if action violates velocity limits"""\n        velocities = self.controller.calculate_velocities_from_action(action)\n        for vel, limit in zip(velocities, self.safety_limits[\'velocity_limits\']):\n            if abs(vel) > limit:\n                return False\n        return True\n\n    def check_collision_safety(self, action):\n        """Check if action might cause collision"""\n        future_positions = self.controller.predict_future_positions(action)\n\n        for pos in future_positions:\n            if self.is_position_in_collision(pos):\n                return False\n        return True\n\n    def get_safe_action(self):\n        """Return safe action when checks fail"""\n        # Return action that brings robot to safe state\n        return self.controller.get_stop_action()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-common-transfer-issues",children:"Troubleshooting Common Transfer Issues"}),"\n",(0,t.jsx)(n.h3,{id:"1-performance-degradation",children:"1. Performance Degradation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def diagnose_performance_degradation():\n    """Diagnose common causes of performance degradation"""\n    issues = []\n\n    # Check for sensor calibration drift\n    if sensor_calibration_drifted():\n        issues.append("Sensor calibration has drifted - recalibrate sensors")\n\n    # Check for actuator wear\n    if actuator_performance_degraded():\n        issues.append("Actuators may be worn - check mechanical components")\n\n    # Check for environmental changes\n    if environment_changed():\n        issues.append("Environment has changed - update simulation or adapt policy")\n\n    # Check for timing differences\n    if timing_mismatch():\n        issues.append("Timing mismatch between sim and real - synchronize control loops")\n\n    return issues\n\ndef sensor_calibration_drifted():\n    """Check if sensor calibration has drifted"""\n    # Compare current sensor readings with known reference\n    return False  # Implementation depends on specific sensors\n\ndef actuator_performance_degraded():\n    """Check if actuators have degraded"""\n    # Compare current performance with baseline\n    return False\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-instability-and-oscillation",children:"2. Instability and Oscillation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def stabilize_unstable_system():\n    """Methods to stabilize unstable transferred system"""\n    # Reduce control gains\n    reduce_control_gains()\n\n    # Add damping\n    increase_damping()\n\n    # Lower control frequency\n    reduce_control_frequency()\n\n    # Add low-pass filtering\n    apply_low_pass_filtering()\n\ndef reduce_control_gains():\n    """Reduce control gains to improve stability"""\n    # Apply conservative gain scheduling\n    current_gains = get_current_gains()\n    new_gains = [g * 0.8 for g in current_gains]  # Reduce by 20%\n    set_control_gains(new_gains)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Sim-to-Real transfer is a critical challenge in robotics that requires careful consideration of multiple factors including visual domain differences, physics modeling, sensor calibration, and control robustness. Success depends on:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Proper Domain Randomization"}),": Creating diverse simulation environments that encompass real-world variations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Identification"}),": Accurately modeling real-world dynamics and characteristics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robust Control"}),": Designing controllers that can handle uncertainties and disturbances"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gradual Deployment"}),": Carefully testing and validating policies with increasing complexity"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety-First Approach"}),": Implementing comprehensive safety measures and emergency procedures"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The NVIDIA Isaac Platform provides excellent tools for sim-to-real transfer including Isaac Sim for high-fidelity simulation, Isaac ROS for real-world integration, and comprehensive calibration and validation tools. By following best practices and using appropriate techniques, complex robotic policies can be successfully transferred from simulation to real-world deployment."}),"\n",(0,t.jsx)(n.p,{children:"In the next section, we'll create a practical lab exercise to apply these sim-to-real transfer concepts."})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>o});var i=a(6540);const t={},s=i.createContext(t);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);